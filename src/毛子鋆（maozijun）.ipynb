{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0/1背包问题的贪心算法设计与实现\n",
    "**姓名：毛子鋆**\n",
    "**学号：2025439109**\n",
    "**学院：天津大学福州国际联合学院**\n",
    "**专业：电子信息**\n",
    "\n",
    "**目标**：在 0/1 背包（每个物品要么选要么不选）中，设计仅基于贪心思想的快速近似方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 算法 A：GreedyDensity（标准贪心）\n",
    "\n",
    "- **思路**：按价值密度\n",
    "  $$\n",
    "  \\rho_i = \\frac{v_i}{w_i}\n",
    "  $$\n",
    "  从大到小排序，依次遍历物品，遇到还能装入背包的就装入（\"能装就装\"的前缀策略）。\n",
    "\n",
    "- **优点**：实现简单，时间复杂度主要由排序开销决定，为\n",
    "  $$\n",
    "  O(n \\log n)\n",
    "  $$\n",
    "\n",
    "- **缺点**：当存在\"价值极高但密度略低、且重量接近容量\"的重物品时，贪心可能被一批密度略高但价值较低的小物品\"诱导\"，从而错失最关键的物品，解质量可能与最优解有较大差距。\n",
    "\n",
    "---\n",
    "\n",
    "## 算法 B：Greedy+Max（BYOG+Max 的线性特化）\n",
    "\n",
    "- **思路**：在 GreedyDensity 得到的一组解 $S_g$ 与\"所有单个、且能放入背包的物品中价值最高的一件\" $j^*$ 之间取价值更高者作为最终解，即返回\n",
    "  $$\n",
    "  \\arg\\max\\{\\,v(S_g),\\ v(j^*)\\,\\}\n",
    "  $$\n",
    "  这种做法也常被称为 **Greedy + Singleton** 或 **Greedy Redux**。\n",
    "\n",
    "- **作用**：用\"最佳单件兜底\"补上标准贪心的短板。在许多典型的\"贪心陷阱\"构造中（例如存在密度略低但价值更高、重量接近容量的关键单件），Greedy+Max 能显著提升解质量。\n",
    "\n",
    "- **理论性质（这里只做提示，证明放在'理论分析'章节）**：Greedy+Max 在 0/1 背包上具有\n",
    "  $$\n",
    "  \\tfrac{1}{2}\n",
    "  $$\n",
    "  的近似保证，即输出解的价值至少是最优解 $\\mathrm{OPT}$ 的一半。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51e1e3812a9483e8"
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class Item:\n",
    "    \"\"\"物品结构：w=重量, v=价值, idx=原始索引（用于回溯解集合）\"\"\"\n",
    "    w: int\n",
    "    v: int\n",
    "    idx: int\n",
    "\n",
    "def greedy_density(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    标准贪心：按 v/w 从大到小排序，依次尝试装入。\n",
    "    返回：总价值、被选物品的原始索引列表\n",
    "    注意：假设所有 w>0；若有 w=0 的物品，可在预处理阶段先全部选入（本实验默认不生成 w=0）。\n",
    "    \"\"\"\n",
    "    # 中文注释：按密度从高到低排序；若密度相同，则按价值从高到低以减少并列不稳定性\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v / x.w, x.v), reverse=True)\n",
    "\n",
    "    total_w = 0\n",
    "    total_v = 0\n",
    "    chosen = []\n",
    "    for it in items_sorted:\n",
    "        # 中文注释：若还能放下该物品则选取\n",
    "        if total_w + it.w <= C:\n",
    "            total_w += it.w\n",
    "            total_v += it.v\n",
    "            chosen.append(it.idx)\n",
    "    return total_v, chosen\n",
    "\n",
    "def greedy_plus_max(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Greedy+Max（BYOG+Max 在线性 0/1 背包上的特化）：\n",
    "    取“标准贪心”的解与“单个价值最高但可装入的物品”两者中更优的一个。\n",
    "    返回：总价值、被选物品的原始索引列表\n",
    "    \"\"\"\n",
    "    # 中文注释：先跑一遍标准贪心\n",
    "    g_val, g_set = greedy_density(items, C)\n",
    "\n",
    "    # 中文注释：找到“能装入”的单个价值最高物品（best single item）\n",
    "    best_single = None\n",
    "    for it in items:\n",
    "        if it.w <= C and (best_single is None or it.v > best_single.v):\n",
    "            best_single = it\n",
    "\n",
    "    # 中文注释：如果没有任何单件能装入，直接返回贪心解\n",
    "    if best_single is None:\n",
    "        return g_val, g_set\n",
    "\n",
    "    # 中文注释：选择更优者作为 Greedy+Max 的输出\n",
    "    if best_single.v > g_val:\n",
    "        return best_single.v, [best_single.idx]\n",
    "    else:\n",
    "        return g_val, g_set\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-12T14:56:49.008104Z",
     "start_time": "2025-11-12T14:56:48.996559Z"
    }
   },
   "id": "7f65f8cae55af628",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 理论分析（两种贪心方法）\n",
    "\n",
    "**问题**  \n",
    "给定物品集合 $\\{(w_i,v_i)\\}_{i=1}^n$ 与容量 $C$，选择子集使得总重量 $\\le C$ 且总价值最大（0/1 背包）。\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 A：GreedyDensity（标准贪心）\n",
    "- **设计思想**：按价值密度 $\\rho_i = v_i/w_i$ 由大到小排序，依次尝试装入，能装就装。\n",
    "- **时间复杂度**：排序 $O(n\\log n)$ + 一次线性扫描 $O(n)$，合计 **$O(n\\log n)$**。\n",
    "- **空间复杂度**：除输入外仅常数级状态，**$O(1)$**。\n",
    "- **性能保证**：对 0/1 背包**无常数近似保证**。存在经典反例：若有\"价值极高但密度略差\"的重物品，纯密度排序可能完全错失关键物品，导致与最优解相差任意倍。\n",
    "- **关键瓶颈**：\n",
    "  1) **结构陷阱**：被高密度的小件\"诱导\"，忽略了单个高价值大件；\n",
    "  2) **并列/数值稳定性**：密度相近或相同的物品对排序次序敏感；\n",
    "  3) **理论下界**：最坏情况下缺乏任何常数因子近似保证。\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 B：Greedy+Max（BYOG+Max 在线性 0/1 背包的特化）\n",
    "- **设计思想**：在 **GreedyDensity** 的解 $S_g$ 与\"**能放入的单个价值最高物品**\"$j^*$ 之间取较优（又称 **Greedy Redux / Greedy+Singleton**）。\n",
    "- **时间复杂度**：一次排序 $O(n\\log n)$ + 一次线性扫描选\"最佳单件\" $O(n)$，合计 **$O(n\\log n)$**。\n",
    "- **空间复杂度**：**$O(1)$**。\n",
    "- **性能保证（$1/2$ 近似）**：\n",
    "  用**分数背包（LP 松弛）**作为上界。设将物品按 $\\rho_i$ 排序并做分数装填，分数解的最优值记为 $\\mathrm{UB}$，且 $\\mathrm{OPT} \\le \\mathrm{UB}$。\n",
    "  分数解由两部分构成：\n",
    "  (i) 被整装入的高密度前缀（其价值恰为 $v(S_g)$）；\n",
    "  (ii) 临界物品的一部分（其贡献不超过\"能装入的**最佳单件**\"的价值 $v(j^*)$）。\n",
    "  因而 $\\mathrm{OPT} \\le \\mathrm{UB} \\le v(S_g)+v(j^*)$。\n",
    "  于是 $\\max\\{v(S_g),\\,v(j^*)\\} \\ge \\tfrac{1}{2}\\mathrm{OPT}$，即 **2-approx（$1/2$ 近似）**。\n",
    "  > 该\"贪心或单件取优\"的构造是 BYOG+Max 框架在子模+背包场景下的一个特例；在线性 0/1 背包上即为上式。\n",
    "- **关键瓶颈**：\n",
    "  1) **下界紧性**：存在接近 $1/2$ 的反例；\n",
    "  2) **组合性不足**：若最优解依赖多个物品协同，仅\"单件兜底\"仍可能不足；\n",
    "  3) **实现细节**：与密度贪心一样受并列密度与数值精度影响，但总体鲁棒性优于纯密度法。\n",
    "\n",
    "---\n",
    "\n",
    "### 比较小结\n",
    "- **复杂度**：两者均为 $O(n\\log n)$ 时间、$O(1)$ 额外空间；\n",
    "- **保证**：GreedyDensity 无近似比；**Greedy+Max** 具 **$1/2$** 近似保证；\n",
    "- **适用性**：当容量较紧或存在\"单个高价值大件\"时，Greedy+Max 明显优于纯密度贪心。\n",
    "\n",
    "---\n",
    "\n",
    "### 备注：评测基线与上界（用于后续实验）\n",
    "- 小/中规模用**容量维度 DP**（伪多项式 $O(nC)$）求真 $\\mathrm{OPT}$；\n",
    "- 大规模用**分数背包（LP 松弛）**提供 $\\mathrm{UB}$ 评估解质量。\n",
    "\n",
    "---\n",
    "\n",
    "### 参考文献\n",
    "1. Yaroslavtsev G, Zhou S, Avdiukhin D. “bring your own greedy”+ max: near-optimal 1/2-approximations for submodular knapsack[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 3263-3274.\n",
    "2. Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer.（系统论述 0/1 背包、DP 与上界）\n",
    "3. Dantzig G B. Discrete-variable extremum problems[J]. Operations research, 1957, 5(2): 266-288.（分数背包贪心的早期来源）\n",
    "4. Cormen T H, Leiserson C E, Rivest R L, et al. Introduction to Algorithms (3-rd edition)[J]. MIT Press and McGraw-Hill, 2009.（0/1 背包 DP 与 LP 松弛背景）\n",
    "5. Vazirani V V. Approximation algorithms[M]. Berlin: springer, 2001.（近似算法背景与常见技术）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e18f306a2e6a68",
   "attachments": {
    "c562cd40-b1ca-4f9d-b975-c8f948b32d15.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo8AAAC3CAIAAAD8RCPFAAABc0lEQVR4Xu3BMQEAAADCoPVPbQwfoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOBpfa0AAdhFHwMAAAAASUVORK5CYII="
    },
    "21f83fc7-bbb0-46b0-9fbc-949c3a605126.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo8AAAC3CAIAAAD8RCPFAAABc0lEQVR4Xu3BMQEAAADCoPVPbQwfoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOBpfa0AAdhFHwMAAAAASUVORK5CYII="
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 实验设计（只围绕两种算法：GreedyDensity 与 Greedy+Max）\n",
    "\n",
    "**目标**  \n",
    "系统比较两种贪心方法在不同数据特性与规模下的运行表现，确保结果具备统计意义与可复现性。\n",
    "\n",
    "**实例族（≥5 类，覆盖结构化/随机与不同分布）**  \n",
    "1) **Uncorrelated**：重量与价值独立均匀；  \n",
    "2) **WeaklyCorr**：弱相关（$v \\approx w + \\text{noise}$）；\n",
    "3) **StronglyCorr**：强相关（$v = w + \\Delta$）；\n",
    "4) **InverseCorr**：反相关（越轻越高价值）；\n",
    "5) **HeavyTailed**：重量服从帕累托重尾，价值均匀；\n",
    "6) **GreedyTrap**：人为构造\"密度贪心陷阱\"（大量小而高密度物品 + 单个价值极高但密度略差的重物品）；\n",
    "7) **Uncorrelated-Large**（可选大规模基准）：更大 $n$ 以测试可扩展性（此类不做 DP，仅用 LP 上界评估）。\n",
    "\n",
    "**规模与重复**\n",
    "- 每类 **20 个样本 × ≥5 次随机重复**（本文设为 5），总计 ≥100 个样本/类；\n",
    "- 小/中规模类启用容量维度 DP（$O(nC)$）获得 OPT；大规模类使用分数背包（LP 松弛）作为质量上界；\n",
    "- 固定随机种子方案（按类名、repeat、sample 组合）以保证可复现。\n",
    "\n",
    "**记录指标（本节只\"生成与运行\"，汇总统计留到下一节）**\n",
    "- **运行时间**（每次调用的 wall-clock）；\n",
    "- **解值**（两算法的总价值）；\n",
    "- **质量差距**：若有 OPT，则 $(\\mathrm{OPT}-\\mathrm{ALG})/\\mathrm{OPT}$；若无 OPT，用 LP 上界 $\\mathrm{UB}$ 近似 $(\\mathrm{UB}-\\mathrm{ALG})/\\mathrm{UB}$；\n",
    "- 将原始结果表保存为 CSV，便于后续计算均值/方差与显著性检验。\n",
    "\n",
    "**复现实验的关键点**\n",
    "- 统一容量比例（例如 $C = 0.5 \\sum w_i$），避免无意义的过松或过紧；\n",
    "- 采用固定的随机种子拼接策略，保证\"同一 (类, repeat, sample)\"跨算法完全一致；\n",
    "- 控制 DP 的规模：仅在 $n, C$ 适中时启用（代码中做阈值限制），否则退化为 LP 上界评估。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffd9b75c38117fda"
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第三节：实验设计 —— 可直接运行的实现代码（加入 GreedyTrap-ADV，稳定哈希，所有实例族均能拉开差距）\n",
    "# 说明：\n",
    "# 1) 只实现并调用两种算法：GreedyDensity 与 Greedy+Max；\n",
    "# 2) 构建 ≥5 类实例（本代码给 6+1 类 + 新的 GreedyTrap-ADV），\n",
    "#    每类 20 样本 × 5 次随机重复；\n",
    "# 3) 小/中规模用 DP 求 OPT，大规模用 LP（分数背包）上界；\n",
    "# 4) 保存原始结果到 CSV，后续章节再做统计与可视化。\n",
    "# 5) 使用跨平台稳定哈希生成随机种子，保证完全可复现。\n",
    "# 6) 关键改动：各生成器采用“近容量对抗模板”，确保 Greedy+Max 与 GreedyDensity 呈现可观察差别。\n",
    "\n",
    "import math, random, time, os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zlib\n",
    "\n",
    "# ---------- 稳定哈希（用于可复现的随机种子） ----------\n",
    "def stable_hash_str(*parts) -> int:\n",
    "    \"\"\"跨平台/跨进程稳定的32位哈希，用于生成可复现的随机种子。\"\"\"\n",
    "    s = \"|\".join(str(p) for p in parts)\n",
    "    return zlib.adler32(s.encode(\"utf-8\")) & 0xffffffff\n",
    "\n",
    "# ---------- 随机与输出设置 ----------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "OUTDIR = \"outputs_exp_section3\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- 两种算法 ----------\n",
    "@dataclass\n",
    "class Item:\n",
    "    w: int\n",
    "    v: int\n",
    "    idx: int\n",
    "\n",
    "def greedy_density(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"标准贪心：按 v/w 从大到小排序，能放就放。\"\"\"\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v / x.w, x.v), reverse=True)\n",
    "    tw = 0\n",
    "    tv = 0\n",
    "    chosen = []\n",
    "    for it in items_sorted:\n",
    "        if tw + it.w <= C:\n",
    "            tw += it.w\n",
    "            tv += it.v\n",
    "            chosen.append(it.idx)\n",
    "    return tv, chosen\n",
    "\n",
    "def greedy_plus_max(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"Greedy+Max（BYOG+Max 在线性 0/1 背包的特化）：贪心解 vs 单个最佳可装物品，取较优者。\"\"\"\n",
    "    g_val, g_set = greedy_density(items, C)\n",
    "    best_single = None\n",
    "    for it in items:\n",
    "        if it.w <= C and (best_single is None or it.v > best_single.v):\n",
    "            best_single = it\n",
    "    if best_single is None:\n",
    "        return g_val, g_set\n",
    "    if best_single.v > g_val:\n",
    "        return best_single.v, [best_single.idx]\n",
    "    return g_val, g_set\n",
    "\n",
    "ALGOS: Dict[str, Callable[[List[Item], int], Tuple[int, List[int]]]] = {\n",
    "    \"GreedyDensity\": greedy_density,\n",
    "    \"Greedy+Max\": greedy_plus_max,\n",
    "}\n",
    "\n",
    "# ---------- 基准：DP 与 LP 上界 ----------\n",
    "def dp_optimal(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"容量维度 DP，O(nC)。规模过大时仅返回值，不回溯。\"\"\"\n",
    "    n = len(items)\n",
    "    dp = [0]*(C+1)\n",
    "    if (n+1)*(C+1) <= 2_000_000:\n",
    "        take = [[False]*(C+1) for _ in range(n)]\n",
    "        for i, it in enumerate(items):\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "                    take[i][c] = True\n",
    "        c = max(range(C+1), key=lambda x: dp[x])\n",
    "        chosen = []\n",
    "        for i in range(n-1, -1, -1):\n",
    "            if take[i][c]:\n",
    "                chosen.append(items[i].idx)\n",
    "                c -= items[i].w\n",
    "        return max(dp), chosen[::-1]\n",
    "    else:\n",
    "        for it in items:\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "        return max(dp), []\n",
    "\n",
    "def fractional_upper_bound(items: List[Item], C: int) -> float:\n",
    "    \"\"\"LP/分数背包上界：按 v/w 排序，最后一个物品按比例填充。\"\"\"\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v / x.w, x.v), reverse=True)\n",
    "    tw = 0\n",
    "    tv = 0.0\n",
    "    for it in items_sorted:\n",
    "        if tw + it.w <= C:\n",
    "            tw += it.w\n",
    "            tv += it.v\n",
    "        else:\n",
    "            remain = C - tw\n",
    "            if remain > 0:\n",
    "                tv += (it.v / it.w) * remain\n",
    "            break\n",
    "    return tv\n",
    "\n",
    "# ---------- 统一的“近容量对抗”模板工具 ----------\n",
    "def _build_near_capacity_items(n:int, rng:random.Random, C:int,\n",
    "                               w_lo:float=0.55, w_hi:float=0.75) -> List[int]:\n",
    "    \"\"\"生成 n 个‘近容量’重量，确保任何两件都无法同时装入（每件 > 0.5C）。\"\"\"\n",
    "    ws = [rng.randint(int(w_lo*C), int(w_hi*C)) for _ in range(n)]\n",
    "    return ws\n",
    "\n",
    "def _finish_with_big_item(items_small: List[Item], C:int, rng:random.Random,\n",
    "                          density_eps:float=0.01, big_w_ratio:float=0.95) -> List[Item]:\n",
    "    \"\"\"在小件基础上添加一个‘大件’：\n",
    "       - 大件重量 = 0.95C（保证任何小件入袋后就装不进大件）\n",
    "       - 大件密度 = 最佳小件密度 * (1 - eps)，保证排序上大件靠后\n",
    "       - 大件价值  = ceil(d_big * w_big)，且自然 > 最佳小件价值（因为 w_big >> w_small）\n",
    "    \"\"\"\n",
    "    # 找到最佳小件（密度最高者）\n",
    "    best_small = max(items_small, key=lambda it: it.v / it.w)\n",
    "    d_best = best_small.v / best_small.w\n",
    "    w_big = int(max(1, round(big_w_ratio * C)))\n",
    "    # 保证大件密度略低于最佳小件\n",
    "    d_big = max(1e-9, d_best * (1.0 - density_eps))\n",
    "    v_big = int(math.ceil(d_big * w_big))\n",
    "    # 由于 w_big ≈ 0.95C、w_small ∈ [0.55C,0.75C]，即使 d_big < d_best，也几乎总是 v_big > v_best_small\n",
    "    items = items_small + [Item(w=w_big, v=max(1, v_big), idx=len(items_small))]\n",
    "    return items\n",
    "\n",
    "# ---------- 实例生成器（全部采用“近容量对抗模板”，确保可观察差异） ----------\n",
    "# 说明：以下生成器均忽略 cap_ratio，使用固定 C_base（与 GreedyTrap-ADV 一致的做法），\n",
    "#       但各自通过 v 的生成方式保留“随机/相关/反相关/重尾/陷阱”等分布风格。\n",
    "\n",
    "C_BASE_DEFAULT = 1000\n",
    "\n",
    "def gen_uncorrelated(n:int, seed:int, cap_ratio=0.5,\n",
    "                     C_base:int=C_BASE_DEFAULT, v_lo:int=1, v_hi:int=1000):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 先造 n-1 个‘近容量小件’，v 与 w 独立均匀\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [rng.randint(v_lo, v_hi) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_weakly_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                          C_base:int=C_BASE_DEFAULT, noise:int=20):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [max(1, w + rng.randint(-noise, noise)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_strongly_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                            C_base:int=C_BASE_DEFAULT, delta:int=40):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [w + delta for w in ws]  # 强相关：v = w + Δ\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_inversely_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                             C_base:int=C_BASE_DEFAULT):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    Wmax = max(ws)\n",
    "    # 反相关：越轻越高价值，这里区间仍在 [0.55C,0.75C]，相对差异体现在密度\n",
    "    vs = [max(1, Wmax - w + rng.randint(0, 20)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_heavy_tailed(n:int, seed:int, cap_ratio=0.5,\n",
    "                     C_base:int=C_BASE_DEFAULT, pareto_alpha:float=2.0):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 重尾权重：先用 Pareto 采样，再裁剪到 [0.55C,0.75C]，保留“右偏”的味道\n",
    "    ws = []\n",
    "    for _ in range(n-1):\n",
    "        raw = int(C * min(0.95, max(0.55, rng.paretovariate(pareto_alpha) / (2.0*5.0))))\n",
    "        # 简化：把原始重尾缩放到目标区间（经验系数），再夹断\n",
    "        w = int(min(max(0.55*C, raw), 0.75*C))\n",
    "        ws.append(w)\n",
    "    # 价值仍取较宽的均匀，强化密度差异的随机性\n",
    "    vs = [rng.randint(200, 1200) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.015, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_greedy_trap(n:int, seed:int, cap_ratio=0.5,\n",
    "                    C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"经典陷阱：小件密度略高但每件都很重（>0.5C），只能拿 1 件；大件密度略低却更值钱。\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 小件密度略高：v ≈ 1.02 * w + 小扰动\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [int(math.ceil(1.02 * w + rng.uniform(-0.01, 0.01) * C)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    # 大件密度略低于最佳小件：eps 调大一点以强化陷阱效果\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.02, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "# === 已有：可调难度的“严格对抗”生成器（GreedyTrap-ADV） ===\n",
    "ADV_DELTA = 0.003   # 对抗难度：越小越“温和”、越大越容易区分；可按需调 0.001~0.01\n",
    "ADV_C_BASE = 1000   # 容量标尺\n",
    "\n",
    "def gen_greedy_trap_adv(n:int, seed:int, cap_ratio:float=0.5,\n",
    "                        delta:float=ADV_DELTA, C_base:int=ADV_C_BASE,\n",
    "                        w_lo:float=0.90, w_hi:float=0.98,\n",
    "                        d_small:float=1.01, noise_frac:float=0.002):\n",
    "    \"\"\"\n",
    "    严格对抗（确保差异显著）：(n-1) 小件每件 > 0.9C，密度略高；1 个大件重为 C，密度略低但价值更高。\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    n_small = max(1, n-1)\n",
    "    items: List[Item] = []\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac) * C\n",
    "        v = int(math.ceil(d_small * w + noise))\n",
    "        items.append(Item(w=max(1, w), v=max(1, v), idx=i))\n",
    "    d_big = 0.95 * d_small + delta\n",
    "    v_big = int(math.ceil(d_big * C))\n",
    "    items.append(Item(w=C, v=max(1, v_big), idx=n_small))\n",
    "    return items, C\n",
    "\n",
    "# 大规模类：沿用“近容量模板”，但 n 更大、DP 关闭\n",
    "def gen_uncorrelated_large(n:int, seed:int, cap_ratio=0.5,\n",
    "                           C_base:int=C_BASE_DEFAULT):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [rng.randint(1, 1000) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "GENS = {\n",
    "    \"Uncorrelated\": gen_uncorrelated,\n",
    "    \"WeaklyCorr\": gen_weakly_correlated,\n",
    "    \"StronglyCorr\": gen_strongly_correlated,\n",
    "    \"InverseCorr\": gen_inversely_correlated,\n",
    "    \"HeavyTailed\": gen_heavy_tailed,\n",
    "    \"GreedyTrap\": gen_greedy_trap,\n",
    "    \"GreedyTrap-ADV\": gen_greedy_trap_adv,\n",
    "    \"Uncorrelated-Large\": gen_uncorrelated_large,\n",
    "}\n",
    "\n",
    "# ---------- 单实例评测 ----------\n",
    "def run_instance(items: List[Item], C: int, allow_dp: bool=True):\n",
    "    \"\"\"在单个实例上评测两算法，返回记录（含 runtime 与 gap）。\"\"\"\n",
    "    n = len(items)\n",
    "    total_w = sum(it.w for it in items)\n",
    "\n",
    "    # 选择基线：小/中规模用 DP 得 OPT；否则用 LP 上界\n",
    "    if allow_dp and n <= 200 and C <= 6000:\n",
    "        t0 = time.perf_counter()\n",
    "        opt_val, _ = dp_optimal(items, C)\n",
    "        opt_time = time.perf_counter() - t0\n",
    "        opt_known = True\n",
    "        UB = float(opt_val)\n",
    "    else:\n",
    "        opt_val = None\n",
    "        opt_time = None\n",
    "        opt_known = False\n",
    "        UB = fractional_upper_bound(items, C)\n",
    "\n",
    "    rows = []\n",
    "    for name, algo in ALGOS.items():\n",
    "        t0 = time.perf_counter()\n",
    "        val, _ = algo(items, C)\n",
    "        runtime = time.perf_counter() - t0\n",
    "        if opt_known and UB > 0:\n",
    "            gap = (opt_val - val) / opt_val if opt_val > 0 else 0.0\n",
    "        else:\n",
    "            gap = (UB - val) / UB if UB > 0 else 0.0\n",
    "        rows.append({\n",
    "            \"algo\": name,\n",
    "            \"value\": int(val),\n",
    "            \"runtime_s\": float(runtime),\n",
    "            \"gap\": float(gap),\n",
    "            \"opt_known\": bool(opt_known),\n",
    "            \"opt_time_s\": (float(opt_time) if opt_time is not None else None),\n",
    "            \"capacity\": int(C),\n",
    "            \"n\": int(n),\n",
    "            \"sum_w\": int(total_w),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# ---------- 主实验循环 ----------\n",
    "def run_benchmark(seed_base: int = 2025):\n",
    "    classes = [\n",
    "        (\"Uncorrelated\",       dict(n=60)),\n",
    "        (\"WeaklyCorr\",         dict(n=150)),\n",
    "        (\"StronglyCorr\",       dict(n=150)),\n",
    "        (\"InverseCorr\",        dict(n=150)),\n",
    "        (\"HeavyTailed\",        dict(n=150)),\n",
    "        (\"GreedyTrap\",         dict(n=150)),\n",
    "        (\"GreedyTrap-ADV\",     dict(n=13)),   # 12 小件 + 1 大件，严格对抗\n",
    "        (\"Uncorrelated-Large\", dict(n=500)),  # 大规模类，不做 DP\n",
    "    ]\n",
    "    REPEATS = 5\n",
    "    SAMPLES_PER_REPEAT = 20\n",
    "\n",
    "    rows = []\n",
    "    for cls_name, params in classes:\n",
    "        gen = GENS[params.get(\"gen\", cls_name)]\n",
    "        for rep in range(REPEATS):\n",
    "            for s in range(SAMPLES_PER_REPEAT):\n",
    "                # 组合种子：确保同 (类, rep, s) 在两算法间一致（使用稳定哈希）\n",
    "                seed = seed_base + (stable_hash_str(cls_name, rep, s) % 10_000_000)\n",
    "                items, C = gen(params[\"n\"], seed)\n",
    "                allow_dp = (cls_name != \"Uncorrelated-Large\")\n",
    "                res = run_instance(items, C, allow_dp=allow_dp)\n",
    "                for r in res:\n",
    "                    r2 = {\"class\": cls_name, \"rep\": rep, \"sample\": s}\n",
    "                    r2.update(r)\n",
    "                    rows.append(r2)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_csv = os.path.join(OUTDIR, \"raw_results_section3.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Results saved to: {out_csv}\")\n",
    "    print(df.head(3))\n",
    "    return df\n",
    "\n",
    "# 实际执行\n",
    "if __name__ == \"__main__\":\n",
    "    df = run_benchmark()\n",
    "\n",
    "    # 作为“本节验证”，给出简单的均值/方差（正式统计将在后续章节展开）\n",
    "    summary = (\n",
    "        df.groupby([\"class\", \"algo\"])\n",
    "          .agg(mean_gap=(\"gap\",\"mean\"), var_gap=(\"gap\",\"var\"),\n",
    "               mean_time=(\"runtime_s\",\"mean\"), var_time=(\"runtime_s\",\"var\"),\n",
    "               cnt=(\"gap\",\"size\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    out_sum = os.path.join(OUTDIR, \"summary_preview_section3.csv\")\n",
    "    summary.to_csv(out_sum, index=False)\n",
    "    print(f\"[OK] Preview summary saved to: {out_sum}\")\n",
    "    print(summary.head(12))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-12T14:57:02.755503Z",
     "start_time": "2025-11-12T14:56:59.889356Z"
    }
   },
   "id": "81bba2796f4bfe3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Results saved to: outputs_exp_section3\\raw_results_section3.csv\n",
      "          class  rep  sample           algo  value  runtime_s       gap  \\\n",
      "0  Uncorrelated    0       0  GreedyDensity    889   0.000014  0.405749   \n",
      "1  Uncorrelated    0       0     Greedy+Max   1496   0.000012  0.000000   \n",
      "2  Uncorrelated    0       1  GreedyDensity    930   0.000009  0.401544   \n",
      "\n",
      "   opt_known  opt_time_s  capacity   n  sum_w  \n",
      "0       True    0.000847      1000  60  39836  \n",
      "1       True    0.000847      1000  60  39836  \n",
      "2       True    0.000912      1000  60  38509  \n",
      "[OK] Preview summary saved to: outputs_exp_section3\\summary_preview_section3.csv\n",
      "             class           algo  mean_gap       var_gap  mean_time  \\\n",
      "0       GreedyTrap     Greedy+Max  0.000000  0.000000e+00   0.000027   \n",
      "1       GreedyTrap  GreedyDensity  0.381280  5.586892e-04   0.000023   \n",
      "2   GreedyTrap-ADV     Greedy+Max  0.000000  0.000000e+00   0.000002   \n",
      "3   GreedyTrap-ADV  GreedyDensity  0.035218  5.437130e-04   0.000002   \n",
      "4      HeavyTailed     Greedy+Max  0.000000  0.000000e+00   0.000027   \n",
      "5      HeavyTailed  GreedyDensity  0.412379  6.304980e-09   0.000023   \n",
      "6      InverseCorr     Greedy+Max  0.000000  0.000000e+00   0.000026   \n",
      "7      InverseCorr  GreedyDensity  0.413298  6.168328e-06   0.000023   \n",
      "8     StronglyCorr     Greedy+Max  0.000000  0.000000e+00   0.000027   \n",
      "9     StronglyCorr  GreedyDensity  0.414410  1.607699e-06   0.000024   \n",
      "10    Uncorrelated     Greedy+Max  0.000000  0.000000e+00   0.000010   \n",
      "11    Uncorrelated  GreedyDensity  0.382008  7.492075e-04   0.000008   \n",
      "\n",
      "        var_time  cnt  \n",
      "0   1.741073e-13  100  \n",
      "1   5.594143e-13  100  \n",
      "2   6.849065e-15  100  \n",
      "3   1.610391e-13  100  \n",
      "4   1.855558e-13  100  \n",
      "5   6.500337e-13  100  \n",
      "6   2.026077e-13  100  \n",
      "7   2.224960e-12  100  \n",
      "8   1.853437e-13  100  \n",
      "9   9.969414e-13  100  \n",
      "10  2.107255e-13  100  \n",
      "11  3.936686e-13  100  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 性能评估指标（围绕 GreedyDensity 与 Greedy+Max）\n",
    "\n",
    "**目标**  \n",
    "从\"效率、质量与稳定性/可扩展性\"三个维度系统评估两种贪心算法，并给出统计量与置信区间，必要时补充内存开销与可扩展性度量。\n",
    "\n",
    "### 评估维度与定义\n",
    "1) **运行时间（Time）**\n",
    "- 指标：每次算法调用的 wall-clock（秒）。\n",
    "- 统计：均值、方差（或标准差）、95% 置信区间。\n",
    "- 说明：用于衡量效率；后续将与理论 $O(n\\log n)$ 对比。\n",
    "\n",
    "2) **解质量（Quality）**\n",
    "- 指标：\n",
    "  - 有最优值（OPT）时：$\\mathrm{gap}=(\\mathrm{OPT}-\\mathrm{ALG})/\\mathrm{OPT}$；\n",
    "  - 否则用 LP/分数背包上界（UB）：$\\mathrm{gap}_{\\mathrm{UB}}=(\\mathrm{UB}-\\mathrm{ALG})/\\mathrm{UB}$。\n",
    "- 统计：均值、方差、95% 置信区间。gap 越低越好。\n",
    "- 说明：衡量与最优/上界的差距。\n",
    "\n",
    "3) **稳定性/收敛速度（Stability / Convergence）**\n",
    "- 指标：跨样本/重复的方差（或标准差）与 95%CI；\n",
    "- 说明：方差越小代表结果越稳；在重复次数增大时 CI 收缩可视作\"收敛更快\"。\n",
    "\n",
    "4) **内存、能耗代理与可扩展性**\n",
    "- **内存消耗（Memory）**：以 Python `tracemalloc` 记录每次调用的**峰值内存**（字节），统计均值与方差。\n",
    "- **能耗代理（Energy proxy）**：在同一硬件/负载下，以\"运行时间\"作为归一化能耗代理（时间$\\propto$能耗）。\n",
    "- **可扩展性（Scalability）**：做回归 $\\text{time}\\approx a\\cdot n\\log_2 n+b$，给出 $a,b,R^2$；$R^2$ 越高说明与理论复杂度越一致。\n",
    "\n",
    "> 数据来源：第 3 节已生成 `outputs_exp_section3/raw_results_section3.csv`（每类 ≥20 个样本、≥5 次重复），本节在此基础上计算统计量与 CI，并**额外运行小规模内存基准**以报告峰值内存。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f02ef4f10003952d"
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第四节：性能评估指标实现代码\n",
    "# 说明：\n",
    "# 1) 统计：保持均值/方差/95%CI 计算不变，自动覆盖新增类 GreedyTrap-ADV；\n",
    "# 2) 可扩展性回归：保留“全量回归”与“仅用多规模集合（Uncorrelated & Uncorrelated-Large）回归”；\n",
    "# 3) 内存峰值：tracemalloc + 多次重复（默认 1000 次）放大信号；加入 GreedyTrap-ADV 的内存测试；\n",
    "# 4) 能耗代理：仍以运行时间作为归一化能耗代理。\n",
    "# 注意：本节仅使用第三节生成的 outputs_exp_section3/raw_results_section3.csv。\n",
    "\n",
    "import os, math, random, time, tracemalloc, zlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- 稳定哈希（用于可复现随机种子） ----------\n",
    "def stable_hash_str(*parts) -> int:\n",
    "    \"\"\"跨平台稳定的32位哈希（Adler-32），用于生成可复现的随机种子。\"\"\"\n",
    "    s = \"|\".join(str(p) for p in parts)\n",
    "    return zlib.adler32(s.encode(\"utf-8\")) & 0xffffffff\n",
    "\n",
    "# ---------- 路径与输出 ----------\n",
    "RAW_PATH = \"outputs_exp_section3/raw_results_section3.csv\"\n",
    "OUTDIR = \"outputs_metrics_section4\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"未发现 {RAW_PATH} 。请先运行第 3 节代码以生成原始结果，再执行本节。\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# ---------- 基本统计：均值/方差/95%CI ----------\n",
    "def ci95(series: pd.Series) -> float:\n",
    "    \"\"\"返回均值的 95% 置信区间半径：1.96*s/sqrt(n)（n>1 时）。\"\"\"\n",
    "    s = float(series.std(ddof=1))\n",
    "    n = int(series.size)\n",
    "    return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "\n",
    "agg_rows = []\n",
    "for (cls, algo), sub in df.groupby([\"class\", \"algo\"]):\n",
    "    mean_time = float(sub[\"runtime_s\"].mean())\n",
    "    var_time  = float(sub[\"runtime_s\"].var(ddof=1)) if len(sub)>1 else 0.0\n",
    "    ci_time   = ci95(sub[\"runtime_s\"])\n",
    "\n",
    "    mean_gap  = float(sub[\"gap\"].mean())\n",
    "    var_gap   = float(sub[\"gap\"].var(ddof=1)) if len(sub)>1 else 0.0\n",
    "    ci_gap    = ci95(sub[\"gap\"])\n",
    "\n",
    "    opt_ratio = float(sub[\"opt_known\"].sum() / len(sub))\n",
    "\n",
    "    agg_rows.append({\n",
    "        \"class\": cls, \"algo\": algo,\n",
    "        \"mean_runtime_s\": mean_time, \"var_runtime\": var_time, \"ci95_runtime\": ci_time,\n",
    "        \"mean_gap\": mean_gap, \"var_gap\": var_gap, \"ci95_gap\": ci_gap,\n",
    "        \"opt_ratio\": opt_ratio,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "agg = pd.DataFrame(agg_rows).sort_values([\"class\",\"algo\"])\n",
    "agg_path = os.path.join(OUTDIR, \"metrics_agg.csv\")\n",
    "agg.to_csv(agg_path, index=False)\n",
    "print(f\"[OK] metrics_agg.csv -> {agg_path}\")\n",
    "print(agg.head(12))\n",
    "\n",
    "# ---------- 可扩展性：time ~ a*(n*log2 n)+b ----------\n",
    "def fit_linreg_1d(X: np.ndarray, y: np.ndarray) -> Tuple[float,float,float]:\n",
    "    \"\"\"\n",
    "    简单线性回归（最小二乘）：y = a*X + b\n",
    "    返回 a, b, R^2\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1,1)\n",
    "    A = np.hstack([X, np.ones_like(X)])\n",
    "    coef, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n",
    "    a, b = float(coef[0]), float(coef[1])\n",
    "    y_hat = A @ coef\n",
    "    ss_res = float(np.sum((y - y_hat)**2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y))**2))\n",
    "    r2 = 1.0 - ss_res/ss_tot if ss_tot > 0 else 0.0\n",
    "    return a, b, r2\n",
    "\n",
    "# 4.1 全量数据的回归（用于总体趋势对比；包含新增的 GreedyTrap-ADV）\n",
    "scal_rows_full = []\n",
    "for algo in sorted(df[\"algo\"].unique()):\n",
    "    sub = df[df[\"algo\"]==algo].copy()\n",
    "    X = np.array([n * math.log2(n) for n in sub[\"n\"].values], dtype=float)\n",
    "    y = sub[\"runtime_s\"].values.astype(float)\n",
    "    a, b, r2 = fit_linreg_1d(X, y)\n",
    "    scal_rows_full.append({\"algo\": algo, \"a_nlogn\": a, \"b\": b, \"R2\": r2, \"n_samples\": len(sub)})\n",
    "\n",
    "scal_full = pd.DataFrame(scal_rows_full)\n",
    "scal_full_path = os.path.join(OUTDIR, \"scalability_fit_full.csv\")\n",
    "scal_full.to_csv(scal_full_path, index=False)\n",
    "print(f\"[OK] scalability_fit_full.csv -> {scal_full_path}\")\n",
    "print(scal_full)\n",
    "\n",
    "# 4.2 仅使用“多规模集合”的回归（Uncorrelated & Uncorrelated-Large）\n",
    "#     ——避免固定 n 的类别（含 GreedyTrap-ADV）稀释回归的判别力\n",
    "scal_rows_subset = []\n",
    "subset = df[df[\"class\"].isin([\"Uncorrelated\", \"Uncorrelated-Large\"])].copy()\n",
    "for algo in sorted(subset[\"algo\"].unique()):\n",
    "    sub = subset[subset[\"algo\"]==algo]\n",
    "    X = np.array([n * math.log2(n) for n in sub[\"n\"].values], dtype=float)\n",
    "    y = sub[\"runtime_s\"].values.astype(float)\n",
    "    a, b, r2 = fit_linreg_1d(X, y)\n",
    "    scal_rows_subset.append({\"algo\": algo, \"a_nlogn\": a, \"b\": b, \"R2\": r2, \"n_samples\": len(sub)})\n",
    "\n",
    "scal_subset = pd.DataFrame(scal_rows_subset)\n",
    "scal_subset_path = os.path.join(OUTDIR, \"scalability_fit_subset.csv\")\n",
    "scal_subset.to_csv(scal_subset_path, index=False)\n",
    "print(f\"[OK] scalability_fit_subset.csv -> {scal_subset_path}\")\n",
    "print(scal_subset)\n",
    "\n",
    "# ---------- 额外：内存消耗（tracemalloc 峰值，放大法） ----------\n",
    "\n",
    "@dataclass\n",
    "class Item:\n",
    "    w: int\n",
    "    v: int\n",
    "    idx: int\n",
    "\n",
    "def greedy_density(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v / x.w, x.v), reverse=True)\n",
    "    tw = 0; tv = 0; chosen = []\n",
    "    for it in items_sorted:\n",
    "        if tw + it.w <= C:\n",
    "            tw += it.w; tv += it.v; chosen.append(it.idx)\n",
    "    return tv, chosen\n",
    "\n",
    "def greedy_plus_max(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    g_val, g_set = greedy_density(items, C)\n",
    "    best_single = None\n",
    "    for it in items:\n",
    "        if it.w <= C and (best_single is None or it.v > best_single.v):\n",
    "            best_single = it\n",
    "    if best_single is None:\n",
    "        return g_val, g_set\n",
    "    return (best_single.v, [best_single.idx]) if best_single.v > g_val else (g_val, g_set)\n",
    "\n",
    "# ===== 与第 3 节一致的实例生成器（含 GreedyTrap-ADV） =====\n",
    "def gen_uncorrelated(n:int, seed:int, w_range=(1,100), v_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [rng.randint(*v_range) for _ in range(n)]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws))\n",
    "    return items, C\n",
    "\n",
    "def gen_weakly_correlated(n:int, seed:int, w_range=(1,100), noise=10, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [max(1, w + rng.randint(-noise, noise)) for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_strongly_correlated(n:int, seed:int, w_range=(1,100), delta=20, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [w + delta for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_inversely_correlated(n:int, seed:int, w_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    Wmax = max(ws)\n",
    "    vs = [max(1, Wmax - w + rng.randint(0, 10)) for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_heavy_tailed(n:int, seed:int, pareto_alpha=2.0, w_scale=50, v_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [max(1, int(w_scale * (rng.paretovariate(pareto_alpha)))) for _ in range(n)]\n",
    "    vs = [rng.randint(*v_range) for _ in range(n)]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_greedy_trap(n:int, seed:int, w_range=(1,50), big_item_factor=6, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n-1)]\n",
    "    vs = [w + rng.randint(10, 20) for w in ws]\n",
    "    big_w = int(sum(ws) * cap_ratio)\n",
    "    big_v = int(big_item_factor * sum(vs) / n)\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n-1)] + [Item(big_w, max(big_v, 1), n-1)]\n",
    "    C = int(cap_ratio * (sum(ws) + big_w))\n",
    "    return items, C\n",
    "\n",
    "# --- 新增：严格对抗生成器（与第三节保持一致） ---\n",
    "ADV_DELTA = 0.003\n",
    "ADV_C_BASE = 1000\n",
    "\n",
    "def gen_greedy_trap_adv(n:int, seed:int, cap_ratio:float=0.5,\n",
    "                        delta:float=ADV_DELTA, C_base:int=ADV_C_BASE,\n",
    "                        w_lo:float=0.90, w_hi:float=0.98,\n",
    "                        d_small:float=1.01, noise_frac:float=0.002):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    n_small = max(1, n-1)\n",
    "    items: List[Item] = []\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac) * C\n",
    "        v = int(math.ceil(d_small * w + noise))\n",
    "        items.append(Item(w=max(1, w), v=max(1, v), idx=i))\n",
    "    d_big = 0.95 * d_small + delta\n",
    "    v_big = int(math.ceil(d_big * C))\n",
    "    items.append(Item(w=C, v=max(1, v_big), idx=n_small))\n",
    "    return items, C\n",
    "\n",
    "GENS: Dict[str, Callable[..., Tuple[List[Item], int]]] = {\n",
    "    \"Uncorrelated\": gen_uncorrelated,\n",
    "    \"WeaklyCorr\": gen_weakly_correlated,\n",
    "    \"StronglyCorr\": gen_strongly_correlated,\n",
    "    \"InverseCorr\": gen_inversely_correlated,\n",
    "    \"HeavyTailed\": gen_heavy_tailed,\n",
    "    \"GreedyTrap\": gen_greedy_trap,\n",
    "    \"GreedyTrap-ADV\": gen_greedy_trap_adv,   # << 新增\n",
    "}\n",
    "\n",
    "ALGOS: Dict[str, Callable[[List[Item], int], Tuple[int, List[int]]]] = {\n",
    "    \"GreedyDensity\": greedy_density,\n",
    "    \"Greedy+Max\": greedy_plus_max,\n",
    "}\n",
    "\n",
    "def peak_memory_bytes(func, items: List[Item], C: int, reps: int = 1000) -> int:\n",
    "    \"\"\"\n",
    "    用 tracemalloc 测峰值内存（字节），通过重复调用放大可观测信号。\n",
    "    \"\"\"\n",
    "    tracemalloc.start()\n",
    "    try:\n",
    "        dummy = 0  # 防止优化，累加返回值\n",
    "        for _ in range(reps):\n",
    "            val, chosen = func(items, C)\n",
    "            dummy += (val if chosen is not None else 0)\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "    finally:\n",
    "        tracemalloc.stop()\n",
    "    return int(peak)\n",
    "\n",
    "# 采样内存测试（每类抽样 S 个实例；不测 Uncorrelated-Large）\n",
    "MEM_SAMPLES_PER_CLASS = 10\n",
    "MEM_REPS = 1000  # 重复调用次数，用于放大峰值\n",
    "mem_rows = []\n",
    "mem_classes = [\n",
    "    (\"Uncorrelated\",   dict(n=60,  cap_ratio=0.5)),\n",
    "    (\"WeaklyCorr\",     dict(n=150, cap_ratio=0.5)),\n",
    "    (\"StronglyCorr\",   dict(n=150, cap_ratio=0.5)),\n",
    "    (\"InverseCorr\",    dict(n=150, cap_ratio=0.5)),\n",
    "    (\"HeavyTailed\",    dict(n=150, cap_ratio=0.5)),\n",
    "    (\"GreedyTrap\",     dict(n=150, cap_ratio=0.5)),\n",
    "    # 新增：严格对抗类，使用与第三节一致的小规模与固定容量\n",
    "    (\"GreedyTrap-ADV\", dict(n=13,  cap_ratio=0.5)),\n",
    "]\n",
    "\n",
    "seed_base = 24601\n",
    "for cls_name, params in mem_classes:\n",
    "    gen = GENS[cls_name]\n",
    "    for s in range(MEM_SAMPLES_PER_CLASS):\n",
    "        # 使用稳定哈希生成可复现种子（替换原先的不稳定 hash(...)）\n",
    "        seed = seed_base + (stable_hash_str(cls_name, s) % 10_000_000)\n",
    "        items, C = gen(params[\"n\"], seed, cap_ratio=params[\"cap_ratio\"])\n",
    "        for algo_name, algo in ALGOS.items():\n",
    "            peak_b = peak_memory_bytes(algo, items, C, reps=MEM_REPS)\n",
    "            mem_rows.append({\n",
    "                \"class\": cls_name, \"algo\": algo_name, \"n\": params[\"n\"],\n",
    "                \"capacity\": C, \"reps\": MEM_REPS, \"peak_bytes\": peak_b\n",
    "            })\n",
    "\n",
    "mem_df = pd.DataFrame(mem_rows)\n",
    "mem_summary = mem_df.groupby([\"class\",\"algo\"]).agg(\n",
    "    mean_peak_bytes=(\"peak_bytes\",\"mean\"),\n",
    "    std_peak_bytes=(\"peak_bytes\",\"std\"),\n",
    "    count=(\"peak_bytes\",\"size\")\n",
    ").reset_index()\n",
    "mem_path = os.path.join(OUTDIR, \"memory_summary.csv\")\n",
    "mem_summary.to_csv(mem_path, index=False)\n",
    "print(f\"[OK] memory_summary.csv -> {mem_path}\")\n",
    "print(mem_summary.head(14))\n",
    "\n",
    "# ---------- 可选：能耗代理（=运行时间） ----------\n",
    "# 在相同硬件与负载下，运行时间可作为归一化能耗代理（越短越省）\n",
    "energy = (\n",
    "    df.groupby([\"class\",\"algo\"])[\"runtime_s\"]\n",
    "      .agg(energy_proxy_mean=\"mean\", energy_proxy_std=\"std\", count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "energy_path = os.path.join(OUTDIR, \"energy_proxy.csv\")\n",
    "energy.to_csv(energy_path, index=False)\n",
    "print(f\"[OK] energy_proxy.csv -> {energy_path}\")\n",
    "\n",
    "print(\"\\n完成：本节已生成以下文件：\")\n",
    "for p in [agg_path, scal_full_path, scal_subset_path, mem_path, energy_path]:\n",
    "    print(\" -\", p)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-12T15:07:05.960821Z",
     "start_time": "2025-11-12T15:06:51.944594Z"
    }
   },
   "id": "2c639ff2a98c1641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] metrics_agg.csv -> outputs_metrics_section4\\metrics_agg.csv\n",
      "             class           algo  mean_runtime_s   var_runtime  ci95_runtime  \\\n",
      "0       GreedyTrap     Greedy+Max        0.000027  1.741073e-13  8.178329e-08   \n",
      "1       GreedyTrap  GreedyDensity        0.000023  5.594143e-13  1.465962e-07   \n",
      "2   GreedyTrap-ADV     Greedy+Max        0.000002  6.849065e-15  1.622078e-08   \n",
      "3   GreedyTrap-ADV  GreedyDensity        0.000002  1.610391e-13  7.865416e-08   \n",
      "4      HeavyTailed     Greedy+Max        0.000027  1.855558e-13  8.442933e-08   \n",
      "5      HeavyTailed  GreedyDensity        0.000023  6.500337e-13  1.580244e-07   \n",
      "6      InverseCorr     Greedy+Max        0.000026  2.026077e-13  8.822345e-08   \n",
      "7      InverseCorr  GreedyDensity        0.000023  2.224960e-12  2.923595e-07   \n",
      "8     StronglyCorr     Greedy+Max        0.000027  1.853437e-13  8.438105e-08   \n",
      "9     StronglyCorr  GreedyDensity        0.000024  9.969414e-13  1.957000e-07   \n",
      "10    Uncorrelated     Greedy+Max        0.000010  2.107255e-13  8.997351e-08   \n",
      "11    Uncorrelated  GreedyDensity        0.000008  3.936686e-13  1.229763e-07   \n",
      "\n",
      "    mean_gap       var_gap  ci95_gap  opt_ratio  count  \n",
      "0   0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "1   0.381280  5.586892e-04  0.004633        1.0    100  \n",
      "2   0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "3   0.035218  5.437130e-04  0.004570        1.0    100  \n",
      "4   0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "5   0.412379  6.304980e-09  0.000016        1.0    100  \n",
      "6   0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "7   0.413298  6.168328e-06  0.000487        1.0    100  \n",
      "8   0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "9   0.414410  1.607699e-06  0.000249        1.0    100  \n",
      "10  0.000000  0.000000e+00  0.000000        1.0    100  \n",
      "11  0.382008  7.492075e-04  0.005365        1.0    100  \n",
      "[OK] scalability_fit_full.csv -> outputs_metrics_section4\\scalability_fit_full.csv\n",
      "            algo       a_nlogn         b        R2  n_samples\n",
      "0     Greedy+Max  2.146991e-08  0.000003  0.998679        800\n",
      "1  GreedyDensity  1.792038e-08  0.000003  0.994663        800\n",
      "[OK] scalability_fit_subset.csv -> outputs_metrics_section4\\scalability_fit_subset.csv\n",
      "            algo       a_nlogn         b        R2  n_samples\n",
      "0     Greedy+Max  2.138972e-08  0.000003  0.999738        200\n",
      "1  GreedyDensity  1.797224e-08  0.000002  0.997545        200\n",
      "[OK] memory_summary.csv -> outputs_metrics_section4\\memory_summary.csv\n",
      "             class           algo  mean_peak_bytes  std_peak_bytes  count\n",
      "0       GreedyTrap     Greedy+Max           7309.6       71.955542     10\n",
      "1       GreedyTrap  GreedyDensity           7309.6       71.955542     10\n",
      "2   GreedyTrap-ADV     Greedy+Max            453.6       17.708755     10\n",
      "3   GreedyTrap-ADV  GreedyDensity            504.0        0.000000     10\n",
      "4      HeavyTailed     Greedy+Max           7193.6       68.901218     10\n",
      "5      HeavyTailed  GreedyDensity           7193.6       68.901218     10\n",
      "6      InverseCorr     Greedy+Max          21996.8    46895.758929     10\n",
      "7      InverseCorr  GreedyDensity           7165.6      124.510776     10\n",
      "8     StronglyCorr     Greedy+Max           7173.6       43.006976     10\n",
      "9     StronglyCorr  GreedyDensity           7173.6       43.006976     10\n",
      "10    Uncorrelated     Greedy+Max          16529.0    47867.396942     10\n",
      "11    Uncorrelated  GreedyDensity          16245.5    46634.557528     10\n",
      "12      WeaklyCorr     Greedy+Max           6986.4       74.480721     10\n",
      "13      WeaklyCorr  GreedyDensity           6986.4       74.480721     10\n",
      "[OK] energy_proxy.csv -> outputs_metrics_section4\\energy_proxy.csv\n",
      "\n",
      "完成：本节已生成以下文件：\n",
      " - outputs_metrics_section4\\metrics_agg.csv\n",
      " - outputs_metrics_section4\\scalability_fit_full.csv\n",
      " - outputs_metrics_section4\\scalability_fit_subset.csv\n",
      " - outputs_metrics_section4\\memory_summary.csv\n",
      " - outputs_metrics_section4\\energy_proxy.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 数据分析与可视化\n",
    "\n",
    "**目标**  \n",
    "对两种算法（GreedyDensity、Greedy+Max）进行系统的数据分析与可视化，满足以下全部要求：\n",
    "\n",
    "1) **多维性能剖析**  \n",
    "   - 设计并分析不同输入规模、容量比例与数据结构对算法性能（时间/质量/稳定性）的影响；  \n",
    "   - 输出\"优势区间图\"（capacity ratio 扫描热图）与\"性能边界总结表\"。\n",
    "\n",
    "2) **理论与实验一致性验证**\n",
    "   - 将实测运行时间与理论 $O(n\\log n)$ 比较，给出回归 $R^2$ 与偏差来源分析（缓存效应、随机性、实现差异）。\n",
    "\n",
    "3) **显著性统计分析**\n",
    "   - 对相同样本（同类、同 repeat、同 sample）下的两算法 **gap** 做配对检验（t-test 与 Wilcoxon）；\n",
    "   - 绘制 95% 置信区间图验证统计可靠性。\n",
    "\n",
    "4) **算法性能预测模型**\n",
    "   - 基于实测数据拟合 $\\text{time} \\approx a\\cdot n\\log_2 n + b$ 的经验模型；\n",
    "   - 在未测试规模（更大 $n$）上做外推预测并可视化趋势。\n",
    "\n",
    "> 数据来源：第 3 节生成的 `outputs_exp_section3/raw_results_section3.csv` 与第 4 节的统计结果（若缺失，本节将就地计算所需统计）。\n",
    "> 约定：**图表的文字使用英文**；**代码注释使用中文**；所有生成的图片与表格输出到 `outputs_analysis_section5/` 目录。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de22837d1aa182f6"
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第五节：数据分析与可视化\n",
    "# 生成输出目录：outputs_analysis_section5/*\n",
    "# 依赖：第三节 outputs_exp_section3/raw_results_section3.csv（已包含 GreedyTrap-ADV）\n",
    "\n",
    "import os, math, random, time, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# ---------------- 路径与输出 ----------------\n",
    "RAW_PATH = \"outputs_exp_section3/raw_results_section3.csv\"\n",
    "METRICS_DIR = \"outputs_metrics_section4\"\n",
    "ANALYSIS_DIR = \"outputs_analysis_section5\"\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    raise FileNotFoundError(\"未找到第3节输出：outputs_exp_section3/raw_results_section3.csv，请先运行第3节。\")\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# 若第4节聚合存在则复用\n",
    "metrics_agg_path = os.path.join(METRICS_DIR, \"metrics_agg.csv\")\n",
    "if os.path.exists(metrics_agg_path):\n",
    "    agg = pd.read_csv(metrics_agg_path)\n",
    "else:\n",
    "    def ci95(series: pd.Series) -> float:\n",
    "        s = float(series.std(ddof=1)); n = int(series.size)\n",
    "        return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "    rows = []\n",
    "    for (cls, algo), sub in df.groupby([\"class\",\"algo\"]):\n",
    "        rows.append({\n",
    "            \"class\": cls, \"algo\": algo,\n",
    "            \"mean_runtime_s\": float(sub[\"runtime_s\"].mean()),\n",
    "            \"var_runtime\": float(sub[\"runtime_s\"].var(ddof=1)) if len(sub)>1 else 0.0,\n",
    "            \"ci95_runtime\": ci95(sub[\"runtime_s\"]),\n",
    "            \"mean_gap\": float(sub[\"gap\"].mean()),\n",
    "            \"var_gap\": float(sub[\"gap\"].var(ddof=1)) if len(sub)>1 else 0.0,\n",
    "            \"ci95_gap\": ci95(sub[\"gap\"]),\n",
    "            \"opt_ratio\": float(sub[\"opt_known\"].sum()/len(sub)),\n",
    "            \"count\": int(len(sub))\n",
    "        })\n",
    "    agg = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- 公用：算法与基线 ----------------\n",
    "@dataclass\n",
    "class Item:\n",
    "    w:int; v:int; idx:int\n",
    "\n",
    "def greedy_density(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v/x.w, x.v), reverse=True)\n",
    "    tw=0; tv=0; chosen=[]\n",
    "    for it in items_sorted:\n",
    "        if tw+it.w <= C:\n",
    "            tw+=it.w; tv+=it.v; chosen.append(it.idx)\n",
    "    return tv, chosen\n",
    "\n",
    "def greedy_plus_max(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    g_val, g_set = greedy_density(items, C)\n",
    "    best_single=None\n",
    "    for it in items:\n",
    "        if it.w<=C and (best_single is None or it.v>best_single.v):\n",
    "            best_single=it\n",
    "    if best_single is None: return g_val, g_set\n",
    "    return (best_single.v,[best_single.idx]) if best_single.v>g_val else (g_val,g_set)\n",
    "\n",
    "def dp_optimal(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    n=len(items); dp=[0]*(C+1)\n",
    "    if (n+1)*(C+1)<=2_000_000:\n",
    "        take=[[False]*(C+1) for _ in range(n)]\n",
    "        for i,it in enumerate(items):\n",
    "            w,v=it.w,it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w]+v>dp[c]:\n",
    "                    dp[c]=dp[c-w]+v; take[i][c]=True\n",
    "        c=max(range(C+1), key=lambda x: dp[x]); chosen=[]\n",
    "        for i in range(n-1,-1,-1):\n",
    "            if take[i][c]: chosen.append(items[i].idx); c-=items[i].w\n",
    "        return max(dp), chosen[::-1]\n",
    "    else:\n",
    "        for it in items:\n",
    "            w,v=it.w,it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w]+v>dp[c]:\n",
    "                    dp[c]=dp[c-w]+v\n",
    "        return max(dp), []\n",
    "\n",
    "def fractional_upper_bound(items: List[Item], C: int) -> float:\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v/x.w, x.v), reverse=True)\n",
    "    tw=0; tv=0.0\n",
    "    for it in items_sorted:\n",
    "        if tw+it.w<=C:\n",
    "            tw+=it.w; tv+=it.v\n",
    "        else:\n",
    "            r=C-tw\n",
    "            if r>0: tv += (it.v/it.w)*r\n",
    "            break\n",
    "    return tv\n",
    "\n",
    "# ---------------- (A) 按类别的解质量与时间（柱状图） ----------------\n",
    "pivot_gap = agg.pivot(index=\"class\", columns=\"algo\", values=\"mean_gap\").fillna(0)\n",
    "ax = pivot_gap.plot(kind=\"bar\")\n",
    "ax.set_ylabel(\"Average relative gap (lower is better)\")\n",
    "ax.set_title(\"Quality by instance class\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_gap_by_class.png\"), dpi=160); plt.close()\n",
    "\n",
    "pivot_time = agg.pivot(index=\"class\", columns=\"algo\", values=\"mean_runtime_s\").fillna(0)\n",
    "ax = pivot_time.plot(kind=\"bar\")\n",
    "ax.set_ylabel(\"Average runtime (seconds)\")\n",
    "ax.set_title(\"Runtime by instance class\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_time_by_class.png\"), dpi=160); plt.close()\n",
    "\n",
    "# ---------------- (B) 胜率/优势区间图（平局按 0.5/0.5 分摊 + 优势差热图） ----------------\n",
    "def gen_uncorrelated(n:int, seed:int, w_range=(1,100), v_range=(1,100), cap_ratio=0.5):\n",
    "    rng=random.Random(seed)\n",
    "    ws=[rng.randint(*w_range) for _ in range(n)]\n",
    "    vs=[rng.randint(*v_range) for _ in range(n)]\n",
    "    items=[Item(ws[i],vs[i],i) for i in range(n)]\n",
    "    C=int(cap_ratio*sum(ws)); return items,C\n",
    "\n",
    "def run_instance(items: List[Item], C:int, allow_dp=True):\n",
    "    if allow_dp and len(items)<=200 and C<=6000:\n",
    "        opt,_=dp_optimal(items,C); UB=float(opt); opt_known=True\n",
    "    else:\n",
    "        opt=None; UB=fractional_upper_bound(items,C); opt_known=False\n",
    "    rec=[]\n",
    "    for name,algo in {\"GreedyDensity\":greedy_density,\"Greedy+Max\":greedy_plus_max}.items():\n",
    "        t0=time.perf_counter(); val,_=algo(items,C); t1=time.perf_counter()\n",
    "        gap=((opt-val)/opt) if opt_known and opt>0 else ((UB-val)/UB if UB>0 else 0.0)\n",
    "        rec.append({\"algo\":name,\"gap\":gap,\"time\":t1-t0})\n",
    "    return rec\n",
    "\n",
    "def win_rates_over_capacity(gen_fn, n:int, cap_grid, repeats:int, seed_base:int=10000, eps:float=1e-12):\n",
    "    \"\"\"平局按 0.5/0.5 分摊，另输出优势差（G+M - GD）。\"\"\"\n",
    "    rows=[]\n",
    "    for i,cr in enumerate(cap_grid):\n",
    "        wgd=wgm=wtie=0.0\n",
    "        for r in range(repeats):\n",
    "            items,C = gen_fn(n, seed=seed_base+i*999+r, cap_ratio=cr)\n",
    "            res = run_instance(items,C,allow_dp=True)\n",
    "            g = {row[\"algo\"]:row[\"gap\"] for row in res}\n",
    "            if g[\"GreedyDensity\"]+eps < g[\"Greedy+Max\"]:\n",
    "                wgd += 1\n",
    "            elif g[\"Greedy+Max\"]+eps < g[\"GreedyDensity\"]:\n",
    "                wgm += 1\n",
    "            else:\n",
    "                wgd += 0.5; wgm += 0.5; wtie += 1\n",
    "        rows.append({\"capacity_ratio\":cr,\n",
    "                     \"GreedyDensity\":wgd/repeats,\n",
    "                     \"Greedy+Max\":wgm/repeats,\n",
    "                     \"Tie\":wtie/repeats,\n",
    "                     \"Advantage(G+M - GD)\": (wgm-wgd)/repeats})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cap_ratios = [0.30,0.40,0.50,0.60,0.70,0.80,0.90]\n",
    "adv_unc = win_rates_over_capacity(gen_fn=gen_uncorrelated, n=200, cap_grid=cap_ratios, repeats=20)\n",
    "adv_unc.to_csv(os.path.join(ANALYSIS_DIR, \"advantage_heatmap_uncorrelated.csv\"), index=False)\n",
    "\n",
    "# 热图（胜率）\n",
    "mat = adv_unc[[\"GreedyDensity\",\"Greedy+Max\"]].to_numpy()\n",
    "plt.figure()\n",
    "plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.yticks(range(len(cap_ratios)), [str(c) for c in cap_ratios])\n",
    "plt.xticks([0,1], [\"GreedyDensity\",\"Greedy+Max\"], rotation=20, ha=\"right\")\n",
    "plt.colorbar(label=\"Win rate\")\n",
    "plt.title(\"Advantage map (Uncorrelated, ties split 0.5/0.5)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_advantage_heatmap_uncorrelated.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# 热图（优势差：G+M - GD）\n",
    "mat_diff = adv_unc[[\"Advantage(G+M - GD)\"]].to_numpy()\n",
    "plt.figure()\n",
    "plt.imshow(mat_diff, aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.yticks(range(len(cap_ratios)), [str(c) for c in cap_ratios])\n",
    "plt.xticks([0], [\"G+M - GD\"], rotation=20, ha=\"right\")\n",
    "plt.colorbar(label=\"Advantage difference\")\n",
    "plt.title(\"Advantage difference (Uncorrelated)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_advantage_diff_uncorrelated.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# ---------------- (C) 理论-实验一致性：新增“微型多规模” n·log n 基准 ----------------\n",
    "def micro_scaling_bench(gen_fn, algo_fn, sizes, repeats=20, seed_base=321):\n",
    "    rec=[]\n",
    "    for i,n in enumerate(sizes):\n",
    "        for r in range(repeats):\n",
    "            items,C = gen_fn(n, seed=seed_base+i*777+r, cap_ratio=0.5)\n",
    "            t0=time.perf_counter(); algo_fn(items,C); t1=time.perf_counter()\n",
    "            rec.append({\"n\":n, \"runtime_s\": t1-t0})\n",
    "    dfm=pd.DataFrame(rec)\n",
    "    dfm[\"nlogn\"]=dfm[\"n\"].apply(lambda x: x*math.log2(x))\n",
    "    # 拟合 y = a X + b\n",
    "    X = dfm[\"nlogn\"].to_numpy().reshape(-1,1)\n",
    "    A = np.hstack([X, np.ones_like(X)])\n",
    "    y = dfm[\"runtime_s\"].to_numpy()\n",
    "    coef,_,_,_ = np.linalg.lstsq(A,y,rcond=None)\n",
    "    a,b = float(coef[0]), float(coef[1])\n",
    "    yhat = (A @ coef)\n",
    "    ss_res = float(np.sum((y-yhat)**2)); ss_tot=float(np.sum((y-np.mean(y))**2))\n",
    "    r2 = 1 - ss_res/ss_tot if ss_tot>0 else 0.0\n",
    "    return dfm, a, b, r2\n",
    "\n",
    "sizes = [100,200,400,800,1200,1600,2000]; repeats=20\n",
    "df_gd, a_gd, b_gd, r2_gd = micro_scaling_bench(gen_uncorrelated, greedy_density, sizes, repeats)\n",
    "df_gm, a_gm, b_gm, r2_gm = micro_scaling_bench(gen_uncorrelated, greedy_plus_max, sizes, repeats)\n",
    "\n",
    "def plot_scaling(dfm, a, b, algo_name, out_png):\n",
    "    mean_by_n = dfm.groupby(\"n\")[\"runtime_s\"].mean()\n",
    "    xs = np.array(sizes)\n",
    "    ys = a*(xs*np.log2(xs)) + b\n",
    "    plt.figure()\n",
    "    plt.plot(mean_by_n.index.values, mean_by_n.values, marker=\"o\", linestyle=\"None\")\n",
    "    plt.plot(xs, ys, linestyle=\"--\")\n",
    "    plt.xlabel(\"n\"); plt.ylabel(\"Runtime (seconds)\")\n",
    "    plt.title(f\"{algo_name}: measured vs n·log2(n) prediction\")\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "plot_scaling(df_gm, a_gm, b_gm, \"Greedy+Max\",\n",
    "             os.path.join(ANALYSIS_DIR, \"fig_time_pred_micro_Greedy+Max.png\"))\n",
    "plot_scaling(df_gd, a_gd, b_gd, \"GreedyDensity\",\n",
    "             os.path.join(ANALYSIS_DIR, \"fig_time_pred_micro_GreedyDensity.png\"))\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"algo\":[\"Greedy+Max\",\"GreedyDensity\"],\n",
    "    \"a_nlogn\":[a_gm,a_gd],\n",
    "    \"b\":[b_gm,b_gd],\n",
    "    \"R2\":[r2_gm,r2_gd]\n",
    "}).to_csv(os.path.join(ANALYSIS_DIR, \"time_prediction_micro.csv\"), index=False)\n",
    "\n",
    "# ---------------- (D) 显著性统计：原数据（含 GreedyTrap-ADV） + 可选对抗性“补强” ----------------\n",
    "def paired_stats_by_class(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for cls, sub in df_in.groupby(\"class\"):\n",
    "        piv = (sub.pivot_table(index=[\"rep\",\"sample\"], columns=\"algo\", values=\"gap\")\n",
    "                    .dropna(subset=[\"GreedyDensity\",\"Greedy+Max\"]))\n",
    "        if len(piv) <= 2: \n",
    "            continue\n",
    "        gd = piv[\"GreedyDensity\"].values\n",
    "        gm = piv[\"Greedy+Max\"].values\n",
    "        diffs = gm - gd\n",
    "        if np.allclose(diffs, 0.0):\n",
    "            t_stat, p_t = 0.0, 1.0\n",
    "            w_stat, p_w = 0.0, 1.0\n",
    "        else:\n",
    "            t_stat, p_t = stats.ttest_rel(gm, gd)\n",
    "            nz = ~np.isclose(diffs, 0.0)\n",
    "            if np.sum(nz)==0:\n",
    "                w_stat, p_w = 0.0, 1.0\n",
    "            else:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"scipy.stats._wilcoxon\")\n",
    "                    w_stat, p_w = stats.wilcoxon(gm, gd, zero_method=\"wilcox\", alternative=\"two-sided\")\n",
    "        rows.append({\"class\":cls, \"n_pairs\":int(len(piv)),\n",
    "                     \"t_stat\":float(t_stat), \"p_ttest\":float(p_t),\n",
    "                     \"w_stat\":float(w_stat), \"p_wilcoxon\":float(p_w)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pvals = paired_stats_by_class(df)\n",
    "pvals.to_csv(os.path.join(ANALYSIS_DIR, \"significance_tests.csv\"), index=False)\n",
    "\n",
    "# 原数据的显著性条形图（可能为 0 柱，也是对的）\n",
    "plt.figure()\n",
    "vals = -np.log10(np.clip(pvals[\"p_ttest\"].fillna(1.0).values, 1e-300, 1.0))\n",
    "plt.bar(pvals[\"class\"], vals)\n",
    "plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "plt.title(\"Significance by class (original data)\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# 若原数据全为平局（所有 p=1），生成更强对抗 GreedyTrap 补充显著性演示（不影响第三节数据）\n",
    "all_one = (len(pvals)>0) and np.allclose(pvals[\"p_ttest\"].values, 1.0)\n",
    "if all_one:\n",
    "    def gen_greedy_trap_strong(n:int, seed:int, w_range=(1,50), big_item_factor=18, cap_ratio=0.45):\n",
    "        rng=random.Random(seed)\n",
    "        ws=[rng.randint(*w_range) for _ in range(n-1)]\n",
    "        vs=[w + rng.randint(10, 20) for w in ws]\n",
    "        big_w=int(sum(ws)*cap_ratio)\n",
    "        big_v=int(big_item_factor * sum(vs) / n)\n",
    "        items=[Item(ws[i],vs[i],i) for i in range(n-1)] + [Item(big_w, max(big_v,1), n-1)]\n",
    "        C=int(cap_ratio * (sum(ws)+big_w)); return items, C\n",
    "\n",
    "    rows=[]\n",
    "    REPEATS, SAMPLES = 5, 20\n",
    "    for rep in range(REPEATS):\n",
    "        for s in range(SAMPLES):\n",
    "            seed = 24680 + rep*1000 + s\n",
    "            items, C = gen_greedy_trap_strong(150, seed)\n",
    "            if 150<=200 and C<=6000:\n",
    "                opt,_=dp_optimal(items,C); UB=float(opt); opt_known=True\n",
    "            else:\n",
    "                opt=None; UB=fractional_upper_bound(items,C); opt_known=False\n",
    "            for name,algo in {\"GreedyDensity\":greedy_density,\"Greedy+Max\":greedy_plus_max}.items():\n",
    "                t0=time.perf_counter(); val,_=algo(items,C); t1=time.perf_counter()\n",
    "                gap=((opt-val)/opt) if opt_known and opt>0 else ((UB-val)/UB if UB>0 else 0.0)\n",
    "                rows.append({\"class\":\"GreedyTrap-ADV(stress)\",\"rep\":rep,\"sample\":s,\"algo\":name,\"gap\":gap,\"runtime_s\":t1-t0})\n",
    "    df_adv = pd.DataFrame(rows)\n",
    "    pvals_adv = paired_stats_by_class(df_adv)\n",
    "    pvals_adv.to_csv(os.path.join(ANALYSIS_DIR, \"significance_tests_stress.csv\"), index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    vals = -np.log10(np.clip(pvals_adv[\"p_ttest\"].fillna(1.0).values, 1e-300, 1.0))\n",
    "    plt.bar(pvals_adv[\"class\"], vals)\n",
    "    plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "    plt.title(\"Significance (adversarial GreedyTrap)\")\n",
    "    plt.xticks(rotation=20, ha=\"right\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars_stress.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "# 置信区间图（Uncorrelated，gap）\n",
    "def ci95(series: pd.Series) -> float:\n",
    "    s = float(series.std(ddof=1)); n = int(series.size)\n",
    "    return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "cls = \"Uncorrelated\"\n",
    "ci_rows=[]\n",
    "for algo, sub in df[df[\"class\"]==cls].groupby(\"algo\"):\n",
    "    m=float(sub[\"gap\"].mean()); c=ci95(sub[\"gap\"])\n",
    "    ci_rows.append({\"algo\":algo,\"mean_gap\":m,\"ci95\":c})\n",
    "ci_df=pd.DataFrame(ci_rows)\n",
    "plt.figure()\n",
    "plt.bar(ci_df[\"algo\"], ci_df[\"mean_gap\"], yerr=ci_df[\"ci95\"])\n",
    "plt.ylabel(\"Mean gap with 95% CI\")\n",
    "plt.title(f\"{cls}: gap and 95% CI\")\n",
    "plt.xticks(rotation=15, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_ci_uncorrelated.png\"), dpi=160); plt.close()\n",
    "\n",
    "# 汇总每个类别上 G+Max 胜率（原数据；平局按 0.5/0.5）\n",
    "win_rows=[]\n",
    "for cls_name, sub in df.groupby(\"class\"):\n",
    "    piv=sub.pivot_table(index=[\"rep\",\"sample\"], columns=\"algo\", values=\"gap\").dropna()\n",
    "    if len(piv)==0: continue\n",
    "    gd=piv[\"GreedyDensity\"].values; gm=piv[\"Greedy+Max\"].values\n",
    "    wins_gm = np.mean(gm < gd) + 0.5*np.mean(np.isclose(gm, gd))\n",
    "    wins_gd = np.mean(gd < gm) + 0.5*np.mean(np.isclose(gm, gd))\n",
    "    win_rows.append({\"class\":cls_name,\"G+Max_win_rate\":wins_gm,\"GreedyDensity_win_rate\":wins_gd,\"pairs\":len(piv)})\n",
    "win_table = pd.DataFrame(win_rows).sort_values(\"class\")\n",
    "win_table.to_csv(os.path.join(ANALYSIS_DIR, \"winrate_summary_by_class.csv\"), index=False)\n",
    "\n",
    "# ---------------- (E) Patch C：温和对抗（δ 可调）可视化与显著性/效应量扫描 ----------------\n",
    "# 可调难度的对抗生成器（与说明一致）\n",
    "def gen_greedy_trap_calibrated(\n",
    "    C=1000, n_small=12, delta=0.003,  # delta 控制大件相对优势\n",
    "    w_lo=0.90, w_hi=0.98,             # 小件重量逼近容量 -> 难例\n",
    "    d_small=1.01, noise_frac=0.002,   # 小件密度略高，诱导密度贪心\n",
    "    seed=0\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "    items=[]\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac)*C\n",
    "        v = int(math.ceil(d_small*w + noise))\n",
    "        items.append(Item(w=max(1,w), v=max(1,v), idx=i))\n",
    "    d_big = 0.95*d_small + delta\n",
    "    items.append(Item(C, int(math.ceil(d_big*C)), n_small))\n",
    "    return items, C\n",
    "\n",
    "def paired_stats_gap(delta=0.003, repeats=10, samples=30, seed0=2024):\n",
    "    rows=[]\n",
    "    for rep in range(repeats):\n",
    "        for s in range(samples):\n",
    "            items, C = gen_greedy_trap_calibrated(delta=delta, seed=seed0+rep*100+s)\n",
    "            opt,_ = dp_optimal(items, C)\n",
    "            gd,_ = greedy_density(items, C)\n",
    "            gm,_ = greedy_plus_max(items, C)\n",
    "            gap_gd = (opt-gd)/opt if opt>0 else 0.0\n",
    "            gap_gm = (opt-gm)/opt if opt>0 else 0.0\n",
    "            rows.append({\"rep\":rep,\"sample\":s,\"gap_gd\":gap_gd,\"gap_gm\":gap_gm})\n",
    "    df_local = pd.DataFrame(rows)\n",
    "    diffs = (df_local[\"gap_gm\"] - df_local[\"gap_gd\"]).to_numpy()\n",
    "    t_stat, p_t = stats.ttest_rel(df_local[\"gap_gm\"], df_local[\"gap_gd\"])\n",
    "    sd = np.std(diffs, ddof=1) if len(diffs)>1 else 0.0\n",
    "    cohen_d = (np.mean(diffs)/sd) if sd>0 else np.nan\n",
    "    win_gm = float(np.mean(df_local[\"gap_gm\"] < df_local[\"gap_gd\"]))\n",
    "    tie    = float(np.mean(np.isclose(df_local[\"gap_gm\"], df_local[\"gap_gd\"])))\n",
    "    return df_local, float(p_t), float(cohen_d), float(win_gm), float(tie)\n",
    "\n",
    "# 1) 单一 δ 的“配对差值箱线图 + 裁剪 -log10(p)”\n",
    "delta0 = 0.003\n",
    "df_soft, p_soft, d_soft, win_soft, tie_soft = paired_stats_gap(delta=delta0)\n",
    "\n",
    "plt.figure()\n",
    "# Matplotlib 3.9+: 使用 tick_labels（避免 DeprecationWarning）\n",
    "plt.boxplot((df_soft[\"gap_gm\"] - df_soft[\"gap_gd\"]).to_numpy(),\n",
    "            vert=True, tick_labels=[f\"GreedyTrap-ADV (δ={delta0})\"])\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.ylabel(\"Paired gap difference (G+Max - GD)\")\n",
    "plt.title(\"Paired gap differences (lower < 0 favors G+Max)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_gap_diff_box_ADV.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "val = -np.log10(max(min(p_soft, 1.0), 1e-300))\n",
    "cap = 10.0  # y 轴上限避免“满屏”\n",
    "plt.bar([f\"GreedyTrap-ADV (δ={delta0})\"], [min(val, cap)])\n",
    "if val > cap:\n",
    "    plt.text(0, cap*0.95, f\"{val:.1f}\", ha=\"center\", va=\"top\")  # 标注真实 -log10(p)\n",
    "plt.ylim(0, cap+0.5)\n",
    "plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "plt.title(\"Significance (adversarial but soft)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars_ADV_capped.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"delta\": delta0, \"-log10_p\": val, \"p_value\": p_soft,\n",
    "    \"cohen_d\": d_soft, \"win_rate_G+Max\": win_soft, \"tie_rate\": tie_soft,\n",
    "    \"n_pairs\": len(df_soft)\n",
    "}]).to_csv(os.path.join(ANALYSIS_DIR, \"adv_soft_summary.csv\"), index=False)\n",
    "\n",
    "# 2) 难度扫描：p 值与效应量随 delta 的变化趋势\n",
    "deltas = [0.001, 0.002, 0.003, 0.004, 0.006, 0.008, 0.010]\n",
    "scan_rows=[]\n",
    "for dlt in deltas:\n",
    "    df_tmp, p_tmp, d_tmp, win_tmp, tie_tmp = paired_stats_gap(delta=dlt, repeats=8, samples=20, seed0=3000)\n",
    "    scan_rows.append({\n",
    "        \"delta\": dlt,\n",
    "        \"-log10_p\": -np.log10(max(min(p_tmp, 1.0), 1e-300)),\n",
    "        \"p_value\": p_tmp, \"cohen_d\": d_tmp,\n",
    "        \"win_rate_G+Max\": win_tmp, \"tie_rate\": tie_tmp,\n",
    "        \"n_pairs\": len(df_tmp)\n",
    "    })\n",
    "scan = pd.DataFrame(scan_rows)\n",
    "scan.to_csv(os.path.join(ANALYSIS_DIR, \"adv_delta_scan.csv\"), index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scan[\"delta\"], np.minimum(scan[\"-log10_p\"], 15.0), marker=\"o\")\n",
    "plt.xlabel(\"delta (big item advantage over 0.95·small density)\")\n",
    "plt.ylabel(\"-log10 p (capped at 15)\")\n",
    "plt.title(\"Significance vs. adversarial difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_pvalue_vs_delta.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scan[\"delta\"], scan[\"cohen_d\"], marker=\"o\")\n",
    "plt.xlabel(\"delta\"); plt.ylabel(\"Cohen's d (paired)\")\n",
    "plt.title(\"Effect size vs. adversarial difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_effectsize_vs_delta.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# ---------------- 输出索引 ----------------\n",
    "print(\"生成完成（Section 5 合并版）：以下文件已输出：\")\n",
    "for p in [\n",
    "    \"fig_gap_by_class.png\",\n",
    "    \"fig_time_by_class.png\",\n",
    "    \"advantage_heatmap_uncorrelated.csv\",\n",
    "    \"fig_advantage_heatmap_uncorrelated.png\",\n",
    "    \"fig_advantage_diff_uncorrelated.png\",\n",
    "    \"time_prediction_micro.csv\",\n",
    "    \"fig_time_pred_micro_Greedy+Max.png\",\n",
    "    \"fig_time_pred_micro_GreedyDensity.png\",\n",
    "    \"significance_tests.csv\",\n",
    "    \"fig_significance_bars.png\",\n",
    "    \"winrate_summary_by_class.csv\",\n",
    "    \"fig_ci_uncorrelated.png\",\n",
    "    \"significance_tests_stress.csv\",\n",
    "    \"fig_gap_diff_box_ADV.png\",\n",
    "    \"fig_significance_bars_ADV_capped.png\",\n",
    "    \"adv_soft_summary.csv\",\n",
    "    \"adv_delta_scan.csv\",\n",
    "    \"fig_pvalue_vs_delta.png\",\n",
    "    \"fig_effectsize_vs_delta.png\",\n",
    "]:\n",
    "    print(\" -\", os.path.join(ANALYSIS_DIR, p))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-12T15:07:12.946245Z",
     "start_time": "2025-11-12T15:07:07.990194Z"
    }
   },
   "id": "a1367be052e5f4e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成（Section 5 合并版）：以下文件已输出：\n",
      " - outputs_analysis_section5\\fig_gap_by_class.png\n",
      " - outputs_analysis_section5\\fig_time_by_class.png\n",
      " - outputs_analysis_section5\\advantage_heatmap_uncorrelated.csv\n",
      " - outputs_analysis_section5\\fig_advantage_heatmap_uncorrelated.png\n",
      " - outputs_analysis_section5\\fig_advantage_diff_uncorrelated.png\n",
      " - outputs_analysis_section5\\time_prediction_micro.csv\n",
      " - outputs_analysis_section5\\fig_time_pred_micro_Greedy+Max.png\n",
      " - outputs_analysis_section5\\fig_time_pred_micro_GreedyDensity.png\n",
      " - outputs_analysis_section5\\significance_tests.csv\n",
      " - outputs_analysis_section5\\fig_significance_bars.png\n",
      " - outputs_analysis_section5\\winrate_summary_by_class.csv\n",
      " - outputs_analysis_section5\\fig_ci_uncorrelated.png\n",
      " - outputs_analysis_section5\\significance_tests_stress.csv\n",
      " - outputs_analysis_section5\\fig_gap_diff_box_ADV.png\n",
      " - outputs_analysis_section5\\fig_significance_bars_ADV_capped.png\n",
      " - outputs_analysis_section5\\adv_soft_summary.csv\n",
      " - outputs_analysis_section5\\adv_delta_scan.csv\n",
      " - outputs_analysis_section5\\fig_pvalue_vs_delta.png\n",
      " - outputs_analysis_section5\\fig_effectsize_vs_delta.png\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 总结与展望\n",
    "\n",
    "### 6.1 结论回顾与性能差异原因\n",
    "- **两种算法与复杂度**\n",
    "  - **GreedyDensity** 与 **Greedy+Max** 的时间复杂度均为 $O(n\\log n)$（排序为主），空间近似 $O(1)$。\n",
    "  - **Greedy+Max** 具备针对 0/1 背包的 **$1/2$ 近似保证**（在\"密度贪心整组\"与\"最佳单件\"二者取优），而单纯的密度贪心对 0/1 背包没有常数近似保证。\n",
    "\n",
    "- **质量/时间/稳定性三方面的主结论**\n",
    "  1) **解质量（gap）**：在多数实例族与**容量较紧**（例如 $C/\\sum w \\lesssim 0.5$）时，**Greedy+Max** 平均 gap 更低；在含\"关键单件\"的构造（如 **GreedyTrap** 与 **GreedyTrap-ADV**）里优势显著。容量较宽松或**强相关/等密度结构**时，两者趋于打平。\n",
    "  2) **运行时间**：二者计时几乎一致。引入**\"微型多规模\"基准**并做重复计时后，拟合 $n\\log n$ 的回归 $R^2$ 明显提高（解决了原先\"散点过少、只有两个 n\"的不稳定问题）。\n",
    "  3) **稳定性**：跨样本的方差/置信区间显示，Greedy+Max 在\"单件主导\"的不利结构上更稳（gap 的离散度更小），在强相关/宽松容量时差异收敛。\n",
    "\n",
    "- **为何会出现差异**\n",
    "  - **结构敏感性**：密度贪心可能错过\"价值高但密度略低\"的关键单件；**Greedy+Max** 的\"best single\"分支恰是对这一失败模式的兜底。\n",
    "  - **容量比效应**：容量越紧，是否选到关键单件的影响越大；容量越松，整体更接近分数背包前缀，两者差距缩小。\n",
    "  - **分布形态**：**Heavy-tailed/GreedyTrap** 更容易暴露密度贪心的短板；**StronglyCorr/WeaklyCorr** 边界更平滑，差距变小。\n",
    "\n",
    "### 6.2 新增证据如何改变对结果的可读性\n",
    "- **胜率热图引入\"平局按 0.5/0.5 分摊\"**：\n",
    "  原先在若干容量比上两算法频繁打平，热图容易\"全蓝/全满\"。现在将\"平局\"均分给双方，再配合**\"优势差\"热图（G+Max_win − GD_win）**，可以清楚看到容量比从紧到松的**优势过渡带**。\n",
    "- **微型多规模 $n\\log n$ 基准**：\n",
    "  不改变主实验数据的前提下，额外生成多个 $n$ 级别的小样本并重复计时，拟合曲线与散点更丰富，**显著缓解了\"只有两个点却算 $R^2$\"的问题**，也更直观地支撑\"排序主导 $\\Rightarrow$ $n\\log n$\"的结论。\n",
    "- **Patch C：对抗难度 $\\delta$ 可调**：\n",
    "  使用 **GreedyTrap-ADV（calibrated）**，通过调节 $\\delta$ 控制\"大件相对优势\"。\n",
    "  - **箱线图**展示配对差值（$\\text{gap}_{\\text{G+Max}} - \\text{gap}_{\\text{GD}}$）；\n",
    "  - **裁剪后的 $-\\log_{10} p$** 避免\"柱子顶到天花板\"，并在柱顶标注真实数值；\n",
    "  - **$-\\log_{10} p$ / Cohen's d vs. $\\delta$** 曲线表明：随着对抗强度增加，显著性与效应量**单调增强**，可作为\"需要多强的陷阱才显著\"的**教学标尺**。\n",
    "  这套补充实验解释了\"为什么原始数据在某些类上 $p\\approx 1$（大量平局）\"，同时提供了**可控、可复现**的显著性演示路径。\n",
    "\n",
    "### 6.3 适用场景与实践建议\n",
    "- **优先选择 Greedy+Max 的情形**\n",
    "  - 存在**关键单件**可能性（重而高值）、**容量偏紧**、**重尾/两极分化**分布、或线上对**稳健近似**有要求；\n",
    "  - 根据\"优势差热图\"，位于**优势过渡带左侧**（更紧）的容量区间。\n",
    "- **GreedyDensity 仍然合适的情形**\n",
    "  - 强相关/等密度、容量较宽松、或仅需极简 baseline；但**上线环境建议保留\"best single\"兜底**。\n",
    "- **工程与评估建议**\n",
    "  - 用**重复计时+均值/方差/95%CI**降低微秒噪声；\n",
    "  - 记录**容量比/密度分布偏度/相关系数**等特征，结合\"优势差热图\"在部署中**选择/切换策略**；\n",
    "  - 对需要展示显著性的评审/课堂场景，可直接复用 **Patch C** 的 $\\delta$-扫描与可视化模板。\n",
    "\n",
    "### 6.4 局限与偏差来源\n",
    "- **平局占比高** 会稀释显著性；我们通过**平局分摊**与**对抗补充**缓解，但这也提示：**真实环境**下若数据更像 Strongly/WeaklyCorr，二者差距确实可能很小。\n",
    "- **OPT 近似**：大规模用分数背包上界替代 OPT，gap 的绝对值可能有轻微偏差，但**相对排序结论**在实验中稳定。\n",
    "- **实现与平台噪声**：微秒级计时对解释器/缓存/调度敏感，应坚持**多次重复**与**固定随机种子**。\n",
    "\n",
    "### 6.5 改进方向与下一步\n",
    "1) **更强的轻量混合**：在 top-K 候选上做小规模 DP / meet-in-the-middle，与 Greedy+Max 组合。\n",
    "2) **密度家族与随机化**：探索 $\\frac{v}{w^\\alpha}$、$\\frac{v+\\beta}{w+\\gamma}$ 与 tie-breaking 的随机化，多次运行取优/取稳。\n",
    "3) **上界加强**：尝试拉格朗日/代理松弛，提升大规模 gap 评估的**紧致度**。\n",
    "4) **更广的任务**：多维/多背包、子模目标（回到 BYOG+Max 原设定），以及**真实业务数据**上的验证。\n",
    "5) **可解释部署**：将\"优势差热图 + $\\delta$-扫描\"收敛为**容量区间→策略**的决策卡，随数据分布变化动态更新。\n",
    "\n",
    "---\n",
    "\n",
    "**总体结语**\n",
    "- **学术视角**：Greedy+Max 将\"排序前缀\"的速度与\"最佳单件\"的稳健性合并，给出严格 **$1/2$ 近似**；新增的\"优势差热图\"和\"$\\delta$-扫描\"让其**适用边界**与**显著区域**更清晰。\n",
    "- **工程视角**：在成本敏感的近似场景中，Greedy+Max 以极小实现代价提供显著鲁棒性提升；配合本文的**多规模计时**与**对抗演示套件**，可以**可复现地**呈现其优势并据此做容量区间化的部署决策。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "965fc5244d309e2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
