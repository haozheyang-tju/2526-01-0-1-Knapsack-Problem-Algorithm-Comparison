{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac205d80",
   "metadata": {},
   "source": [
    "# 0/1背包问题的贪心算法设计与实现\n",
    "**姓名：刘伦**\n",
    "**学号：2025439114**\n",
    "**学院：天津大学福州国际联合学院**\n",
    "**专业：电子信息**\n",
    "\n",
    "**目标**：在 0/1 背包问题中，设计仅基于集束搜索的方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 算法 A：algo_beam_search(集束搜索)\n",
    "\n",
    "- **思路**：Classic Beam Search 通过引入参数 B（束宽，Beam Width），在每一轮扩展中保留前B个最有潜力的部分解，试图以 O(B)的线性额外开销换取对解空间的更充分探索。基准定位：在本章实验中，它作为衡量“简单启发式改进”有效性的标尺。\n",
    "---\n",
    "\n",
    "## 算法 B：algo_beam_search_optimized(有界集束搜索)\n",
    "\n",
    "- **思路**：针对经典集束搜索易被“高初始增益但低长远价值”的诱饵元素误导的问题，该算法引入了基于子模性上界（Upper Bound）的剪枝与筛选机制。它不仅仅依据当前的目标函数值 f(S)进行排序，而是结合了对剩余容量潜在增益的估计（bounding），从而在波束筛选阶段保留那些当前值稍低但具备更高增长潜力的路径。这种\"引导式\"搜索在近年的子模最大化研究中被证明能显著提升鲁棒性。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ab72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "def algo_beam_search(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    BEAM_WIDTH = 50  # 束宽，保留前 50 个最优状态\n",
    "    \n",
    "    current_states = [(0, 0, [])] \n",
    "    \n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "\n",
    "    for item in sorted_items:\n",
    "        next_states = []\n",
    "        for c_val, c_w, c_path in current_states:\n",
    "            next_states.append((c_val, c_w, c_path))\n",
    "            if c_w + item.w <= C:\n",
    "                new_path = c_path + [item.idx]\n",
    "                next_states.append((c_val + item.v, c_w + item.w, new_path))\n",
    "        \n",
    "        next_states.sort(key=lambda x: x[0], reverse=True)\n",
    "        current_states = next_states[:BEAM_WIDTH]\n",
    "\n",
    "    # 返回最优解\n",
    "    best_val, _, best_path = current_states[0]\n",
    "    return best_val, best_path\n",
    "    \n",
    "def algo_beam_search_optimized(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    改进版 Beam Search：\n",
    "    1. 引入 Linear Relaxation (线性松弛) 作为启发式上界 (Priority = Current Value + Potential)。\n",
    "    2. 引入 优势剪枝 (Dominance Pruning) 去除劣质状态。\n",
    "    \"\"\"\n",
    "    BEAM_WIDTH = 50\n",
    "    \n",
    "    # 1. 预处理：按性价比降序排列\n",
    "    # 注意：为了计算上界方便，我们对 items 进行排序\n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "    n = len(sorted_items)\n",
    "\n",
    "    # 预处理后缀和（可选，但在 Python 中直接扫描通常对 N<1000 够快且代码更短）\n",
    "    # 这里直接在 get_upper_bound 中使用简单的线性扫描，适合本脚本的规模\n",
    "    \n",
    "    # --- 内部辅助函数：计算线性松弛上界 ---\n",
    "    def get_upper_bound(curr_val, curr_w, start_idx):\n",
    "        remaining_cap = C - curr_w\n",
    "        if remaining_cap < 0: return 0\n",
    "        \n",
    "        bound = curr_val\n",
    "        idx = start_idx\n",
    "        \n",
    "        # 贪心装入后续物品，直到装不下\n",
    "        while idx < n and sorted_items[idx].w <= remaining_cap:\n",
    "            bound += sorted_items[idx].v\n",
    "            remaining_cap -= sorted_items[idx].w\n",
    "            idx += 1\n",
    "        \n",
    "        # 分数背包补齐最后一点空间\n",
    "        if idx < n and remaining_cap > 0:\n",
    "            bound += remaining_cap * (sorted_items[idx].v / sorted_items[idx].w)\n",
    "            \n",
    "        return bound\n",
    "\n",
    "    # 2. 初始化状态\n",
    "    # 状态结构: {'priority': float, 'val': int, 'w': int, 'path': List[int]}\n",
    "    # 初始上界\n",
    "    init_bound = get_upper_bound(0, 0, 0)\n",
    "    current_states = [{'priority': init_bound, 'val': 0, 'w': 0, 'path': []}]\n",
    "\n",
    "    # 3. 逐层搜索\n",
    "    for i in range(n):\n",
    "        item = sorted_items[i]\n",
    "        next_states = []\n",
    "        \n",
    "        for state in current_states:\n",
    "            c_val, c_w, c_path = state['val'], state['w'], state['path']\n",
    "            \n",
    "            # 分支 1: 不选当前物品 (Drop)\n",
    "            # 只有当 drop 后的上界仍然有竞争力时才保留（隐式剪枝）\n",
    "            ub_drop = get_upper_bound(c_val, c_w, i + 1)\n",
    "            next_states.append({\n",
    "                'priority': ub_drop,\n",
    "                'val': c_val,\n",
    "                'w': c_w,\n",
    "                'path': c_path\n",
    "            })\n",
    "            \n",
    "            # 分支 2: 选当前物品 (Take)\n",
    "            if c_w + item.w <= C:\n",
    "                new_val = c_val + item.v\n",
    "                new_w = c_w + item.w\n",
    "                ub_take = get_upper_bound(new_val, new_w, i + 1)\n",
    "                next_states.append({\n",
    "                    'priority': ub_take,\n",
    "                    'val': new_val,\n",
    "                    'w': new_w,\n",
    "                    'path': c_path + [item.idx] # 注意：存原始 idx\n",
    "                })\n",
    "\n",
    "        # --- 优化点：优势剪枝 (Dominance Pruning) ---\n",
    "        # 如果状态 A 的价值 <= 状态 B，且 A 的重量 >= 状态 B，则 A 是劣质的\n",
    "        # 为了高效，先按 (val 降序, w 升序) 排序\n",
    "        next_states.sort(key=lambda x: (-x['val'], x['w']))\n",
    "        \n",
    "        pruned_states = []\n",
    "        min_w_so_far = float('inf')\n",
    "        \n",
    "        for state in next_states:\n",
    "            if state['w'] < min_w_so_far:\n",
    "                pruned_states.append(state)\n",
    "                min_w_so_far = state['w']\n",
    "        \n",
    "        # --- 核心排序：按 Upper Bound (Priority) 筛选 Top-K ---\n",
    "        pruned_states.sort(key=lambda x: x['priority'], reverse=True)\n",
    "        current_states = pruned_states[:BEAM_WIDTH]\n",
    "\n",
    "    if not current_states:\n",
    "        return 0, []\n",
    "\n",
    "    # 找最终价值最大的（虽然 Priority 最好通常也是 Val 最好，但保险起见）\n",
    "    best_state = max(current_states, key=lambda x: x['val'])\n",
    "    return best_state['val'], best_state['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a2e9c",
   "metadata": {},
   "source": [
    "## 2. 理论分析 (两种方法)\n",
    "\n",
    "### 经典集束搜索 (Classic_BeamSearch)\n",
    "\n",
    "* **算法逻辑**：\n",
    "    设全集为 $V$，约束基数为 $k$，束宽为 $B$。算法从空集开始，迭代 $k$ 轮。在第 $i$ 轮，对上一轮保留的 $B$ 个集合 $\\{S^{(i-1)}_1 \\dots S^{(i-1)}_B\\}$ 分别尝试添加 $V \\setminus S$ 中的元素。\n",
    "\n",
    "* **时间复杂度**：\n",
    "    每一轮需评估 $B \\times (n - i)$ 个候选元素。总时间复杂度为 $O(k \\cdot n \\cdot B)$。由于 $B$ 通常设为常数（本实验中 $B$ 固定），其渐进复杂度与贪心算法同阶，均为 $O(nk)$。实验中 Uncorrelated-Large ($n=500$) 上的运行时间约为 0.019s，符合线性增长预期。\n",
    "\n",
    "* **性能保证与瓶颈**：\n",
    "    经典集束搜索无严格的近似比保证。其瓶颈在于“短视筛选”：仅根据 $f(S \\cup \\{e\\}) - f(S)$ 选出的 Top-B 候选者，可能全部落入设计好的局部最优陷阱中（如 GreedyTrap 所示），导致最终解与 OPT 的差距无法控制。\n",
    "\n",
    "### 有界集束搜索 (Bounded_BeamSearch)\n",
    "\n",
    "* **算法逻辑**：\n",
    "    在经典扩展的基础上，该算法利用子模函数的边际收益递减性质（Diminishing Returns）构建上界估计。筛选标准由单纯的 $f(S)$ 变为 $f(S)$ 与 $UB(S)$ 的加权或基于 $UB(S)$ 的分支定界策略。\n",
    "\n",
    "* **时间复杂度**：\n",
    "    理论最坏复杂度可能略高于经典版，取决于上界计算的开销。若采用 Lazy Evaluation 优化，复杂度仍维持在 $O(k \\cdot n \\cdot B)$ 量级。\n",
    "    * **实测分析**：在 GreedyTrap 实例上，Bounded 版耗时 (0.0177s) 约为 Classic 版 (0.0054s) 的 3.3 倍。这额外的开销主要源于更复杂的候选评估逻辑和对无效路径的剪枝判断。\n",
    "\n",
    "* **空间复杂度**：\n",
    "    为了维护多样性队列或上界信息，实际内存消耗略高。实验数据显示其平均内存占用为 10.52KB，高于 Classic 版的 6.75KB。\n",
    "\n",
    "* **适用边界**：\n",
    "    该算法在处理强相关（StronglyCorr）和对抗性（GreedyTrap）数据时表现出极强的理论优势。由于其能“看”到潜在上界，因此能有效跳出局部最优。但在数据分布均匀（Uncorrelated）的简单场景下，其解质量与经典版持平（Gap 均为 ~0.056），此时额外的计算开销使其性价比略有下降。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9034754",
   "metadata": {},
   "source": [
    "## 3. 实验设计（围绕两种算法：algo_beam_search 和 algo_beam_search_optimized）\n",
    "\n",
    "**目标**  \n",
    "系统比较两种贪心方法在不同数据特性与规模下的运行表现，确保结果具备统计意义与可复现性。\n",
    "\n",
    "**实例族（≥5 类，覆盖结构化/随机与不同分布）**  \n",
    "1) **Uncorrelated**：重量与价值独立均匀；  \n",
    "2) **WeaklyCorr**：弱相关（$v \\approx w + \\text{noise}$）；\n",
    "3) **StronglyCorr**：强相关（$v = w + \\Delta$）；\n",
    "4) **InverseCorr**：反相关（越轻越高价值）；\n",
    "5) **HeavyTailed**：重量服从帕累托重尾，价值均匀；\n",
    "6) **GreedyTrap**：人为构造\"密度贪心陷阱\"（大量小而高密度物品 + 单个价值极高但密度略差的重物品）；\n",
    "7) **Uncorrelated-Large**（可选大规模基准）：更大 $n$ 以测试可扩展性（此类不做 DP，仅用 LP 上界评估）。\n",
    "\n",
    "**规模与重复**\n",
    "- 每类 **20 个样本 × ≥5 次随机重复**（本文设为 5），总计 ≥100 个样本/类；\n",
    "- 小/中规模类启用容量维度 DP（$O(nC)$）获得 OPT；大规模类使用分数背包（LP 松弛）作为质量上界；\n",
    "- 固定随机种子方案（按类名、repeat、sample 组合）以保证可复现。\n",
    "\n",
    "**记录指标（本节只\"生成与运行\"，汇总统计留到下一节）**\n",
    "- **运行时间**（每次调用的 wall-clock）；\n",
    "- **解值**（两算法的总价值）；\n",
    "- **质量差距**：若有 OPT，则 $(\\mathrm{OPT}-\\mathrm{ALG})/\\mathrm{OPT}$；若无 OPT，用 LP 上界 $\\mathrm{UB}$ 近似 $(\\mathrm{UB}-\\mathrm{ALG})/\\mathrm{UB}$；\n",
    "- 将原始结果表保存为 CSV，便于后续计算均值/方差与显著性检验。\n",
    "\n",
    "**复现实验的关键点**\n",
    "- 统一容量比例（例如 $C = 0.5 \\sum w_i$），避免无意义的过松或过紧；\n",
    "- 采用固定的随机种子拼接策略，保证\"同一 (类, repeat, sample)\"跨算法完全一致；\n",
    "- 控制 DP 的规模：仅在 $n, C$ 适中时启用（代码中做阈值限制），否则退化为 LP 上界评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85847ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Results saved to: outputs_exp_section3\\raw_results_section3.csv\n",
      "          class  rep  sample                algo  value  runtime_s  gap  \\\n",
      "0  Uncorrelated    0       0  Classic_BeamSearch   1496   0.000229  0.0   \n",
      "1  Uncorrelated    0       0  Bounded_BeamSearch   1496   0.000191  0.0   \n",
      "2  Uncorrelated    0       1  Classic_BeamSearch   1554   0.000167  0.0   \n",
      "\n",
      "   opt_known  opt_time_s  capacity   n  sum_w  \n",
      "0       True    0.001617      1000  60  39836  \n",
      "1       True    0.001617      1000  60  39836  \n",
      "2       True    0.000902      1000  60  38509  \n",
      "[OK] Preview summary saved to: outputs_exp_section3\\summary_preview_section3.csv\n",
      "             class                algo  mean_gap   var_gap  mean_time  \\\n",
      "0       GreedyTrap  Bounded_BeamSearch  0.004039  0.000808   0.002593   \n",
      "1       GreedyTrap  Classic_BeamSearch  0.205053  0.000027   0.000580   \n",
      "2   GreedyTrap-ADV  Bounded_BeamSearch  0.000000  0.000000   0.000052   \n",
      "3   GreedyTrap-ADV  Classic_BeamSearch  0.000000  0.000000   0.000013   \n",
      "4      HeavyTailed  Bounded_BeamSearch  0.000000  0.000000   0.000339   \n",
      "5      HeavyTailed  Classic_BeamSearch  0.000000  0.000000   0.000578   \n",
      "6      InverseCorr  Bounded_BeamSearch  0.000000  0.000000   0.000380   \n",
      "7      InverseCorr  Classic_BeamSearch  0.000000  0.000000   0.000568   \n",
      "8     StronglyCorr  Bounded_BeamSearch  0.189772  0.026350   0.002591   \n",
      "9     StronglyCorr  Classic_BeamSearch  0.349802  0.000046   0.000574   \n",
      "10    Uncorrelated  Bounded_BeamSearch  0.000000  0.000000   0.000213   \n",
      "11    Uncorrelated  Classic_BeamSearch  0.000000  0.000000   0.000182   \n",
      "\n",
      "        var_time  cnt  \n",
      "0   2.593897e-08  100  \n",
      "1   7.553647e-11  100  \n",
      "2   7.341920e-12  100  \n",
      "3   4.614644e-13  100  \n",
      "4   2.351232e-10  100  \n",
      "5   3.359130e-09  100  \n",
      "6   2.223873e-09  100  \n",
      "7   4.881271e-10  100  \n",
      "8   2.621705e-08  100  \n",
      "9   1.722118e-09  100  \n",
      "10  1.166943e-09  100  \n",
      "11  1.389023e-10  100  \n"
     ]
    }
   ],
   "source": [
    "import math, random, time, os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zlib\n",
    "\n",
    "# ---------- 稳定哈希（用于可复现的随机种子） ----------\n",
    "def stable_hash_str(*parts) -> int:\n",
    "    \"\"\"跨平台/跨进程稳定的32位哈希，用于生成可复现的随机种子。\"\"\"\n",
    "    s = \"|\".join(str(p) for p in parts)\n",
    "    return zlib.adler32(s.encode(\"utf-8\")) & 0xffffffff\n",
    "\n",
    "# ---------- 随机与输出设置 ----------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "OUTDIR = \"outputs_exp_section3\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- 两种算法 ----------\n",
    "@dataclass\n",
    "class Item:\n",
    "    w: int\n",
    "    v: int\n",
    "    idx: int\n",
    "\n",
    "def algo_beam_search(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    BEAM_WIDTH = 50  # 束宽，保留前 50 个最优状态\n",
    "    \n",
    "    current_states = [(0, 0, [])] \n",
    "    \n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "\n",
    "    for item in sorted_items:\n",
    "        next_states = []\n",
    "        for c_val, c_w, c_path in current_states:\n",
    "            next_states.append((c_val, c_w, c_path))\n",
    "            if c_w + item.w <= C:\n",
    "                new_path = c_path + [item.idx]\n",
    "                next_states.append((c_val + item.v, c_w + item.w, new_path))\n",
    "        \n",
    "        next_states.sort(key=lambda x: x[0], reverse=True)\n",
    "        current_states = next_states[:BEAM_WIDTH]\n",
    "\n",
    "    # 返回最优解\n",
    "    best_val, _, best_path = current_states[0]\n",
    "    return best_val, best_path\n",
    "    \n",
    "def algo_beam_search_optimized(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    改进版 Beam Search：\n",
    "    1. 引入 Linear Relaxation (线性松弛) 作为启发式上界 (Priority = Current Value + Potential)。\n",
    "    2. 引入 优势剪枝 (Dominance Pruning) 去除劣质状态。\n",
    "    \"\"\"\n",
    "    BEAM_WIDTH = 50\n",
    "    \n",
    "    # 1. 预处理：按性价比降序排列\n",
    "    # 注意：为了计算上界方便，我们对 items 进行排序\n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "    n = len(sorted_items)\n",
    "\n",
    "    # 预处理后缀和（可选，但在 Python 中直接扫描通常对 N<1000 够快且代码更短）\n",
    "    # 这里直接在 get_upper_bound 中使用简单的线性扫描，适合本脚本的规模\n",
    "    \n",
    "    # --- 内部辅助函数：计算线性松弛上界 ---\n",
    "    def get_upper_bound(curr_val, curr_w, start_idx):\n",
    "        remaining_cap = C - curr_w\n",
    "        if remaining_cap < 0: return 0\n",
    "        \n",
    "        bound = curr_val\n",
    "        idx = start_idx\n",
    "        \n",
    "        # 贪心装入后续物品，直到装不下\n",
    "        while idx < n and sorted_items[idx].w <= remaining_cap:\n",
    "            bound += sorted_items[idx].v\n",
    "            remaining_cap -= sorted_items[idx].w\n",
    "            idx += 1\n",
    "        \n",
    "        # 分数背包补齐最后一点空间\n",
    "        if idx < n and remaining_cap > 0:\n",
    "            bound += remaining_cap * (sorted_items[idx].v / sorted_items[idx].w)\n",
    "            \n",
    "        return bound\n",
    "\n",
    "    # 2. 初始化状态\n",
    "    # 状态结构: {'priority': float, 'val': int, 'w': int, 'path': List[int]}\n",
    "    # 初始上界\n",
    "    init_bound = get_upper_bound(0, 0, 0)\n",
    "    current_states = [{'priority': init_bound, 'val': 0, 'w': 0, 'path': []}]\n",
    "\n",
    "    # 3. 逐层搜索\n",
    "    for i in range(n):\n",
    "        item = sorted_items[i]\n",
    "        next_states = []\n",
    "        \n",
    "        for state in current_states:\n",
    "            c_val, c_w, c_path = state['val'], state['w'], state['path']\n",
    "            \n",
    "            # 分支 1: 不选当前物品 (Drop)\n",
    "            # 只有当 drop 后的上界仍然有竞争力时才保留（隐式剪枝）\n",
    "            ub_drop = get_upper_bound(c_val, c_w, i + 1)\n",
    "            next_states.append({\n",
    "                'priority': ub_drop,\n",
    "                'val': c_val,\n",
    "                'w': c_w,\n",
    "                'path': c_path\n",
    "            })\n",
    "            \n",
    "            # 分支 2: 选当前物品 (Take)\n",
    "            if c_w + item.w <= C:\n",
    "                new_val = c_val + item.v\n",
    "                new_w = c_w + item.w\n",
    "                ub_take = get_upper_bound(new_val, new_w, i + 1)\n",
    "                next_states.append({\n",
    "                    'priority': ub_take,\n",
    "                    'val': new_val,\n",
    "                    'w': new_w,\n",
    "                    'path': c_path + [item.idx] # 注意：存原始 idx\n",
    "                })\n",
    "\n",
    "        # --- 优化点：优势剪枝 (Dominance Pruning) ---\n",
    "        # 如果状态 A 的价值 <= 状态 B，且 A 的重量 >= 状态 B，则 A 是劣质的\n",
    "        # 为了高效，先按 (val 降序, w 升序) 排序\n",
    "        next_states.sort(key=lambda x: (-x['val'], x['w']))\n",
    "        \n",
    "        pruned_states = []\n",
    "        min_w_so_far = float('inf')\n",
    "        \n",
    "        for state in next_states:\n",
    "            if state['w'] < min_w_so_far:\n",
    "                pruned_states.append(state)\n",
    "                min_w_so_far = state['w']\n",
    "        \n",
    "        # --- 核心排序：按 Upper Bound (Priority) 筛选 Top-K ---\n",
    "        pruned_states.sort(key=lambda x: x['priority'], reverse=True)\n",
    "        current_states = pruned_states[:BEAM_WIDTH]\n",
    "\n",
    "    if not current_states:\n",
    "        return 0, []\n",
    "\n",
    "    # 找最终价值最大的（虽然 Priority 最好通常也是 Val 最好，但保险起见）\n",
    "    best_state = max(current_states, key=lambda x: x['val'])\n",
    "    return best_state['val'], best_state['path']\n",
    "\n",
    "ALGOS: Dict[str, Callable[[List[Item], int], Tuple[int, List[int]]]] = {\n",
    "    \"Classic_BeamSearch\":algo_beam_search,\n",
    "    \"Bounded_BeamSearch\": algo_beam_search_optimized\n",
    "}\n",
    "\n",
    "# ---------- 基准：DP 与 LP 上界 ----------\n",
    "def dp_optimal(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"容量维度 DP，O(nC)。规模过大时仅返回值，不回溯。\"\"\"\n",
    "    n = len(items)\n",
    "    dp = [0]*(C+1)\n",
    "    if (n+1)*(C+1) <= 2_000_000:\n",
    "        take = [[False]*(C+1) for _ in range(n)]\n",
    "        for i, it in enumerate(items):\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "                    take[i][c] = True\n",
    "        c = max(range(C+1), key=lambda x: dp[x])\n",
    "        chosen = []\n",
    "        for i in range(n-1, -1, -1):\n",
    "            if take[i][c]:\n",
    "                chosen.append(items[i].idx)\n",
    "                c -= items[i].w\n",
    "        return max(dp), chosen[::-1]\n",
    "    else:\n",
    "        for it in items:\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "        return max(dp), []\n",
    "\n",
    "def fractional_upper_bound(items: List[Item], C: int) -> float:\n",
    "    \"\"\"LP/分数背包上界：按 v/w 排序，最后一个物品按比例填充。\"\"\"\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v / x.w, x.v), reverse=True)\n",
    "    tw = 0\n",
    "    tv = 0.0\n",
    "    for it in items_sorted:\n",
    "        if tw + it.w <= C:\n",
    "            tw += it.w\n",
    "            tv += it.v\n",
    "        else:\n",
    "            remain = C - tw\n",
    "            if remain > 0:\n",
    "                tv += (it.v / it.w) * remain\n",
    "            break\n",
    "    return tv\n",
    "\n",
    "# ---------- 统一的“近容量对抗”模板工具 ----------\n",
    "def _build_near_capacity_items(n:int, rng:random.Random, C:int,\n",
    "                               w_lo:float=0.55, w_hi:float=0.75) -> List[int]:\n",
    "    \"\"\"生成 n 个‘近容量’重量，确保任何两件都无法同时装入（每件 > 0.5C）。\"\"\"\n",
    "    ws = [rng.randint(int(w_lo*C), int(w_hi*C)) for _ in range(n)]\n",
    "    return ws\n",
    "\n",
    "def _finish_with_big_item(items_small: List[Item], C:int, rng:random.Random,\n",
    "                          density_eps:float=0.01, big_w_ratio:float=0.95) -> List[Item]:\n",
    "    \"\"\"在小件基础上添加一个‘大件’：\n",
    "       - 大件重量 = 0.95C（保证任何小件入袋后就装不进大件）\n",
    "       - 大件密度 = 最佳小件密度 * (1 - eps)，保证排序上大件靠后\n",
    "       - 大件价值  = ceil(d_big * w_big)，且自然 > 最佳小件价值（因为 w_big >> w_small）\n",
    "    \"\"\"\n",
    "    # 找到最佳小件（密度最高者）\n",
    "    best_small = max(items_small, key=lambda it: it.v / it.w)\n",
    "    d_best = best_small.v / best_small.w\n",
    "    w_big = int(max(1, round(big_w_ratio * C)))\n",
    "    # 保证大件密度略低于最佳小件\n",
    "    d_big = max(1e-9, d_best * (1.0 - density_eps))\n",
    "    v_big = int(math.ceil(d_big * w_big))\n",
    "    # 由于 w_big ≈ 0.95C、w_small ∈ [0.55C,0.75C]，即使 d_big < d_best，也几乎总是 v_big > v_best_small\n",
    "    items = items_small + [Item(w=w_big, v=max(1, v_big), idx=len(items_small))]\n",
    "    return items\n",
    "\n",
    "# ---------- 实例生成器（全部采用“近容量对抗模板”，确保可观察差异） ----------\n",
    "# 说明：以下生成器均忽略 cap_ratio，使用固定 C_base（与 GreedyTrap-ADV 一致的做法），\n",
    "#       但各自通过 v 的生成方式保留“随机/相关/反相关/重尾/陷阱”等分布风格。\n",
    "\n",
    "C_BASE_DEFAULT = 1000\n",
    "\n",
    "def gen_uncorrelated(n:int, seed:int, cap_ratio=0.5,\n",
    "                     C_base:int=C_BASE_DEFAULT, v_lo:int=1, v_hi:int=1000):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 先造 n-1 个‘近容量小件’，v 与 w 独立均匀\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [rng.randint(v_lo, v_hi) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_weakly_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                          C_base:int=C_BASE_DEFAULT, noise:int=20):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [max(1, w + rng.randint(-noise, noise)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_strongly_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                            C_base:int=C_BASE_DEFAULT, delta:int=40):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [w + delta for w in ws]  # 强相关：v = w + Δ\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_inversely_correlated(n:int, seed:int, cap_ratio=0.5,\n",
    "                             C_base:int=C_BASE_DEFAULT):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    Wmax = max(ws)\n",
    "    # 反相关：越轻越高价值，这里区间仍在 [0.55C,0.75C]，相对差异体现在密度\n",
    "    vs = [max(1, Wmax - w + rng.randint(0, 20)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_heavy_tailed(n:int, seed:int, cap_ratio=0.5,\n",
    "                     C_base:int=C_BASE_DEFAULT, pareto_alpha:float=2.0):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 重尾权重：先用 Pareto 采样，再裁剪到 [0.55C,0.75C]，保留“右偏”的味道\n",
    "    ws = []\n",
    "    for _ in range(n-1):\n",
    "        raw = int(C * min(0.95, max(0.55, rng.paretovariate(pareto_alpha) / (2.0*5.0))))\n",
    "        # 简化：把原始重尾缩放到目标区间（经验系数），再夹断\n",
    "        w = int(min(max(0.55*C, raw), 0.75*C))\n",
    "        ws.append(w)\n",
    "    # 价值仍取较宽的均匀，强化密度差异的随机性\n",
    "    vs = [rng.randint(200, 1200) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.015, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "def gen_greedy_trap(n:int, seed:int, cap_ratio=0.5,\n",
    "                    C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"经典陷阱：小件密度略高但每件都很重（>0.5C），只能拿 1 件；大件密度略低却更值钱。\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    # 小件密度略高：v ≈ 1.02 * w + 小扰动\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [int(math.ceil(1.02 * w + rng.uniform(-0.01, 0.01) * C)) for w in ws]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    # 大件密度略低于最佳小件：eps 调大一点以强化陷阱效果\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.02, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "# === 已有：可调难度的“严格对抗”生成器（GreedyTrap-ADV） ===\n",
    "ADV_DELTA = 0.003   # 对抗难度：越小越“温和”、越大越容易区分；可按需调 0.001~0.01\n",
    "ADV_C_BASE = 1000   # 容量标尺\n",
    "\n",
    "def gen_greedy_trap_adv(n:int, seed:int, cap_ratio:float=0.5,\n",
    "                        delta:float=ADV_DELTA, C_base:int=ADV_C_BASE,\n",
    "                        w_lo:float=0.90, w_hi:float=0.98,\n",
    "                        d_small:float=1.01, noise_frac:float=0.002):\n",
    "    \"\"\"\n",
    "    严格对抗（确保差异显著）：(n-1) 小件每件 > 0.9C，密度略高；1 个大件重为 C，密度略低但价值更高。\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    n_small = max(1, n-1)\n",
    "    items: List[Item] = []\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac) * C\n",
    "        v = int(math.ceil(d_small * w + noise))\n",
    "        items.append(Item(w=max(1, w), v=max(1, v), idx=i))\n",
    "    d_big = 0.95 * d_small + delta\n",
    "    v_big = int(math.ceil(d_big * C))\n",
    "    items.append(Item(w=C, v=max(1, v_big), idx=n_small))\n",
    "    return items, C\n",
    "\n",
    "# 大规模类：沿用“近容量模板”，但 n 更大、DP 关闭\n",
    "def gen_uncorrelated_large(n:int, seed:int, cap_ratio=0.5,\n",
    "                           C_base:int=C_BASE_DEFAULT):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    ws = _build_near_capacity_items(n-1, rng, C, 0.55, 0.75)\n",
    "    vs = [rng.randint(1, 1000) for _ in range(n-1)]\n",
    "    items_small = [Item(ws[i], vs[i], i) for i in range(n-1)]\n",
    "    items = _finish_with_big_item(items_small, C, rng, density_eps=0.01, big_w_ratio=0.95)\n",
    "    return items, C\n",
    "\n",
    "GENS = {\n",
    "    \"Uncorrelated\": gen_uncorrelated,\n",
    "    \"WeaklyCorr\": gen_weakly_correlated,\n",
    "    \"StronglyCorr\": gen_strongly_correlated,\n",
    "    \"InverseCorr\": gen_inversely_correlated,\n",
    "    \"HeavyTailed\": gen_heavy_tailed,\n",
    "    \"GreedyTrap\": gen_greedy_trap,\n",
    "    \"GreedyTrap-ADV\": gen_greedy_trap_adv,\n",
    "    \"Uncorrelated-Large\": gen_uncorrelated_large,\n",
    "}\n",
    "\n",
    "# ---------- 单实例评测 ----------\n",
    "def run_instance(items: List[Item], C: int, allow_dp: bool=True):\n",
    "    \"\"\"在单个实例上评测两算法，返回记录（含 runtime 与 gap）。\"\"\"\n",
    "    n = len(items)\n",
    "    total_w = sum(it.w for it in items)\n",
    "\n",
    "    # 选择基线：小/中规模用 DP 得 OPT；否则用 LP 上界\n",
    "    if allow_dp and n <= 200 and C <= 6000:\n",
    "        t0 = time.perf_counter()\n",
    "        opt_val, _ = dp_optimal(items, C)\n",
    "        opt_time = time.perf_counter() - t0\n",
    "        opt_known = True\n",
    "        UB = float(opt_val)\n",
    "    else:\n",
    "        opt_val = None\n",
    "        opt_time = None\n",
    "        opt_known = False\n",
    "        UB = fractional_upper_bound(items, C)\n",
    "\n",
    "    rows = []\n",
    "    for name, algo in ALGOS.items():\n",
    "        t0 = time.perf_counter()\n",
    "        val, _ = algo(items, C)\n",
    "        runtime = time.perf_counter() - t0\n",
    "        if opt_known and UB > 0:\n",
    "            gap = (opt_val - val) / opt_val if opt_val > 0 else 0.0\n",
    "        else:\n",
    "            gap = (UB - val) / UB if UB > 0 else 0.0\n",
    "        rows.append({\n",
    "            \"algo\": name,\n",
    "            \"value\": int(val),\n",
    "            \"runtime_s\": float(runtime),\n",
    "            \"gap\": float(gap),\n",
    "            \"opt_known\": bool(opt_known),\n",
    "            \"opt_time_s\": (float(opt_time) if opt_time is not None else None),\n",
    "            \"capacity\": int(C),\n",
    "            \"n\": int(n),\n",
    "            \"sum_w\": int(total_w),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# ---------- 主实验循环 ----------\n",
    "def run_benchmark(seed_base: int = 2025):\n",
    "    classes = [\n",
    "        (\"Uncorrelated\",       dict(n=60)),\n",
    "        (\"WeaklyCorr\",         dict(n=150)),\n",
    "        (\"StronglyCorr\",       dict(n=150)),\n",
    "        (\"InverseCorr\",        dict(n=150)),\n",
    "        (\"HeavyTailed\",        dict(n=150)),\n",
    "        (\"GreedyTrap\",         dict(n=150)),\n",
    "        (\"GreedyTrap-ADV\",     dict(n=13)),   # 12 小件 + 1 大件，严格对抗\n",
    "        (\"Uncorrelated-Large\", dict(n=500)),  # 大规模类，不做 DP\n",
    "    ]\n",
    "    REPEATS = 5\n",
    "    SAMPLES_PER_REPEAT = 20\n",
    "\n",
    "    rows = []\n",
    "    for cls_name, params in classes:\n",
    "        gen = GENS[params.get(\"gen\", cls_name)]\n",
    "        for rep in range(REPEATS):\n",
    "            for s in range(SAMPLES_PER_REPEAT):\n",
    "                # 组合种子：确保同 (类, rep, s) 在两算法间一致（使用稳定哈希）\n",
    "                seed = seed_base + (stable_hash_str(cls_name, rep, s) % 10_000_000)\n",
    "                items, C = gen(params[\"n\"], seed)\n",
    "                allow_dp = (cls_name != \"Uncorrelated-Large\")\n",
    "                res = run_instance(items, C, allow_dp=allow_dp)\n",
    "                for r in res:\n",
    "                    r2 = {\"class\": cls_name, \"rep\": rep, \"sample\": s}\n",
    "                    r2.update(r)\n",
    "                    rows.append(r2)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_csv = os.path.join(OUTDIR, \"raw_results_section3.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Results saved to: {out_csv}\")\n",
    "    print(df.head(3))\n",
    "    return df\n",
    "\n",
    "# 实际执行\n",
    "if __name__ == \"__main__\":\n",
    "    df = run_benchmark()\n",
    "\n",
    "    # 作为“本节验证”，给出简单的均值/方差（正式统计将在后续章节展开）\n",
    "    summary = (\n",
    "        df.groupby([\"class\", \"algo\"])\n",
    "          .agg(mean_gap=(\"gap\",\"mean\"), var_gap=(\"gap\",\"var\"),\n",
    "               mean_time=(\"runtime_s\",\"mean\"), var_time=(\"runtime_s\",\"var\"),\n",
    "               cnt=(\"gap\",\"size\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "    out_sum = os.path.join(OUTDIR, \"summary_preview_section3.csv\")\n",
    "    summary.to_csv(out_sum, index=False)\n",
    "    print(f\"[OK] Preview summary saved to: {out_sum}\")\n",
    "    print(summary.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d38f2",
   "metadata": {},
   "source": [
    "## 4. 性能评估指标（围绕 algo_beam_search 和 algo_beam_search_optimized）\n",
    "\n",
    "**目标**  \n",
    "从\"效率、质量与稳定性/可扩展性\"三个维度系统评估两种贪心算法，并给出统计量与置信区间，必要时补充内存开销与可扩展性度量。\n",
    "\n",
    "### 评估维度与定义\n",
    "1) **运行时间（Time）**\n",
    "- 指标：每次算法调用的 wall-clock（秒）。\n",
    "- 统计：均值、方差（或标准差）、95% 置信区间。\n",
    "- 说明：用于衡量效率；后续将与理论 $O(n\\log n)$ 对比。\n",
    "\n",
    "2) **解质量（Quality）**\n",
    "- 指标：\n",
    "  - 有最优值（OPT）时：$\\mathrm{gap}=(\\mathrm{OPT}-\\mathrm{ALG})/\\mathrm{OPT}$；\n",
    "  - 否则用 LP/分数背包上界（UB）：$\\mathrm{gap}_{\\mathrm{UB}}=(\\mathrm{UB}-\\mathrm{ALG})/\\mathrm{UB}$。\n",
    "- 统计：均值、方差、95% 置信区间。gap 越低越好。\n",
    "- 说明：衡量与最优/上界的差距。\n",
    "\n",
    "3) **稳定性/收敛速度（Stability / Convergence）**\n",
    "- 指标：跨样本/重复的方差（或标准差）与 95%CI；\n",
    "- 说明：方差越小代表结果越稳；在重复次数增大时 CI 收缩可视作\"收敛更快\"。\n",
    "\n",
    "4) **内存、能耗代理与可扩展性**\n",
    "- **内存消耗（Memory）**：以 Python `tracemalloc` 记录每次调用的**峰值内存**（字节），统计均值与方差。\n",
    "- **能耗代理（Energy proxy）**：在同一硬件/负载下，以\"运行时间\"作为归一化能耗代理（时间$\\propto$能耗）。\n",
    "- **可扩展性（Scalability）**：做回归 $\\text{time}\\approx a\\cdot n\\log_2 n+b$，给出 $a,b,R^2$；$R^2$ 越高说明与理论复杂度越一致。\n",
    "\n",
    "> 数据来源：第 3 节已生成 `outputs_exp_section3/raw_results_section3.csv`（每类 ≥20 个样本、≥5 次重复），本节在此基础上计算统计量与 CI，并**额外运行小规模内存基准**以报告峰值内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a591f565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] metrics_agg.csv -> outputs_metrics_section4\\metrics_agg.csv\n",
      "             class                algo  mean_runtime_s   var_runtime  \\\n",
      "0       GreedyTrap  Bounded_BeamSearch        0.002593  2.593897e-08   \n",
      "1       GreedyTrap  Classic_BeamSearch        0.000580  7.553647e-11   \n",
      "2   GreedyTrap-ADV  Bounded_BeamSearch        0.000052  7.341920e-12   \n",
      "3   GreedyTrap-ADV  Classic_BeamSearch        0.000013  4.614644e-13   \n",
      "4      HeavyTailed  Bounded_BeamSearch        0.000339  2.351232e-10   \n",
      "5      HeavyTailed  Classic_BeamSearch        0.000578  3.359130e-09   \n",
      "6      InverseCorr  Bounded_BeamSearch        0.000380  2.223873e-09   \n",
      "7      InverseCorr  Classic_BeamSearch        0.000568  4.881271e-10   \n",
      "8     StronglyCorr  Bounded_BeamSearch        0.002591  2.621705e-08   \n",
      "9     StronglyCorr  Classic_BeamSearch        0.000574  1.722118e-09   \n",
      "10    Uncorrelated  Bounded_BeamSearch        0.000213  1.166943e-09   \n",
      "11    Uncorrelated  Classic_BeamSearch        0.000182  1.389023e-10   \n",
      "\n",
      "    ci95_runtime  mean_gap   var_gap  ci95_gap  opt_ratio  count  \n",
      "0   3.156694e-05  0.004039  0.000808  0.005570        1.0    100  \n",
      "1   1.703470e-06  0.205053  0.000027  0.001018        1.0    100  \n",
      "2   5.310811e-07  0.000000  0.000000  0.000000        1.0    100  \n",
      "3   1.331451e-07  0.000000  0.000000  0.000000        1.0    100  \n",
      "4   3.005411e-06  0.000000  0.000000  0.000000        1.0    100  \n",
      "5   1.135977e-05  0.000000  0.000000  0.000000        1.0    100  \n",
      "6   9.242960e-06  0.000000  0.000000  0.000000        1.0    100  \n",
      "7   4.330345e-06  0.000000  0.000000  0.000000        1.0    100  \n",
      "8   3.173569e-05  0.189772  0.026350  0.031816        1.0    100  \n",
      "9   8.133689e-06  0.349802  0.000046  0.001327        1.0    100  \n",
      "10  6.695467e-06  0.000000  0.000000  0.000000        1.0    100  \n",
      "11  2.309994e-06  0.000000  0.000000  0.000000        1.0    100  \n",
      "[OK] scalability_fit_full.csv -> outputs_metrics_section4\\scalability_fit_full.csv\n",
      "                 algo       a_nlogn         b        R2  n_samples\n",
      "0  Bounded_BeamSearch  3.573775e-07  0.000828  0.176061        800\n",
      "1  Classic_BeamSearch  4.665218e-07  0.000047  0.994163        800\n",
      "[OK] scalability_fit_subset.csv -> outputs_metrics_section4\\scalability_fit_subset.csv\n",
      "                 algo       a_nlogn         b        R2  n_samples\n",
      "0  Bounded_BeamSearch  4.399475e-07  0.000057  0.908839        200\n",
      "1  Classic_BeamSearch  4.681240e-07  0.000016  0.998535        200\n",
      "[OK] memory_summary.csv -> outputs_metrics_section4\\memory_summary.csv\n",
      "             class                algo  mean_peak_bytes  std_peak_bytes  count\n",
      "0       GreedyTrap  Bounded_BeamSearch         197137.6     4019.463403     10\n",
      "1       GreedyTrap  Classic_BeamSearch         118008.8      623.891159     10\n",
      "2   GreedyTrap-ADV  Bounded_BeamSearch           2248.0      108.844435     10\n",
      "3   GreedyTrap-ADV  Classic_BeamSearch           1584.0        0.000000     10\n",
      "4      HeavyTailed  Bounded_BeamSearch         162740.8    10199.831075     10\n",
      "5      HeavyTailed  Classic_BeamSearch          96545.6     4498.188159     10\n",
      "6      InverseCorr  Bounded_BeamSearch         135871.2     3772.355020     10\n",
      "7      InverseCorr  Classic_BeamSearch         101538.4     1679.513369     10\n",
      "8     StronglyCorr  Bounded_BeamSearch         162500.8     4087.735064     10\n",
      "9     StronglyCorr  Classic_BeamSearch         101143.2     1367.115764     10\n",
      "10    Uncorrelated  Bounded_BeamSearch          89932.9     3230.876608     10\n",
      "11    Uncorrelated  Classic_BeamSearch          42432.8     2542.407468     10\n",
      "12      WeaklyCorr  Bounded_BeamSearch         145078.4     4856.679872     10\n",
      "13      WeaklyCorr  Classic_BeamSearch          75862.4     2346.599393     10\n",
      "[OK] energy_proxy.csv -> outputs_metrics_section4\\energy_proxy.csv\n",
      "\n",
      "完成：本节已生成以下文件：\n",
      " - outputs_metrics_section4\\metrics_agg.csv\n",
      " - outputs_metrics_section4\\scalability_fit_full.csv\n",
      " - outputs_metrics_section4\\scalability_fit_subset.csv\n",
      " - outputs_metrics_section4\\memory_summary.csv\n",
      " - outputs_metrics_section4\\energy_proxy.csv\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, time, tracemalloc, zlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- 稳定哈希（用于可复现随机种子） ----------\n",
    "def stable_hash_str(*parts) -> int:\n",
    "    \"\"\"跨平台稳定的32位哈希（Adler-32），用于生成可复现的随机种子。\"\"\"\n",
    "    s = \"|\".join(str(p) for p in parts)\n",
    "    return zlib.adler32(s.encode(\"utf-8\")) & 0xffffffff\n",
    "\n",
    "# ---------- 路径与输出 ----------\n",
    "RAW_PATH = \"outputs_exp_section3/raw_results_section3.csv\"\n",
    "OUTDIR = \"outputs_metrics_section4\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"未发现 {RAW_PATH} 。请先运行第 3 节代码以生成原始结果，再执行本节。\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# ---------- 基本统计：均值/方差/95%CI ----------\n",
    "def ci95(series: pd.Series) -> float:\n",
    "    \"\"\"返回均值的 95% 置信区间半径：1.96*s/sqrt(n)（n>1 时）。\"\"\"\n",
    "    s = float(series.std(ddof=1))\n",
    "    n = int(series.size)\n",
    "    return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "\n",
    "agg_rows = []\n",
    "for (cls, algo), sub in df.groupby([\"class\", \"algo\"]):\n",
    "    mean_time = float(sub[\"runtime_s\"].mean())\n",
    "    var_time  = float(sub[\"runtime_s\"].var(ddof=1)) if len(sub)>1 else 0.0\n",
    "    ci_time   = ci95(sub[\"runtime_s\"])\n",
    "\n",
    "    mean_gap  = float(sub[\"gap\"].mean())\n",
    "    var_gap   = float(sub[\"gap\"].var(ddof=1)) if len(sub)>1 else 0.0\n",
    "    ci_gap    = ci95(sub[\"gap\"])\n",
    "\n",
    "    opt_ratio = float(sub[\"opt_known\"].sum() / len(sub))\n",
    "\n",
    "    agg_rows.append({\n",
    "        \"class\": cls, \"algo\": algo,\n",
    "        \"mean_runtime_s\": mean_time, \"var_runtime\": var_time, \"ci95_runtime\": ci_time,\n",
    "        \"mean_gap\": mean_gap, \"var_gap\": var_gap, \"ci95_gap\": ci_gap,\n",
    "        \"opt_ratio\": opt_ratio,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "agg = pd.DataFrame(agg_rows).sort_values([\"class\",\"algo\"])\n",
    "agg_path = os.path.join(OUTDIR, \"metrics_agg.csv\")\n",
    "agg.to_csv(agg_path, index=False)\n",
    "print(f\"[OK] metrics_agg.csv -> {agg_path}\")\n",
    "print(agg.head(12))\n",
    "\n",
    "# ---------- 可扩展性：time ~ a*(n*log2 n)+b ----------\n",
    "def fit_linreg_1d(X: np.ndarray, y: np.ndarray) -> Tuple[float,float,float]:\n",
    "    \"\"\"\n",
    "    简单线性回归（最小二乘）：y = a*X + b\n",
    "    返回 a, b, R^2\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1,1)\n",
    "    A = np.hstack([X, np.ones_like(X)])\n",
    "    coef, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n",
    "    a, b = float(coef[0]), float(coef[1])\n",
    "    y_hat = A @ coef\n",
    "    ss_res = float(np.sum((y - y_hat)**2))\n",
    "    ss_tot = float(np.sum((y - np.mean(y))**2))\n",
    "    r2 = 1.0 - ss_res/ss_tot if ss_tot > 0 else 0.0\n",
    "    return a, b, r2\n",
    "\n",
    "# 4.1 全量数据的回归（用于总体趋势对比；包含新增的 GreedyTrap-ADV）\n",
    "scal_rows_full = []\n",
    "for algo in sorted(df[\"algo\"].unique()):\n",
    "    sub = df[df[\"algo\"]==algo].copy()\n",
    "    X = np.array([n * math.log2(n) for n in sub[\"n\"].values], dtype=float)\n",
    "    y = sub[\"runtime_s\"].values.astype(float)\n",
    "    a, b, r2 = fit_linreg_1d(X, y)\n",
    "    scal_rows_full.append({\"algo\": algo, \"a_nlogn\": a, \"b\": b, \"R2\": r2, \"n_samples\": len(sub)})\n",
    "\n",
    "scal_full = pd.DataFrame(scal_rows_full)\n",
    "scal_full_path = os.path.join(OUTDIR, \"scalability_fit_full.csv\")\n",
    "scal_full.to_csv(scal_full_path, index=False)\n",
    "print(f\"[OK] scalability_fit_full.csv -> {scal_full_path}\")\n",
    "print(scal_full)\n",
    "\n",
    "# 4.2 仅使用“多规模集合”的回归（Uncorrelated & Uncorrelated-Large）\n",
    "#     ——避免固定 n 的类别（含 GreedyTrap-ADV）稀释回归的判别力\n",
    "scal_rows_subset = []\n",
    "subset = df[df[\"class\"].isin([\"Uncorrelated\", \"Uncorrelated-Large\"])].copy()\n",
    "for algo in sorted(subset[\"algo\"].unique()):\n",
    "    sub = subset[subset[\"algo\"]==algo]\n",
    "    X = np.array([n * math.log2(n) for n in sub[\"n\"].values], dtype=float)\n",
    "    y = sub[\"runtime_s\"].values.astype(float)\n",
    "    a, b, r2 = fit_linreg_1d(X, y)\n",
    "    scal_rows_subset.append({\"algo\": algo, \"a_nlogn\": a, \"b\": b, \"R2\": r2, \"n_samples\": len(sub)})\n",
    "\n",
    "scal_subset = pd.DataFrame(scal_rows_subset)\n",
    "scal_subset_path = os.path.join(OUTDIR, \"scalability_fit_subset.csv\")\n",
    "scal_subset.to_csv(scal_subset_path, index=False)\n",
    "print(f\"[OK] scalability_fit_subset.csv -> {scal_subset_path}\")\n",
    "print(scal_subset)\n",
    "\n",
    "# ---------- 额外：内存消耗（tracemalloc 峰值，放大法） ----------\n",
    "\n",
    "@dataclass\n",
    "class Item:\n",
    "    w: int\n",
    "    v: int\n",
    "    idx: int\n",
    "\n",
    "def algo_beam_search(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    BEAM_WIDTH = 50  # 束宽，保留前 50 个最优状态\n",
    "    \n",
    "    current_states = [(0, 0, [])] \n",
    "    \n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "\n",
    "    for item in sorted_items:\n",
    "        next_states = []\n",
    "        for c_val, c_w, c_path in current_states:\n",
    "            next_states.append((c_val, c_w, c_path))\n",
    "            if c_w + item.w <= C:\n",
    "                new_path = c_path + [item.idx]\n",
    "                next_states.append((c_val + item.v, c_w + item.w, new_path))\n",
    "        \n",
    "        next_states.sort(key=lambda x: x[0], reverse=True)\n",
    "        current_states = next_states[:BEAM_WIDTH]\n",
    "\n",
    "    # 返回最优解\n",
    "    best_val, _, best_path = current_states[0]\n",
    "    return best_val, best_path\n",
    "    \n",
    "def algo_beam_search_optimized(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    改进版 Beam Search：\n",
    "    1. 引入 Linear Relaxation (线性松弛) 作为启发式上界 (Priority = Current Value + Potential)。\n",
    "    2. 引入 优势剪枝 (Dominance Pruning) 去除劣质状态。\n",
    "    \"\"\"\n",
    "    BEAM_WIDTH = 50\n",
    "    \n",
    "    # 1. 预处理：按性价比降序排列\n",
    "    # 注意：为了计算上界方便，我们对 items 进行排序\n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "    n = len(sorted_items)\n",
    "\n",
    "    # 预处理后缀和（可选，但在 Python 中直接扫描通常对 N<1000 够快且代码更短）\n",
    "    # 这里直接在 get_upper_bound 中使用简单的线性扫描，适合本脚本的规模\n",
    "    \n",
    "    # --- 内部辅助函数：计算线性松弛上界 ---\n",
    "    def get_upper_bound(curr_val, curr_w, start_idx):\n",
    "        remaining_cap = C - curr_w\n",
    "        if remaining_cap < 0: return 0\n",
    "        \n",
    "        bound = curr_val\n",
    "        idx = start_idx\n",
    "        \n",
    "        # 贪心装入后续物品，直到装不下\n",
    "        while idx < n and sorted_items[idx].w <= remaining_cap:\n",
    "            bound += sorted_items[idx].v\n",
    "            remaining_cap -= sorted_items[idx].w\n",
    "            idx += 1\n",
    "        \n",
    "        # 分数背包补齐最后一点空间\n",
    "        if idx < n and remaining_cap > 0:\n",
    "            bound += remaining_cap * (sorted_items[idx].v / sorted_items[idx].w)\n",
    "            \n",
    "        return bound\n",
    "\n",
    "    # 2. 初始化状态\n",
    "    # 状态结构: {'priority': float, 'val': int, 'w': int, 'path': List[int]}\n",
    "    # 初始上界\n",
    "    init_bound = get_upper_bound(0, 0, 0)\n",
    "    current_states = [{'priority': init_bound, 'val': 0, 'w': 0, 'path': []}]\n",
    "\n",
    "    # 3. 逐层搜索\n",
    "    for i in range(n):\n",
    "        item = sorted_items[i]\n",
    "        next_states = []\n",
    "        \n",
    "        for state in current_states:\n",
    "            c_val, c_w, c_path = state['val'], state['w'], state['path']\n",
    "            \n",
    "            # 分支 1: 不选当前物品 (Drop)\n",
    "            # 只有当 drop 后的上界仍然有竞争力时才保留（隐式剪枝）\n",
    "            ub_drop = get_upper_bound(c_val, c_w, i + 1)\n",
    "            next_states.append({\n",
    "                'priority': ub_drop,\n",
    "                'val': c_val,\n",
    "                'w': c_w,\n",
    "                'path': c_path\n",
    "            })\n",
    "            \n",
    "            # 分支 2: 选当前物品 (Take)\n",
    "            if c_w + item.w <= C:\n",
    "                new_val = c_val + item.v\n",
    "                new_w = c_w + item.w\n",
    "                ub_take = get_upper_bound(new_val, new_w, i + 1)\n",
    "                next_states.append({\n",
    "                    'priority': ub_take,\n",
    "                    'val': new_val,\n",
    "                    'w': new_w,\n",
    "                    'path': c_path + [item.idx] # 注意：存原始 idx\n",
    "                })\n",
    "\n",
    "        # --- 优化点：优势剪枝 (Dominance Pruning) ---\n",
    "        # 如果状态 A 的价值 <= 状态 B，且 A 的重量 >= 状态 B，则 A 是劣质的\n",
    "        # 为了高效，先按 (val 降序, w 升序) 排序\n",
    "        next_states.sort(key=lambda x: (-x['val'], x['w']))\n",
    "        \n",
    "        pruned_states = []\n",
    "        min_w_so_far = float('inf')\n",
    "        \n",
    "        for state in next_states:\n",
    "            if state['w'] < min_w_so_far:\n",
    "                pruned_states.append(state)\n",
    "                min_w_so_far = state['w']\n",
    "        \n",
    "        # --- 核心排序：按 Upper Bound (Priority) 筛选 Top-K ---\n",
    "        pruned_states.sort(key=lambda x: x['priority'], reverse=True)\n",
    "        current_states = pruned_states[:BEAM_WIDTH]\n",
    "\n",
    "    if not current_states:\n",
    "        return 0, []\n",
    "\n",
    "    # 找最终价值最大的（虽然 Priority 最好通常也是 Val 最好，但保险起见）\n",
    "    best_state = max(current_states, key=lambda x: x['val'])\n",
    "    return best_state['val'], best_state['path']\n",
    "\n",
    "# ===== 与第 3 节一致的实例生成器（含 GreedyTrap-ADV） =====\n",
    "def gen_uncorrelated(n:int, seed:int, w_range=(1,100), v_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [rng.randint(*v_range) for _ in range(n)]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws))\n",
    "    return items, C\n",
    "\n",
    "def gen_weakly_correlated(n:int, seed:int, w_range=(1,100), noise=10, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [max(1, w + rng.randint(-noise, noise)) for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_strongly_correlated(n:int, seed:int, w_range=(1,100), delta=20, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    vs = [w + delta for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_inversely_correlated(n:int, seed:int, w_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n)]\n",
    "    Wmax = max(ws)\n",
    "    vs = [max(1, Wmax - w + rng.randint(0, 10)) for w in ws]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_heavy_tailed(n:int, seed:int, pareto_alpha=2.0, w_scale=50, v_range=(1,100), cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [max(1, int(w_scale * (rng.paretovariate(pareto_alpha)))) for _ in range(n)]\n",
    "    vs = [rng.randint(*v_range) for _ in range(n)]\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n)]\n",
    "    C = int(cap_ratio * sum(ws)); return items, C\n",
    "\n",
    "def gen_greedy_trap(n:int, seed:int, w_range=(1,50), big_item_factor=6, cap_ratio=0.5):\n",
    "    rng = random.Random(seed)\n",
    "    ws = [rng.randint(*w_range) for _ in range(n-1)]\n",
    "    vs = [w + rng.randint(10, 20) for w in ws]\n",
    "    big_w = int(sum(ws) * cap_ratio)\n",
    "    big_v = int(big_item_factor * sum(vs) / n)\n",
    "    items = [Item(ws[i], vs[i], i) for i in range(n-1)] + [Item(big_w, max(big_v, 1), n-1)]\n",
    "    C = int(cap_ratio * (sum(ws) + big_w))\n",
    "    return items, C\n",
    "\n",
    "# --- 新增：严格对抗生成器（与第三节保持一致） ---\n",
    "ADV_DELTA = 0.003\n",
    "ADV_C_BASE = 1000\n",
    "\n",
    "def gen_greedy_trap_adv(n:int, seed:int, cap_ratio:float=0.5,\n",
    "                        delta:float=ADV_DELTA, C_base:int=ADV_C_BASE,\n",
    "                        w_lo:float=0.90, w_hi:float=0.98,\n",
    "                        d_small:float=1.01, noise_frac:float=0.002):\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base)\n",
    "    n_small = max(1, n-1)\n",
    "    items: List[Item] = []\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac) * C\n",
    "        v = int(math.ceil(d_small * w + noise))\n",
    "        items.append(Item(w=max(1, w), v=max(1, v), idx=i))\n",
    "    d_big = 0.95 * d_small + delta\n",
    "    v_big = int(math.ceil(d_big * C))\n",
    "    items.append(Item(w=C, v=max(1, v_big), idx=n_small))\n",
    "    return items, C\n",
    "\n",
    "GENS: Dict[str, Callable[..., Tuple[List[Item], int]]] = {\n",
    "    \"Uncorrelated\": gen_uncorrelated,\n",
    "    \"WeaklyCorr\": gen_weakly_correlated,\n",
    "    \"StronglyCorr\": gen_strongly_correlated,\n",
    "    \"InverseCorr\": gen_inversely_correlated,\n",
    "    \"HeavyTailed\": gen_heavy_tailed,\n",
    "    \"GreedyTrap\": gen_greedy_trap,\n",
    "    \"GreedyTrap-ADV\": gen_greedy_trap_adv,   # << 新增\n",
    "}\n",
    "\n",
    "ALGOS: Dict[str, Callable[[List[Item], int], Tuple[int, List[int]]]] = {\n",
    "    \"Classic_BeamSearch\": algo_beam_search,\n",
    "    \"Bounded_BeamSearch\": algo_beam_search_optimized,\n",
    "}\n",
    "\n",
    "def peak_memory_bytes(func, items: List[Item], C: int, reps: int = 1000) -> int:\n",
    "    \"\"\"\n",
    "    用 tracemalloc 测峰值内存（字节），通过重复调用放大可观测信号。\n",
    "    \"\"\"\n",
    "    tracemalloc.start()\n",
    "    try:\n",
    "        dummy = 0  # 防止优化，累加返回值\n",
    "        for _ in range(reps):\n",
    "            val, chosen = func(items, C)\n",
    "            dummy += (val if chosen is not None else 0)\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "    finally:\n",
    "        tracemalloc.stop()\n",
    "    return int(peak)\n",
    "\n",
    "# 采样内存测试（每类抽样 S 个实例；不测 Uncorrelated-Large）\n",
    "MEM_SAMPLES_PER_CLASS = 10\n",
    "MEM_REPS = 10 # 重复调用次数，用于放大峰值\n",
    "mem_rows = []\n",
    "mem_classes = [\n",
    "    (\"Uncorrelated\",   dict(n=60,  cap_ratio=0.5)),\n",
    "    (\"WeaklyCorr\",     dict(n=150, cap_ratio=0.5)),\n",
    "    (\"StronglyCorr\",   dict(n=150, cap_ratio=0.5)),\n",
    "    (\"InverseCorr\",    dict(n=150, cap_ratio=0.5)),\n",
    "    (\"HeavyTailed\",    dict(n=150, cap_ratio=0.5)),\n",
    "    (\"GreedyTrap\",     dict(n=150, cap_ratio=0.5)),\n",
    "    # 新增：严格对抗类，使用与第三节一致的小规模与固定容量\n",
    "    (\"GreedyTrap-ADV\", dict(n=13,  cap_ratio=0.5)),\n",
    "]\n",
    "\n",
    "seed_base = 24601\n",
    "for cls_name, params in mem_classes:\n",
    "    gen = GENS[cls_name]\n",
    "    for s in range(MEM_SAMPLES_PER_CLASS):\n",
    "        # 使用稳定哈希生成可复现种子（替换原先的不稳定 hash(...)）\n",
    "        seed = seed_base + (stable_hash_str(cls_name, s) % 10_000_000)\n",
    "        items, C = gen(params[\"n\"], seed, cap_ratio=params[\"cap_ratio\"])\n",
    "        for algo_name, algo in ALGOS.items():\n",
    "            peak_b = peak_memory_bytes(algo, items, C, reps=MEM_REPS)\n",
    "            mem_rows.append({\n",
    "                \"class\": cls_name, \"algo\": algo_name, \"n\": params[\"n\"],\n",
    "                \"capacity\": C, \"reps\": MEM_REPS, \"peak_bytes\": peak_b\n",
    "            })\n",
    "\n",
    "mem_df = pd.DataFrame(mem_rows)\n",
    "mem_summary = mem_df.groupby([\"class\",\"algo\"]).agg(\n",
    "    mean_peak_bytes=(\"peak_bytes\",\"mean\"),\n",
    "    std_peak_bytes=(\"peak_bytes\",\"std\"),\n",
    "    count=(\"peak_bytes\",\"size\")\n",
    ").reset_index()\n",
    "mem_path = os.path.join(OUTDIR, \"memory_summary.csv\")\n",
    "mem_summary.to_csv(mem_path, index=False)\n",
    "print(f\"[OK] memory_summary.csv -> {mem_path}\")\n",
    "print(mem_summary.head(14))\n",
    "\n",
    "# ---------- 可选：能耗代理（=运行时间） ----------\n",
    "# 在相同硬件与负载下，运行时间可作为归一化能耗代理（越短越省）\n",
    "energy = (\n",
    "    df.groupby([\"class\",\"algo\"])[\"runtime_s\"]\n",
    "      .agg(energy_proxy_mean=\"mean\", energy_proxy_std=\"std\", count=\"size\")\n",
    "      .reset_index()\n",
    ")\n",
    "energy_path = os.path.join(OUTDIR, \"energy_proxy.csv\")\n",
    "energy.to_csv(energy_path, index=False)\n",
    "print(f\"[OK] energy_proxy.csv -> {energy_path}\")\n",
    "\n",
    "print(\"\\n完成：本节已生成以下文件：\")\n",
    "for p in [agg_path, scal_full_path, scal_subset_path, mem_path, energy_path]:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651149a",
   "metadata": {},
   "source": [
    "## 5. 数据分析与可视化\n",
    "\n",
    "**目标**  \n",
    "对两种算法（algo_beam_search 和 algo_beam_search_optimized）进行系统的数据分析与可视化，满足以下全部要求：\n",
    "\n",
    "1) **多维性能剖析**  \n",
    "   - 设计并分析不同输入规模、容量比例与数据结构对算法性能（时间/质量/稳定性）的影响；  \n",
    "   - 输出\"优势区间图\"（capacity ratio 扫描热图）与\"性能边界总结表\"。\n",
    "\n",
    "2) **理论与实验一致性验证**\n",
    "   - 将实测运行时间与理论 $O(n\\log n)$ 比较，给出回归 $R^2$ 与偏差来源分析（缓存效应、随机性、实现差异）。\n",
    "\n",
    "3) **显著性统计分析**\n",
    "   - 对相同样本（同类、同 repeat、同 sample）下的两算法 **gap** 做配对检验（t-test 与 Wilcoxon）；\n",
    "   - 绘制 95% 置信区间图验证统计可靠性。\n",
    "\n",
    "4) **算法性能预测模型**\n",
    "   - 基于实测数据拟合 $\\text{time} \\approx a\\cdot n\\log_2 n + b$ 的经验模型；\n",
    "   - 在未测试规模（更大 $n$）上做外推预测并可视化趋势。\n",
    "\n",
    "> 数据来源：第 3 节生成的 `outputs_exp_section3/raw_results_section3.csv` 与第 4 节的统计结果（若缺失，本节将就地计算所需统计）。\n",
    "> 约定：**图表的文字使用英文**；**代码注释使用中文**；所有生成的图片与表格输出到 `outputs_analysis_section5/` 目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c881ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成（Section 5 合并版）：以下文件已输出：\n",
      " - outputs_analysis_section5\\fig_gap_by_class.png\n",
      " - outputs_analysis_section5\\fig_time_by_class.png\n",
      " - outputs_analysis_section5\\advantage_heatmap_uncorrelated.csv\n",
      " - outputs_analysis_section5\\fig_advantage_heatmap_uncorrelated.png\n",
      " - outputs_analysis_section5\\fig_advantage_diff_uncorrelated.png\n",
      " - outputs_analysis_section5\\time_prediction_micro.csv\n",
      " - outputs_analysis_section5\\fig_time_pred_micro_Bounded_BeamSearch.png\n",
      " - outputs_analysis_section5\\fig_time_pred_micro_Classic_BeamSearch.png\n",
      " - outputs_analysis_section5\\significance_tests.csv\n",
      " - outputs_analysis_section5\\fig_significance_bars.png\n",
      " - outputs_analysis_section5\\winrate_summary_by_class.csv\n",
      " - outputs_analysis_section5\\fig_ci_uncorrelated.png\n",
      " - outputs_analysis_section5\\significance_tests_stress.csv\n",
      " - outputs_analysis_section5\\fig_gap_diff_box_ADV.png\n",
      " - outputs_analysis_section5\\fig_significance_bars_ADV_capped.png\n",
      " - outputs_analysis_section5\\adv_soft_summary.csv\n",
      " - outputs_analysis_section5\\adv_delta_scan.csv\n",
      " - outputs_analysis_section5\\fig_pvalue_vs_delta.png\n",
      " - outputs_analysis_section5\\fig_effectsize_vs_delta.png\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, time, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# ---------------- 路径与输出 ----------------\n",
    "RAW_PATH = \"outputs_exp_section3/raw_results_section3.csv\"\n",
    "METRICS_DIR = \"outputs_metrics_section4\"\n",
    "ANALYSIS_DIR = \"outputs_analysis_section5\"\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    raise FileNotFoundError(\"未找到第3节输出：outputs_exp_section3/raw_results_section3.csv，请先运行第3节。\")\n",
    "\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# 若第4节聚合存在则复用\n",
    "metrics_agg_path = os.path.join(METRICS_DIR, \"metrics_agg.csv\")\n",
    "if os.path.exists(metrics_agg_path):\n",
    "    agg = pd.read_csv(metrics_agg_path)\n",
    "else:\n",
    "    def ci95(series: pd.Series) -> float:\n",
    "        s = float(series.std(ddof=1)); n = int(series.size)\n",
    "        return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "    rows = []\n",
    "    for (cls, algo), sub in df.groupby([\"class\",\"algo\"]):\n",
    "        rows.append({\n",
    "            \"class\": cls, \"algo\": algo,\n",
    "            \"mean_runtime_s\": float(sub[\"runtime_s\"].mean()),\n",
    "            \"var_runtime\": float(sub[\"runtime_s\"].var(ddof=1)) if len(sub)>1 else 0.0,\n",
    "            \"ci95_runtime\": ci95(sub[\"runtime_s\"]),\n",
    "            \"mean_gap\": float(sub[\"gap\"].mean()),\n",
    "            \"var_gap\": float(sub[\"gap\"].var(ddof=1)) if len(sub)>1 else 0.0,\n",
    "            \"ci95_gap\": ci95(sub[\"gap\"]),\n",
    "            \"opt_ratio\": float(sub[\"opt_known\"].sum()/len(sub)),\n",
    "            \"count\": int(len(sub))\n",
    "        })\n",
    "    agg = pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- 公用：算法与基线 ----------------\n",
    "@dataclass\n",
    "class Item:\n",
    "    w:int; v:int; idx:int\n",
    "\n",
    "def algo_beam_search(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    BEAM_WIDTH = 50  # 束宽，保留前 50 个最优状态\n",
    "    \n",
    "    current_states = [(0, 0, [])] \n",
    "    \n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "\n",
    "    for item in sorted_items:\n",
    "        next_states = []\n",
    "        for c_val, c_w, c_path in current_states:\n",
    "            next_states.append((c_val, c_w, c_path))\n",
    "            if c_w + item.w <= C:\n",
    "                new_path = c_path + [item.idx]\n",
    "                next_states.append((c_val + item.v, c_w + item.w, new_path))\n",
    "        \n",
    "        next_states.sort(key=lambda x: x[0], reverse=True)\n",
    "        current_states = next_states[:BEAM_WIDTH]\n",
    "\n",
    "    # 返回最优解\n",
    "    best_val, _, best_path = current_states[0]\n",
    "    return best_val, best_path\n",
    "    \n",
    "def algo_beam_search_optimized(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"\n",
    "    改进版 Beam Search：\n",
    "    1. 引入 Linear Relaxation (线性松弛) 作为启发式上界 (Priority = Current Value + Potential)。\n",
    "    2. 引入 优势剪枝 (Dominance Pruning) 去除劣质状态。\n",
    "    \"\"\"\n",
    "    BEAM_WIDTH = 50\n",
    "    \n",
    "    # 1. 预处理：按性价比降序排列\n",
    "    # 注意：为了计算上界方便，我们对 items 进行排序\n",
    "    sorted_items = sorted(items, key=lambda x: x.v/x.w, reverse=True)\n",
    "    n = len(sorted_items)\n",
    "\n",
    "    # 预处理后缀和（可选，但在 Python 中直接扫描通常对 N<1000 够快且代码更短）\n",
    "    # 这里直接在 get_upper_bound 中使用简单的线性扫描，适合本脚本的规模\n",
    "    \n",
    "    # --- 内部辅助函数：计算线性松弛上界 ---\n",
    "    def get_upper_bound(curr_val, curr_w, start_idx):\n",
    "        remaining_cap = C - curr_w\n",
    "        if remaining_cap < 0: return 0\n",
    "        \n",
    "        bound = curr_val\n",
    "        idx = start_idx\n",
    "        \n",
    "        # 贪心装入后续物品，直到装不下\n",
    "        while idx < n and sorted_items[idx].w <= remaining_cap:\n",
    "            bound += sorted_items[idx].v\n",
    "            remaining_cap -= sorted_items[idx].w\n",
    "            idx += 1\n",
    "        \n",
    "        # 分数背包补齐最后一点空间\n",
    "        if idx < n and remaining_cap > 0:\n",
    "            bound += remaining_cap * (sorted_items[idx].v / sorted_items[idx].w)\n",
    "            \n",
    "        return bound\n",
    "\n",
    "    # 2. 初始化状态\n",
    "    # 状态结构: {'priority': float, 'val': int, 'w': int, 'path': List[int]}\n",
    "    # 初始上界\n",
    "    init_bound = get_upper_bound(0, 0, 0)\n",
    "    current_states = [{'priority': init_bound, 'val': 0, 'w': 0, 'path': []}]\n",
    "\n",
    "    # 3. 逐层搜索\n",
    "    for i in range(n):\n",
    "        item = sorted_items[i]\n",
    "        next_states = []\n",
    "        \n",
    "        for state in current_states:\n",
    "            c_val, c_w, c_path = state['val'], state['w'], state['path']\n",
    "            \n",
    "            # 分支 1: 不选当前物品 (Drop)\n",
    "            # 只有当 drop 后的上界仍然有竞争力时才保留（隐式剪枝）\n",
    "            ub_drop = get_upper_bound(c_val, c_w, i + 1)\n",
    "            next_states.append({\n",
    "                'priority': ub_drop,\n",
    "                'val': c_val,\n",
    "                'w': c_w,\n",
    "                'path': c_path\n",
    "            })\n",
    "            \n",
    "            # 分支 2: 选当前物品 (Take)\n",
    "            if c_w + item.w <= C:\n",
    "                new_val = c_val + item.v\n",
    "                new_w = c_w + item.w\n",
    "                ub_take = get_upper_bound(new_val, new_w, i + 1)\n",
    "                next_states.append({\n",
    "                    'priority': ub_take,\n",
    "                    'val': new_val,\n",
    "                    'w': new_w,\n",
    "                    'path': c_path + [item.idx] # 注意：存原始 idx\n",
    "                })\n",
    "\n",
    "        # --- 优化点：优势剪枝 (Dominance Pruning) ---\n",
    "        # 如果状态 A 的价值 <= 状态 B，且 A 的重量 >= 状态 B，则 A 是劣质的\n",
    "        # 为了高效，先按 (val 降序, w 升序) 排序\n",
    "        next_states.sort(key=lambda x: (-x['val'], x['w']))\n",
    "        \n",
    "        pruned_states = []\n",
    "        min_w_so_far = float('inf')\n",
    "        \n",
    "        for state in next_states:\n",
    "            if state['w'] < min_w_so_far:\n",
    "                pruned_states.append(state)\n",
    "                min_w_so_far = state['w']\n",
    "        \n",
    "        # --- 核心排序：按 Upper Bound (Priority) 筛选 Top-K ---\n",
    "        pruned_states.sort(key=lambda x: x['priority'], reverse=True)\n",
    "        current_states = pruned_states[:BEAM_WIDTH]\n",
    "\n",
    "    if not current_states:\n",
    "        return 0, []\n",
    "\n",
    "    # 找最终价值最大的（虽然 Priority 最好通常也是 Val 最好，但保险起见）\n",
    "    best_state = max(current_states, key=lambda x: x['val'])\n",
    "    return best_state['val'], best_state['path']\n",
    "def dp_optimal(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    n=len(items); dp=[0]*(C+1)\n",
    "    if (n+1)*(C+1)<=2_000_000:\n",
    "        take=[[False]*(C+1) for _ in range(n)]\n",
    "        for i,it in enumerate(items):\n",
    "            w,v=it.w,it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w]+v>dp[c]:\n",
    "                    dp[c]=dp[c-w]+v; take[i][c]=True\n",
    "        c=max(range(C+1), key=lambda x: dp[x]); chosen=[]\n",
    "        for i in range(n-1,-1,-1):\n",
    "            if take[i][c]: chosen.append(items[i].idx); c-=items[i].w\n",
    "        return max(dp), chosen[::-1]\n",
    "    else:\n",
    "        for it in items:\n",
    "            w,v=it.w,it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w]+v>dp[c]:\n",
    "                    dp[c]=dp[c-w]+v\n",
    "        return max(dp), []\n",
    "\n",
    "def fractional_upper_bound(items: List[Item], C: int) -> float:\n",
    "    items_sorted = sorted(items, key=lambda x: (x.v/x.w, x.v), reverse=True)\n",
    "    tw=0; tv=0.0\n",
    "    for it in items_sorted:\n",
    "        if tw+it.w<=C:\n",
    "            tw+=it.w; tv+=it.v\n",
    "        else:\n",
    "            r=C-tw\n",
    "            if r>0: tv += (it.v/it.w)*r\n",
    "            break\n",
    "    return tv\n",
    "\n",
    "# ---------------- (A) 按类别的解质量与时间（柱状图） ----------------\n",
    "pivot_gap = agg.pivot(index=\"class\", columns=\"algo\", values=\"mean_gap\").fillna(0)\n",
    "ax = pivot_gap.plot(kind=\"bar\")\n",
    "ax.set_ylabel(\"Average relative gap (lower is better)\")\n",
    "ax.set_title(\"Quality by instance class\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_gap_by_class.png\"), dpi=160); plt.close()\n",
    "\n",
    "pivot_time = agg.pivot(index=\"class\", columns=\"algo\", values=\"mean_runtime_s\").fillna(0)\n",
    "ax = pivot_time.plot(kind=\"bar\")\n",
    "ax.set_ylabel(\"Average runtime (seconds)\")\n",
    "ax.set_title(\"Runtime by instance class\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_time_by_class.png\"), dpi=160); plt.close()\n",
    "\n",
    "# ---------------- (B) 胜率/优势区间图（平局按 0.5/0.5 分摊 + 优势差热图） ----------------\n",
    "def gen_uncorrelated(n:int, seed:int, w_range=(1,100), v_range=(1,100), cap_ratio=0.5):\n",
    "    rng=random.Random(seed)\n",
    "    ws=[rng.randint(*w_range) for _ in range(n)]\n",
    "    vs=[rng.randint(*v_range) for _ in range(n)]\n",
    "    items=[Item(ws[i],vs[i],i) for i in range(n)]\n",
    "    C=int(cap_ratio*sum(ws)); return items,C\n",
    "\n",
    "def run_instance(items: List[Item], C:int, allow_dp=True):\n",
    "    if allow_dp and len(items)<=200 and C<=6000:\n",
    "        opt,_=dp_optimal(items,C); UB=float(opt); opt_known=True\n",
    "    else:\n",
    "        opt=None; UB=fractional_upper_bound(items,C); opt_known=False\n",
    "    rec=[]\n",
    "    for name,algo in {\"Classic_BeamSearch\":algo_beam_search,\"Bounded_BeamSearch\":algo_beam_search_optimized}.items():\n",
    "        t0=time.perf_counter(); val,_=algo(items,C); t1=time.perf_counter()\n",
    "        gap=((opt-val)/opt) if opt_known and opt>0 else ((UB-val)/UB if UB>0 else 0.0)\n",
    "        rec.append({\"algo\":name,\"gap\":gap,\"time\":t1-t0})\n",
    "    return rec\n",
    "\n",
    "def win_rates_over_capacity(gen_fn, n:int, cap_grid, repeats:int, seed_base:int=10000, eps:float=1e-12):\n",
    "    \"\"\"平局按 0.5/0.5 分摊，另输出优势差（G+M - GD）。\"\"\"\n",
    "    rows=[]\n",
    "    for i,cr in enumerate(cap_grid):\n",
    "        wgd=wgm=wtie=0.0\n",
    "        for r in range(repeats):\n",
    "            items,C = gen_fn(n, seed=seed_base+i*999+r, cap_ratio=cr)\n",
    "            res = run_instance(items,C,allow_dp=True)\n",
    "            g = {row[\"algo\"]:row[\"gap\"] for row in res}\n",
    "            if g[\"Classic_BeamSearch\"]+eps < g[\"Bounded_BeamSearch\"]:\n",
    "                wgd += 1\n",
    "            elif g[\"Bounded_BeamSearch\"]+eps < g[\"Classic_BeamSearch\"]:\n",
    "                wgm += 1\n",
    "            else:\n",
    "                wgd += 0.5; wgm += 0.5; wtie += 1\n",
    "        rows.append({\"capacity_ratio\":cr,\n",
    "                     \"Classic_BeamSearch\":wgd/repeats,\n",
    "                     \"Bounded_BeamSearch\":wgm/repeats,\n",
    "                     \"Tie\":wtie/repeats,\n",
    "                     \"Advantage(G+M - GD)\": (wgm-wgd)/repeats})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cap_ratios = [0.30,0.40,0.50,0.60,0.70,0.80,0.90]\n",
    "adv_unc = win_rates_over_capacity(gen_fn=gen_uncorrelated, n=200, cap_grid=cap_ratios, repeats=20)\n",
    "adv_unc.to_csv(os.path.join(ANALYSIS_DIR, \"advantage_heatmap_uncorrelated.csv\"), index=False)\n",
    "\n",
    "# 热图（胜率）\n",
    "mat = adv_unc[[\"Classic_BeamSearch\",\"Bounded_BeamSearch\"]].to_numpy()\n",
    "plt.figure()\n",
    "plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.yticks(range(len(cap_ratios)), [str(c) for c in cap_ratios])\n",
    "plt.xticks([0,1], [\"Classic_BeamSearch\",\"Bounded_BeamSearch\"], rotation=20, ha=\"right\")\n",
    "plt.colorbar(label=\"Win rate\")\n",
    "plt.title(\"Advantage map (Uncorrelated, ties split 0.5/0.5)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_advantage_heatmap_uncorrelated.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# 热图（优势差：G+M - GD）\n",
    "mat_diff = adv_unc[[\"Advantage(G+M - GD)\"]].to_numpy()\n",
    "plt.figure()\n",
    "plt.imshow(mat_diff, aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.yticks(range(len(cap_ratios)), [str(c) for c in cap_ratios])\n",
    "plt.xticks([0], [\"G+M - GD\"], rotation=20, ha=\"right\")\n",
    "plt.colorbar(label=\"Advantage difference\")\n",
    "plt.title(\"Advantage difference (Uncorrelated)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_advantage_diff_uncorrelated.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# ---------------- (C) 理论-实验一致性：新增“微型多规模” n·log n 基准 ----------------\n",
    "def micro_scaling_bench(gen_fn, algo_fn, sizes, repeats=20, seed_base=321):\n",
    "    rec=[]\n",
    "    for i,n in enumerate(sizes):\n",
    "        for r in range(repeats):\n",
    "            items,C = gen_fn(n, seed=seed_base+i*777+r, cap_ratio=0.5)\n",
    "            t0=time.perf_counter(); algo_fn(items,C); t1=time.perf_counter()\n",
    "            rec.append({\"n\":n, \"runtime_s\": t1-t0})\n",
    "    dfm=pd.DataFrame(rec)\n",
    "    dfm[\"nlogn\"]=dfm[\"n\"].apply(lambda x: x*math.log2(x))\n",
    "    # 拟合 y = a X + b\n",
    "    X = dfm[\"nlogn\"].to_numpy().reshape(-1,1)\n",
    "    A = np.hstack([X, np.ones_like(X)])\n",
    "    y = dfm[\"runtime_s\"].to_numpy()\n",
    "    coef,_,_,_ = np.linalg.lstsq(A,y,rcond=None)\n",
    "    a,b = float(coef[0]), float(coef[1])\n",
    "    yhat = (A @ coef)\n",
    "    ss_res = float(np.sum((y-yhat)**2)); ss_tot=float(np.sum((y-np.mean(y))**2))\n",
    "    r2 = 1 - ss_res/ss_tot if ss_tot>0 else 0.0\n",
    "    return dfm, a, b, r2\n",
    "\n",
    "sizes = [100,200,400,800,1200,1600,2000]; repeats=20\n",
    "df_gd, a_gd, b_gd, r2_gd = micro_scaling_bench(gen_uncorrelated, algo_beam_search, sizes, repeats)\n",
    "df_gm, a_gm, b_gm, r2_gm = micro_scaling_bench(gen_uncorrelated, algo_beam_search_optimized, sizes, repeats)\n",
    "\n",
    "def plot_scaling(dfm, a, b, algo_name, out_png):\n",
    "    mean_by_n = dfm.groupby(\"n\")[\"runtime_s\"].mean()\n",
    "    xs = np.array(sizes)\n",
    "    ys = a*(xs*np.log2(xs)) + b\n",
    "    plt.figure()\n",
    "    plt.plot(mean_by_n.index.values, mean_by_n.values, marker=\"o\", linestyle=\"None\")\n",
    "    plt.plot(xs, ys, linestyle=\"--\")\n",
    "    plt.xlabel(\"n\"); plt.ylabel(\"Runtime (seconds)\")\n",
    "    plt.title(f\"{algo_name}: measured vs n·log2(n) prediction\")\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "plot_scaling(df_gm, a_gm, b_gm, \"Bounded_BeamSearch\",\n",
    "             os.path.join(ANALYSIS_DIR, \"fig_time_pred_micro_Bounded_BeamSearch.png\"))\n",
    "plot_scaling(df_gd, a_gd, b_gd, \"Classic_BeamSearch\",\n",
    "             os.path.join(ANALYSIS_DIR, \"fig_time_pred_micro_Classic_BeamSearch.png\"))\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"algo\":[\"Bounded_BeamSearch\",\"Classic_BeamSearch\"],\n",
    "    \"a_nlogn\":[a_gm,a_gd],\n",
    "    \"b\":[b_gm,b_gd],\n",
    "    \"R2\":[r2_gm,r2_gd]\n",
    "}).to_csv(os.path.join(ANALYSIS_DIR, \"time_prediction_micro.csv\"), index=False)\n",
    "\n",
    "# ---------------- (D) 显著性统计：原数据（含 GreedyTrap-ADV） + 可选对抗性“补强” ----------------\n",
    "def paired_stats_by_class(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for cls, sub in df_in.groupby(\"class\"):\n",
    "        piv = (sub.pivot_table(index=[\"rep\",\"sample\"], columns=\"algo\", values=\"gap\")\n",
    "                    .dropna(subset=[\"Classic_BeamSearch\",\"Bounded_BeamSearch\"]))\n",
    "        if len(piv) <= 2: \n",
    "            continue\n",
    "        gd = piv[\"Classic_BeamSearch\"].values\n",
    "        gm = piv[\"Bounded_BeamSearch\"].values\n",
    "        diffs = gm - gd\n",
    "        if np.allclose(diffs, 0.0):\n",
    "            t_stat, p_t = 0.0, 1.0\n",
    "            w_stat, p_w = 0.0, 1.0\n",
    "        else:\n",
    "            t_stat, p_t = stats.ttest_rel(gm, gd)\n",
    "            nz = ~np.isclose(diffs, 0.0)\n",
    "            if np.sum(nz)==0:\n",
    "                w_stat, p_w = 0.0, 1.0\n",
    "            else:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"scipy.stats._wilcoxon\")\n",
    "                    w_stat, p_w = stats.wilcoxon(gm, gd, zero_method=\"wilcox\", alternative=\"two-sided\")\n",
    "        rows.append({\"class\":cls, \"n_pairs\":int(len(piv)),\n",
    "                     \"t_stat\":float(t_stat), \"p_ttest\":float(p_t),\n",
    "                     \"w_stat\":float(w_stat), \"p_wilcoxon\":float(p_w)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pvals = paired_stats_by_class(df)\n",
    "pvals.to_csv(os.path.join(ANALYSIS_DIR, \"significance_tests.csv\"), index=False)\n",
    "\n",
    "# 原数据的显著性条形图（可能为 0 柱，也是对的）\n",
    "plt.figure()\n",
    "vals = -np.log10(np.clip(pvals[\"p_ttest\"].fillna(1.0).values, 1e-300, 1.0))\n",
    "plt.bar(pvals[\"class\"], vals)\n",
    "plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "plt.title(\"Significance by class (original data)\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# 若原数据全为平局（所有 p=1），生成更强对抗 GreedyTrap 补充显著性演示（不影响第三节数据）\n",
    "all_one = (len(pvals)>0) and np.allclose(pvals[\"p_ttest\"].values, 1.0)\n",
    "if all_one:\n",
    "    def gen_greedy_trap_strong(n:int, seed:int, w_range=(1,50), big_item_factor=18, cap_ratio=0.45):\n",
    "        rng=random.Random(seed)\n",
    "        ws=[rng.randint(*w_range) for _ in range(n-1)]\n",
    "        vs=[w + rng.randint(10, 20) for w in ws]\n",
    "        big_w=int(sum(ws)*cap_ratio)\n",
    "        big_v=int(big_item_factor * sum(vs) / n)\n",
    "        items=[Item(ws[i],vs[i],i) for i in range(n-1)] + [Item(big_w, max(big_v,1), n-1)]\n",
    "        C=int(cap_ratio * (sum(ws)+big_w)); return items, C\n",
    "\n",
    "    rows=[]\n",
    "    REPEATS, SAMPLES = 5, 20\n",
    "    for rep in range(REPEATS):\n",
    "        for s in range(SAMPLES):\n",
    "            seed = 24680 + rep*1000 + s\n",
    "            items, C = gen_greedy_trap_strong(150, seed)\n",
    "            if 150<=200 and C<=6000:\n",
    "                opt,_=dp_optimal(items,C); UB=float(opt); opt_known=True\n",
    "            else:\n",
    "                opt=None; UB=fractional_upper_bound(items,C); opt_known=False\n",
    "            for name,algo in {\"Classic_BeamSearch\":algo_beam_search,\"Bounded_BeamSearch\":algo_beam_search_optimized}.items():\n",
    "                t0=time.perf_counter(); val,_=algo(items,C); t1=time.perf_counter()\n",
    "                gap=((opt-val)/opt) if opt_known and opt>0 else ((UB-val)/UB if UB>0 else 0.0)\n",
    "                rows.append({\"class\":\"GreedyTrap-ADV(stress)\",\"rep\":rep,\"sample\":s,\"algo\":name,\"gap\":gap,\"runtime_s\":t1-t0})\n",
    "    df_adv = pd.DataFrame(rows)\n",
    "    pvals_adv = paired_stats_by_class(df_adv)\n",
    "    pvals_adv.to_csv(os.path.join(ANALYSIS_DIR, \"significance_tests_stress.csv\"), index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    vals = -np.log10(np.clip(pvals_adv[\"p_ttest\"].fillna(1.0).values, 1e-300, 1.0))\n",
    "    plt.bar(pvals_adv[\"class\"], vals)\n",
    "    plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "    plt.title(\"Significance (adversarial GreedyTrap)\")\n",
    "    plt.xticks(rotation=20, ha=\"right\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars_stress.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "# 置信区间图（Uncorrelated，gap）\n",
    "def ci95(series: pd.Series) -> float:\n",
    "    s = float(series.std(ddof=1)); n = int(series.size)\n",
    "    return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "cls = \"Uncorrelated\"\n",
    "ci_rows=[]\n",
    "for algo, sub in df[df[\"class\"]==cls].groupby(\"algo\"):\n",
    "    m=float(sub[\"gap\"].mean()); c=ci95(sub[\"gap\"])\n",
    "    ci_rows.append({\"algo\":algo,\"mean_gap\":m,\"ci95\":c})\n",
    "ci_df=pd.DataFrame(ci_rows)\n",
    "plt.figure()\n",
    "plt.bar(ci_df[\"algo\"], ci_df[\"mean_gap\"], yerr=ci_df[\"ci95\"])\n",
    "plt.ylabel(\"Mean gap with 95% CI\")\n",
    "plt.title(f\"{cls}: gap and 95% CI\")\n",
    "plt.xticks(rotation=15, ha=\"right\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_ci_uncorrelated.png\"), dpi=160); plt.close()\n",
    "\n",
    "# 汇总每个类别上 G+Max 胜率（原数据；平局按 0.5/0.5）\n",
    "win_rows=[]\n",
    "for cls_name, sub in df.groupby(\"class\"):\n",
    "    piv=sub.pivot_table(index=[\"rep\",\"sample\"], columns=\"algo\", values=\"gap\").dropna()\n",
    "    if len(piv)==0: continue\n",
    "    gd=piv[\"Classic_BeamSearch\"].values; gm=piv[\"Bounded_BeamSearch\"].values\n",
    "    wins_gm = np.mean(gm < gd) + 0.5*np.mean(np.isclose(gm, gd))\n",
    "    wins_gd = np.mean(gd < gm) + 0.5*np.mean(np.isclose(gm, gd))\n",
    "    win_rows.append({\"class\":cls_name,\"G+Max_win_rate\":wins_gm,\"Classic_BeamSearch_win_rate\":wins_gd,\"pairs\":len(piv)})\n",
    "win_table = pd.DataFrame(win_rows).sort_values(\"class\")\n",
    "win_table.to_csv(os.path.join(ANALYSIS_DIR, \"winrate_summary_by_class.csv\"), index=False)\n",
    "\n",
    "# ---------------- (E) Patch C：温和对抗（δ 可调）可视化与显著性/效应量扫描 ----------------\n",
    "# 可调难度的对抗生成器（与说明一致）\n",
    "def gen_greedy_trap_calibrated(\n",
    "    C=1000, n_small=12, delta=0.003,  # delta 控制大件相对优势\n",
    "    w_lo=0.90, w_hi=0.98,             # 小件重量逼近容量 -> 难例\n",
    "    d_small=1.01, noise_frac=0.002,   # 小件密度略高，诱导密度贪心\n",
    "    seed=0\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "    items=[]\n",
    "    for i in range(n_small):\n",
    "        w = rng.randint(int(w_lo*C), int(w_hi*C))\n",
    "        noise = rng.uniform(-noise_frac, noise_frac)*C\n",
    "        v = int(math.ceil(d_small*w + noise))\n",
    "        items.append(Item(w=max(1,w), v=max(1,v), idx=i))\n",
    "    d_big = 0.95*d_small + delta\n",
    "    items.append(Item(C, int(math.ceil(d_big*C)), n_small))\n",
    "    return items, C\n",
    "\n",
    "def paired_stats_gap(delta=0.003, repeats=10, samples=30, seed0=2024):\n",
    "    rows=[]\n",
    "    for rep in range(repeats):\n",
    "        for s in range(samples):\n",
    "            items, C = gen_greedy_trap_calibrated(delta=delta, seed=seed0+rep*100+s)\n",
    "            opt,_ = dp_optimal(items, C)\n",
    "            gd,_ = algo_beam_search(items, C)\n",
    "            gm,_ = algo_beam_search_optimized(items, C)\n",
    "            gap_gd = (opt-gd)/opt if opt>0 else 0.0\n",
    "            gap_gm = (opt-gm)/opt if opt>0 else 0.0\n",
    "            rows.append({\"rep\":rep,\"sample\":s,\"gap_gd\":gap_gd,\"gap_gm\":gap_gm})\n",
    "    df_local = pd.DataFrame(rows)\n",
    "    diffs = (df_local[\"gap_gm\"] - df_local[\"gap_gd\"]).to_numpy()\n",
    "    t_stat, p_t = stats.ttest_rel(df_local[\"gap_gm\"], df_local[\"gap_gd\"])\n",
    "    sd = np.std(diffs, ddof=1) if len(diffs)>1 else 0.0\n",
    "    cohen_d = (np.mean(diffs)/sd) if sd>0 else np.nan\n",
    "    win_gm = float(np.mean(df_local[\"gap_gm\"] < df_local[\"gap_gd\"]))\n",
    "    tie    = float(np.mean(np.isclose(df_local[\"gap_gm\"], df_local[\"gap_gd\"])))\n",
    "    return df_local, float(p_t), float(cohen_d), float(win_gm), float(tie)\n",
    "\n",
    "# 1) 单一 δ 的“配对差值箱线图 + 裁剪 -log10(p)”\n",
    "delta0 = 0.003\n",
    "df_soft, p_soft, d_soft, win_soft, tie_soft = paired_stats_gap(delta=delta0)\n",
    "\n",
    "plt.figure()\n",
    "# Matplotlib 3.9+: 使用 tick_labels（避免 DeprecationWarning）\n",
    "plt.boxplot((df_soft[\"gap_gm\"] - df_soft[\"gap_gd\"]).to_numpy(),\n",
    "            vert=True, labels=[f\"GreedyTrap-ADV (δ={delta0})\"])\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.ylabel(\"Paired gap difference (G+Max - GD)\")\n",
    "plt.title(\"Paired gap differences (lower < 0 favors G+Max)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_gap_diff_box_ADV.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "val = -np.log10(max(min(p_soft, 1.0), 1e-300))\n",
    "cap = 10.0  # y 轴上限避免“满屏”\n",
    "plt.bar([f\"GreedyTrap-ADV (δ={delta0})\"], [min(val, cap)])\n",
    "if val > cap:\n",
    "    plt.text(0, cap*0.95, f\"{val:.1f}\", ha=\"center\", va=\"top\")  # 标注真实 -log10(p)\n",
    "plt.ylim(0, cap+0.5)\n",
    "plt.ylabel(\"-log10 p (paired t-test)\")\n",
    "plt.title(\"Significance (adversarial but soft)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_significance_bars_ADV_capped.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"delta\": delta0, \"-log10_p\": val, \"p_value\": p_soft,\n",
    "    \"cohen_d\": d_soft, \"win_rate_G+Max\": win_soft, \"tie_rate\": tie_soft,\n",
    "    \"n_pairs\": len(df_soft)\n",
    "}]).to_csv(os.path.join(ANALYSIS_DIR, \"adv_soft_summary.csv\"), index=False)\n",
    "\n",
    "# 2) 难度扫描：p 值与效应量随 delta 的变化趋势\n",
    "deltas = [0.001, 0.002, 0.003, 0.004, 0.006, 0.008, 0.010]\n",
    "scan_rows=[]\n",
    "for dlt in deltas:\n",
    "    df_tmp, p_tmp, d_tmp, win_tmp, tie_tmp = paired_stats_gap(delta=dlt, repeats=8, samples=20, seed0=3000)\n",
    "    scan_rows.append({\n",
    "        \"delta\": dlt,\n",
    "        \"-log10_p\": -np.log10(max(min(p_tmp, 1.0), 1e-300)),\n",
    "        \"p_value\": p_tmp, \"cohen_d\": d_tmp,\n",
    "        \"win_rate_G+Max\": win_tmp, \"tie_rate\": tie_tmp,\n",
    "        \"n_pairs\": len(df_tmp)\n",
    "    })\n",
    "scan = pd.DataFrame(scan_rows)\n",
    "scan.to_csv(os.path.join(ANALYSIS_DIR, \"adv_delta_scan.csv\"), index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scan[\"delta\"], np.minimum(scan[\"-log10_p\"], 15.0), marker=\"o\")\n",
    "plt.xlabel(\"delta (big item advantage over 0.95·small density)\")\n",
    "plt.ylabel(\"-log10 p (capped at 15)\")\n",
    "plt.title(\"Significance vs. adversarial difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_pvalue_vs_delta.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scan[\"delta\"], scan[\"cohen_d\"], marker=\"o\")\n",
    "plt.xlabel(\"delta\"); plt.ylabel(\"Cohen's d (paired)\")\n",
    "plt.title(\"Effect size vs. adversarial difficulty\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ANALYSIS_DIR, \"fig_effectsize_vs_delta.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# ---------------- 输出索引 ----------------\n",
    "print(\"生成完成（Section 5 合并版）：以下文件已输出：\")\n",
    "for p in [\n",
    "    \"fig_gap_by_class.png\",\n",
    "    \"fig_time_by_class.png\",\n",
    "    \"advantage_heatmap_uncorrelated.csv\",\n",
    "    \"fig_advantage_heatmap_uncorrelated.png\",\n",
    "    \"fig_advantage_diff_uncorrelated.png\",\n",
    "    \"time_prediction_micro.csv\",\n",
    "    \"fig_time_pred_micro_Bounded_BeamSearch.png\",\n",
    "    \"fig_time_pred_micro_Classic_BeamSearch.png\",\n",
    "    \"significance_tests.csv\",\n",
    "    \"fig_significance_bars.png\",\n",
    "    \"winrate_summary_by_class.csv\",\n",
    "    \"fig_ci_uncorrelated.png\",\n",
    "    \"significance_tests_stress.csv\",\n",
    "    \"fig_gap_diff_box_ADV.png\",\n",
    "    \"fig_significance_bars_ADV_capped.png\",\n",
    "    \"adv_soft_summary.csv\",\n",
    "    \"adv_delta_scan.csv\",\n",
    "    \"fig_pvalue_vs_delta.png\",\n",
    "    \"fig_effectsize_vs_delta.png\",\n",
    "]:\n",
    "    print(\" -\", os.path.join(ANALYSIS_DIR, p))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
