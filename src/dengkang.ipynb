{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e1e3812a9483e8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 0/1背包问题的分支限界算法设计与实现\n",
    "**姓名：邓康**\n",
    "**学号：2025439148**\n",
    "**学院：天津大学福州国际联合学院**\n",
    "**专业：电子信息**\n",
    "\n",
    "**目标**：在 0/1 背包（每个物品要么选要么不选）中，设计基于分支限界的经典方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 算法 A：Branch-and-Bound Basic（基础分支限界）\n",
    "\n",
    "- **思路**：采用深度优先搜索（DFS）策略，在递归树中枚举每个物品的选（1）与不选（0）分支。在每一个树节点，算法计算一个分数背包上界：假设后续物品可以按价值密度$（\\rho_i = \\frac{v_i}{w_i}）$\n",
    "降序部分装入，由此得到的最大可能价值\n",
    "\n",
    "- **优点**：能保证找到全局最优解。分数背包上界计算高效，提供了有效的剪枝依据\n",
    "\n",
    "- **缺点**：搜索策略（DFS）相对盲目，可能长时间探索潜力较低的子树。剪枝仅依赖一个较宽松的上界，在复杂实例中搜索树规模仍可能爆炸性增长，求解效率不稳定。\n",
    "\n",
    "---\n",
    "\n",
    "## 算法 B：Enhanced Branch-and-Bound via Probing & Node Selection（探测和最佳界限节点选择优化）\n",
    "\n",
    "- **思路**：在正式对一个变量（物品）进行分支（设为0或1）之前，进行预探索,临时固定该变量，快速评估其两个子问题的线性松弛上界。在节点选择上选择优先级最高（即上界最紧、理论上潜力最大）的节点。\n",
    "  $$\n",
    "  P(node)=UB_{frac}(node)\n",
    "  $$\n",
    "\n",
    "- **作用**：改变了基础版本中深度优先的“盲目”搜索顺序。该策略能引导算法最快地找到高质量可行解，从而迅速提升全局下界，配合探测技术实现更早、更有效的剪\n",
    "\n",
    "- **理论性质（这里只做提示，证明放在'理论分析'章节）**：“探测”与“基于上界的节点选择”相结合，能系统性减少为证明最优性所需探索的搜索树规模（节点数）。这直接转化为更短的求解时间，尤其是在那些传统方法需要大量回溯的“难”实例上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f65f8cae55af628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:56:49.008104Z",
     "start_time": "2025-11-12T14:56:48.996559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional\n",
    "import heapq\n",
    "from copy import deepcopy\n",
    "\n",
    "@dataclass\n",
    "class Item:\n",
    "    \"\"\"物品结构\"\"\"\n",
    "    w: int  # 重量\n",
    "    v: int  # 价值\n",
    "    idx: int  # 原始索引\n",
    "    \n",
    "    @property\n",
    "    def density(self) -> float:\n",
    "        \"\"\"价值密度\"\"\"\n",
    "        return self.v / self.w if self.w > 0 else 0\n",
    "\n",
    "@dataclass(order=True)\n",
    "class BBNode:\n",
    "    \"\"\"\n",
    "    分支限界树节点结构（用于增强版优先队列）\n",
    "    按上界值降序排列，Python的heapq是最小堆，因此用负值实现最大堆效果\n",
    "    \"\"\"\n",
    "    neg_bound: float = field(compare=True)  # 负的上界值（用于优先队列排序）\n",
    "    level: int = field(compare=False)       # 决策层级（已考虑的物品数）\n",
    "    value: int = field(compare=False)       # 当前总价值\n",
    "    weight: int = field(compare=False)      # 当前总重量\n",
    "    taken: List[int] = field(compare=False) # 已选物品索引\n",
    "    bound: float = field(compare=False)     # 上界值\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"确保taken列表是副本，避免引用问题\"\"\"\n",
    "        self.taken = self.taken.copy()\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def fractional_knapsack_bound(\n",
    "    items: List[Item],\n",
    "    capacity: int,\n",
    "    level: int,\n",
    "    current_value: int,\n",
    "    current_weight: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算分数背包上界（贪心松弛上界）\n",
    "    \n",
    "    返回: (上界值, 分数部分的价值)\n",
    "    原理:\n",
    "    1. 从第level个物品开始（0-indexed，假设items已按密度降序排序）\n",
    "    2. 尽可能装入完整物品\n",
    "    3. 剩余容量装入第一个无法完整装入物品的一部分（按密度比例）\n",
    "    \"\"\"\n",
    "    if current_weight > capacity:\n",
    "        return -float('inf'), 0.0  # 不可行节点\n",
    "    \n",
    "    bound_value = float(current_value)\n",
    "    remaining_capacity = capacity - current_weight\n",
    "    \n",
    "    # 遍历剩余物品\n",
    "    for i in range(level, len(items)):\n",
    "        if items[i].w <= remaining_capacity:\n",
    "            # 可以完整装入\n",
    "            bound_value += items[i].v\n",
    "            remaining_capacity -= items[i].w\n",
    "        else:\n",
    "            # 只能装入一部分\n",
    "            fraction = remaining_capacity / items[i].w\n",
    "            fractional_value = items[i].v * fraction\n",
    "            bound_value += fractional_value\n",
    "            return bound_value, fractional_value\n",
    "    \n",
    "    return bound_value, 0.0  # 所有物品都能完整装入\n",
    "\n",
    "def sort_items_by_density(items: List[Item]) -> List[Item]:\n",
    "    \"\"\"按价值密度降序排序物品（用于上界计算）\"\"\"\n",
    "    return sorted(items, key=lambda x: x.v / x.w, reverse=True)\n",
    "\n",
    "# ---------- 算法一：基础分支限界法 (bb_basic) ----------\n",
    "def branch_and_bound_basic(\n",
    "    items: List[Item],\n",
    "    capacity: int\n",
    ") -> Tuple[int, List[int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    基础分支限界法（深度优先搜索 + 分数背包上界剪枝）\n",
    "    返回: (最优价值, 最优解物品索引列表, 性能指标字典)\n",
    "    \n",
    "    算法特点:\n",
    "    1. 深度优先搜索（栈实现）\n",
    "    2. 物品按价值密度降序预处理，以获得更紧的上界\n",
    "    3. 使用分数背包上界进行剪枝\n",
    "    \n",
    "    性能指标:\n",
    "    - nodes_generated: 生成的节点总数\n",
    "    - nodes_explored: 实际探索的节点数\n",
    "    - first_opt_node: 首次找到最优解的节点编号\n",
    "    \"\"\"\n",
    "    # 按价值密度降序排序（提高上界质量）\n",
    "    sorted_items = sort_items_by_density(items)\n",
    "    n = len(sorted_items)\n",
    "    \n",
    "    # 性能指标记录\n",
    "    metrics = {\n",
    "        'nodes_generated': 1,  # 初始节点\n",
    "        'nodes_explored': 0,\n",
    "        'first_opt_node': 0\n",
    "    }\n",
    "    \n",
    "    # 全局最优解记录\n",
    "    best_value = 0\n",
    "    best_taken = []\n",
    "    \n",
    "    # 使用栈进行DFS（每个元素: (level, value, weight, taken)）\n",
    "    stack = [(0, 0, 0, [])]  # 从第0层开始\n",
    "    \n",
    "    while stack:\n",
    "        level, value, weight, taken = stack.pop()\n",
    "        metrics['nodes_explored'] += 1\n",
    "        \n",
    "        # 检查是否到达叶子节点\n",
    "        if level == n:\n",
    "            if value > best_value and weight <= capacity:\n",
    "                best_value = value\n",
    "                best_taken = taken\n",
    "                if metrics['first_opt_node'] == 0:\n",
    "                    metrics['first_opt_node'] = metrics['nodes_explored']\n",
    "            continue\n",
    "        \n",
    "        # 计算上界（包括当前节点的价值）\n",
    "        bound, _ = fractional_knapsack_bound(\n",
    "            sorted_items, capacity, level, value, weight\n",
    "        )\n",
    "        \n",
    "        # 剪枝：如果上界不大于当前最优解，放弃该分支\n",
    "        if bound <= best_value:\n",
    "            continue\n",
    "        \n",
    "        # 处理当前物品\n",
    "        current_item = sorted_items[level]\n",
    "        \n",
    "        # 分支1：不选当前物品（左子树）\n",
    "        stack.append((level + 1, value, weight, taken.copy()))\n",
    "        metrics['nodes_generated'] += 1\n",
    "        \n",
    "        # 分支2：选当前物品（右子树）- 需检查可行性\n",
    "        new_weight = weight + current_item.w\n",
    "        if new_weight <= capacity:\n",
    "            new_taken = taken + [current_item.idx]  # 记录原始索引\n",
    "            new_value = value + current_item.v\n",
    "            stack.append((level + 1, new_value, new_weight, new_taken))\n",
    "            metrics['nodes_generated'] += 1\n",
    "    \n",
    "    return best_value, best_taken, metrics\n",
    "\n",
    "# ---------- 算法二：增强分支限界法 (bb_enhanced) ----------\n",
    "def branch_and_bound_enhanced(\n",
    "    items: List[Item],\n",
    "    capacity: int\n",
    ") -> Tuple[int, List[int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    增强分支限界法（探测技术 + 最佳上界优先）\n",
    "    \n",
    "    返回: (最优价值, 最优解物品索引列表, 性能指标字典)\n",
    "    \n",
    "    核心优化：\n",
    "    1. 探测技术（Probing）：分支前评估子问题潜力，提前剪枝无望分支\n",
    "    2. 最佳界限节点选择（Best-Bound Node Selection）：总是扩展上界最高的节点\n",
    "    \n",
    "    性能指标:\n",
    "    - nodes_generated: 生成的节点总数\n",
    "    - nodes_explored: 实际探索的节点数\n",
    "    - first_opt_node: 首次找到最优解的节点编号\n",
    "    - nodes_pruned_by_probing: 被探测技术剪枝的节点数\n",
    "    \"\"\"\n",
    "    # 按价值密度降序排序\n",
    "    sorted_items = sort_items_by_density(items)\n",
    "    n = len(sorted_items)\n",
    "    \n",
    "    # 性能指标记录\n",
    "    metrics = {\n",
    "        'nodes_generated': 1,  # 初始节点\n",
    "        'nodes_explored': 0,\n",
    "        'first_opt_node': 0,\n",
    "        'nodes_pruned_by_probing': 0\n",
    "    }\n",
    "    \n",
    "    # 全局最优解记录\n",
    "    best_value = 0\n",
    "    best_taken = []\n",
    "    \n",
    "    # 活节点表（优先队列）- 实现最佳界限优先\n",
    "    priority_queue = []\n",
    "    \n",
    "    # 创建初始节点（包含上界信息）\n",
    "    initial_bound, _ = fractional_knapsack_bound(\n",
    "        sorted_items, capacity, 0, 0, 0\n",
    "    )\n",
    "    heapq.heappush(\n",
    "        priority_queue,\n",
    "        BBNode(\n",
    "            neg_bound=-initial_bound,  # 负值实现最大堆（优先取上界高的）\n",
    "            level=0,\n",
    "            value=0,\n",
    "            weight=0,\n",
    "            taken=[],\n",
    "            bound=initial_bound\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    while priority_queue:\n",
    "        # 1：最佳界限节点选择\n",
    "        current_node = heapq.heappop(priority_queue)\n",
    "        metrics['nodes_explored'] += 1\n",
    "        \n",
    "        level = current_node.level\n",
    "        value = current_node.value\n",
    "        weight = current_node.weight\n",
    "        taken = current_node.taken\n",
    "        current_bound = current_node.bound\n",
    "        \n",
    "        # 检查是否到达叶子节点\n",
    "        if level == n:\n",
    "            if value > best_value and weight <= capacity:\n",
    "                best_value = value\n",
    "                best_taken = taken\n",
    "                if metrics['first_opt_node'] == 0:\n",
    "                    metrics['first_opt_node'] = metrics['nodes_explored']\n",
    "            continue\n",
    "        \n",
    "        current_item = sorted_items[level]\n",
    "        \n",
    "        # 2：探测技术（Probing）\n",
    "        # 评估左分支（不选当前物品）\n",
    "        left_bound, _ = fractional_knapsack_bound(\n",
    "            sorted_items, capacity, level + 1, value, weight\n",
    "        )\n",
    "        \n",
    "        # 评估右分支（选当前物品）- 仅在可行时\n",
    "        new_weight = weight + current_item.w\n",
    "        right_bound = -float('inf')\n",
    "        right_feasible = new_weight <= capacity\n",
    "        \n",
    "        if right_feasible:\n",
    "            new_value = value + current_item.v\n",
    "            right_bound, _ = fractional_knapsack_bound(\n",
    "                sorted_items, capacity, level + 1, new_value, new_weight\n",
    "            )\n",
    "        \n",
    "        # 基于探测结果的剪枝决策\n",
    "        max_child_bound = max(left_bound, right_bound) if right_feasible else left_bound\n",
    "        \n",
    "        if max_child_bound <= best_value:\n",
    "            # 所有子分支都不可能产生更优解，完全剪枝该节点\n",
    "            metrics['nodes_pruned_by_probing'] += 1\n",
    "            continue\n",
    "        \n",
    "        # 创建子节点，按上界值加入优先队列\n",
    "        # 先创建右分支（选当前物品）\n",
    "        if right_feasible and right_bound > best_value:\n",
    "            new_taken = taken + [current_item.idx]\n",
    "            new_value = value + current_item.v\n",
    "            \n",
    "            right_node = BBNode(\n",
    "                neg_bound=-right_bound,\n",
    "                level=level + 1,\n",
    "                value=new_value,\n",
    "                weight=new_weight,\n",
    "                taken=new_taken,\n",
    "                bound=right_bound\n",
    "            )\n",
    "            heapq.heappush(priority_queue, right_node)\n",
    "            metrics['nodes_generated'] += 1\n",
    "        \n",
    "        # 创建左分支（不选当前物品）\n",
    "        if left_bound > best_value:\n",
    "            left_node = BBNode(\n",
    "                neg_bound=-left_bound,\n",
    "                level=level + 1,\n",
    "                value=value,\n",
    "                weight=weight,\n",
    "                taken=taken.copy(),\n",
    "                bound=left_bound\n",
    "            )\n",
    "            heapq.heappush(priority_queue, left_node)\n",
    "            metrics['nodes_generated'] += 1\n",
    "    \n",
    "    return best_value, best_taken, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e18f306a2e6a68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 理论分析（两种分支限界方法）\n",
    "\n",
    "**问题**\n",
    "给定物品集合 $\\{(w_i,v_i)\\}_{i=1}^n$ 与容量 $C$，选择子集使得总重量 $\\le C$ 且总价值最大（0/1 背包）。\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 A：Branch-and-Bound Basic（基础分支限界法）\n",
    "- **设计思想**：采用深度优先搜索策略，在递归树中枚举每个物品的选（1）与不选（0）分支。在每一个树节点，算法计算一个分数背包上界：假设后续物品可以按价值密度$\\rho_i = v_i/w_i$降序部分装入，由此得到的最大可能价值。若某节点的上界不大于当前全局最优解，则剪除该分支。\n",
    "- **时间复杂度**：最坏情况下需探索指数级数量的节点$O(2^n)$。通过上界剪枝，实际探索的节点数通常远小于理论最坏情况。每次节点处理需计算分数背包上界，其复杂度为$O(n)$,总时间复杂度上界为$O(2^n⋅n)$。\n",
    "- **空间复杂度**：深度优先搜索使用栈最多同时存储$O(n)$个节点。\n",
    "- **性能保证**：分支限界法是一种精确算法，能够保证找到全局最优解。\n",
    "- **关键瓶颈**：\n",
    "  1) **搜索策略的盲目性**：深度优先搜索可能长时间陷入非最优的分支，导致延迟找到高质量可行解，从而影响全局剪枝效率；\n",
    "  2) **上界紧度有限**：分数背包上界虽比线性松弛更紧，但对某些结构化实例（如物品价值与重量高度相关）仍然不够紧，导致剪枝效果不足；\n",
    "  3) **节点扩展顺序固定**：按物品顺序分支（先考虑不选，再考虑选），缺乏对问题特征的适应性调整。\n",
    "\n",
    "---\n",
    "\n",
    "### 方法 B：Enhanced Branch-and-Bound via Probing & Node Selection（探测和最佳界限节点选择优化）\n",
    "- **设计思想**：在正式对一个变量（物品）进行分支之前，进行预探索。使用优先队列管理活节点，总是选择上界值最高的节点进行扩展。\n",
    "- **时间复杂度**：最坏情况下仍为$O(2^n)$，但期望通过更强的剪枝（探测）和更优的节点选择，大幅减少实际探索的节点数。\n",
    "- **空间复杂度**：优先队列可能存储大量节点，最坏情况下空间复杂度$O(2^n)$。\n",
    "- **性能保证**：增强分支限界法同样是一种精确算法，保证找到全局最优解。优化组件仅改变搜索顺序和剪枝时机，不改变算法完备性。\n",
    "(i) 搜索效率提升：论文通过大量实验证明，\"探测\"与\"基于上界的节点选择\"相结合，能系统性减少为证明最优性所需探索的搜索树规模（节点数）；\n",
    "(ii) 更快找到高质量解：最佳界限优先策略能引导算法迅速找到优质可行解，从而快速提升全局下界，加速后续剪枝；\n",
    "(iii) 适应性：探测技术能根据实例特征动态调整搜索方向，对传统方法难以处理的\"贪心陷阱\"类实例效果显著。\n",
    "- **关键瓶颈**：\n",
    "  1) **空间复杂度**：最佳界限优先策略需使用优先队列维护所有活节点，在最坏情况下队列规模可达 $O(2^n)$；\n",
    "  2) **探测计算开销**：每个节点的分支前需额外计算两次完整的上界评估（左、右分支），若探测剪枝效果不明显，额外计算可能成为主要开销；\n",
    "  3) **浮点数精度问题**：分数背包上界涉及浮点运算，可能因数值精度误差导致错误的剪枝决策。\n",
    "\n",
    "---\n",
    "\n",
    "### 比较小结\n",
    "- **复杂度**：两者最坏时间均为$O(2^n⋅n)$，但增强版通过前沿优化期望大幅减少实际探索节点数；\n",
    "- **保证**：两者均为精确算法，保证找到最优解;\n",
    "- **适用性**：增强版在大多数实例上，尤其是传统方法需大量回溯的\"难\"实例，应表现出更快的求解速度与更少的节点探索。\n",
    "\n",
    "\n",
    "### 参考文献\n",
    "- 1.Land, A. H. & Doig, A. G. (1960). An Automatic Method of Solving Discrete Programming Problems. Econometrica, 28, pp. 497-520.\n",
    "- 2.Mans, Bernard & Roucairol, Catherine. (1996). Performance of parallel branch-and-bound algorithms with best-first search. Discrete Applied Mathematics. 66. 57-74. 10.     1016/0166-218X(94)00137-3. \n",
    "- 3.David R. Morrison, Sheldon H. Jacobson, Jason J. Sauppe, and Edward C. Sewell. 2016. Branch-and-bound algorithms. Discret. Optim. 19, C (February 2016), 79–102.doi.org/10.1016/j.disopt.2016.01.005\n",
    "- 4.Forget, N. and Parragh, S. N., “Enhancing Branch-and-Bound for Multi-Objective 0-1 Programming”, <i>arXiv e-prints</i>, Art. no. arXiv:2210.05385, 2022. doi:10.48550/arXiv.2210.05385.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9b75c38117fda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 实验设计（只围绕两种算法：Branch-and-Bound Basic 与 Enhanced Branch-and-Bound via Probing & Node Selection）\n",
    "\n",
    "**目标**\n",
    "系统比较两种贪心方法在不同数据特性与规模下的运行表现，确保结果具备统计意义与可复现性。\n",
    "\n",
    "**实例族（≥5 类，覆盖结构化/随机与不同分布）**\n",
    "1) **Uncorrelated**：重量与价值独立均匀；\n",
    "2) **WeaklyCorr**：弱相关（$v \\approx w + \\text{noise}$）；\n",
    "3) **StronglyCorr**：强相关（$v = w + \\Delta$）；\n",
    "4) **GreedyTrap**：大量小而高密度物品 + 单个价值极高但密度略差的重物品；\n",
    "5) **SubsetSum**：子集和问题（价值=重量，搜索树对称，最优解通常位于边界）\n",
    "6) **ProbingBenefit**：探测优势类（一个“决定性的”高价值物品，一组“诱饵”物品：密度略高但总价值有限，一组“填充”物品：密度很低，价值有限）。\n",
    "\n",
    "**规模与重复**\n",
    "- 每类 **20 个样本 × ≥5 次随机重复**（本文设为 5），总计 ≥100 个样本/类；\n",
    "- 启用容量维度 DP（$O(nC)$）获得 OPT；\n",
    "- 固定随机种子方案（按类名、repeat、sample 组合）以保证可复现。\n",
    "\n",
    "**记录指标（本节只\"生成与运行\"，汇总统计留到下一节）**\n",
    "- **运行时间**（每次调用的 wall-clock）；\n",
    "- **生成节点数**（算法创建的所有节点，包括被剪枝的）；\n",
    "- **探索节点数**（算法实际从栈/队列中弹出并处理的节点总数）；\n",
    "- **剪枝比例**：(1 - 探索节点数/生成节点数)×100%其中\"生成节点数\"=所有创建的节点（包括被剪枝的）；\n",
    "- **首次最优节点**：算法首次达到全局最优解时已探索的节点数\n",
    "- **解值**（两算法的总价值与DP）\n",
    "- 将原始结果表保存为 CSV，便于后续计算均值/方差与显著性检验。\n",
    "\n",
    "**复现实验的关键点**\n",
    "- 统一容量比例（例如 $C = 0.5 \\sum w_i$），避免无意义的过松或过紧；\n",
    "- 采用固定的随机种子拼接策略，保证\"同一 (类, repeat, sample)\"跨算法完全一致；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81bba2796f4bfe3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:57:02.755503Z",
     "start_time": "2025-11-12T14:56:59.889356Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分支限界法对比实验：基础版本 vs 优化版本（带内存测量）\n",
      "配置: 6种实例类型 × 4种规模 × 20样本\n",
      "算法: ['bb_basic', 'bb_enhanced']\n",
      "\n",
      "[OK] Results saved to: outputs_branch_bound\\raw_results_branch_bound.csv\n",
      "\n",
      "原始数据文件前4行:\n",
      "          algo  value  runtime_s  nodes_generated  nodes_explored  pruning_ratio  first_opt_node  memory_peak_mb  opt_known  opt_value  opt_time_s  correct  capacity   n  sum_w  timeout error         class  sample     seed\n",
      "0     bb_basic    647   0.001303               81              81            0.0              16        0.001656       True        647    0.000908     True       500  15    661    False  None  Uncorrelated       0  7673696\n",
      "1  bb_enhanced    647   0.001750               59              59            0.0              34        0.008522       True        647    0.000908     True       500  15    661    False  None  Uncorrelated       0  7673696\n",
      "2     bb_basic    713   0.000285               43              43            0.0              16        0.001671       True        713    0.000896     True       500  15    632    False  None  Uncorrelated       1  7739233\n",
      "3  bb_enhanced    713   0.000849               36              36            0.0              20        0.004898       True        713    0.000896     True       500  15    632    False  None  Uncorrelated       1  7739233\n",
      "\n",
      "[OK] Summary saved to: outputs_branch_bound\\summary_preview_branch_bound.csv\n",
      "\n",
      "总结文件前4行:\n",
      "            class         algo  mean_time      var_time  mean_nodes     var_nodes  mean_memory  cnt\n",
      "0      GreedyTrap     bb_basic   0.000697  6.787352e-08     46.0000  1.265823e+02     0.003378   80\n",
      "1      GreedyTrap  bb_enhanced   0.001814  4.014862e-07     46.0000  1.265823e+02     0.007189   80\n",
      "2  ProbingBenefit     bb_basic   0.003969  4.037978e-05    634.5375  1.007935e+06     0.002560   80\n",
      "3  ProbingBenefit  bb_enhanced   0.013494  6.394190e-04    546.5125  9.294124e+05     0.058784   80\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第三节：实验设计 —— 可直接运行的实现代码\n",
    "\"\"\"\n",
    "分支限界法对比实验：基础版本 vs 优化版本\n",
    "实验框架参考：稳定哈希、实例生成器、可复现性\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import heapq\n",
    "import tracemalloc  # 添加内存测量\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zlib\n",
    "\n",
    "# ---------- 稳定哈希（用于可复现的随机种子） ----------\n",
    "def stable_hash_str(*parts) -> int:\n",
    "    \"\"\"跨平台/跨进程稳定的32位哈希，用于生成可复现的随机种子。\"\"\"\n",
    "    s = \"|\".join(str(p) for p in parts)\n",
    "    return zlib.adler32(s.encode(\"utf-8\")) & 0xffffffff\n",
    "\n",
    "# ---------- 随机与输出设置 ----------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "OUTDIR = \"outputs_branch_bound\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---------- 数据模型定义 ----------\n",
    "@dataclass\n",
    "class Item:\n",
    "    \"\"\"物品结构\"\"\"\n",
    "    w: int  # 重量\n",
    "    v: int  # 价值\n",
    "    idx: int  # 原始索引\n",
    "    \n",
    "    @property\n",
    "    def density(self) -> float:\n",
    "        \"\"\"价值密度\"\"\"\n",
    "        return self.v / self.w if self.w > 0 else 0\n",
    "\n",
    "@dataclass(order=True)\n",
    "class BBNode:\n",
    "    \"\"\"\n",
    "    分支限界树节点结构（用于增强版优先队列）\n",
    "    按上界值降序排列，Python的heapq是最小堆，因此用负值实现最大堆效果\n",
    "    \"\"\"\n",
    "    neg_bound: float = field(compare=True)  # 负的上界值（用于优先队列排序）\n",
    "    level: int = field(compare=False)       # 决策层级（已考虑的物品数）\n",
    "    value: int = field(compare=False)       # 当前总价值\n",
    "    weight: int = field(compare=False)      # 当前总重量\n",
    "    taken: List[int] = field(compare=False) # 已选物品索引\n",
    "    bound: float = field(compare=False)     # 上界值\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"确保taken列表是副本，避免引用问题\"\"\"\n",
    "        self.taken = self.taken.copy()\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def fractional_knapsack_bound(\n",
    "    items: List[Item],\n",
    "    capacity: int,\n",
    "    level: int,\n",
    "    current_value: int,\n",
    "    current_weight: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算分数背包上界（贪心松弛上界）\n",
    "    \n",
    "    返回: (上界值, 分数部分的价值)\n",
    "    原理:\n",
    "    1. 从第level个物品开始（0-indexed，假设items已按密度降序排序）\n",
    "    2. 尽可能装入完整物品\n",
    "    3. 剩余容量装入第一个无法完整装入物品的一部分（按密度比例）\n",
    "    \"\"\"\n",
    "    if current_weight > capacity:\n",
    "        return -float('inf'), 0.0  # 不可行节点\n",
    "    \n",
    "    bound_value = float(current_value)\n",
    "    remaining_capacity = capacity - current_weight\n",
    "    \n",
    "    # 遍历剩余物品\n",
    "    for i in range(level, len(items)):\n",
    "        if items[i].w <= remaining_capacity:\n",
    "            # 可以完整装入\n",
    "            bound_value += items[i].v\n",
    "            remaining_capacity -= items[i].w\n",
    "        else:\n",
    "            # 只能装入一部分\n",
    "            fraction = remaining_capacity / items[i].w\n",
    "            fractional_value = items[i].v * fraction\n",
    "            bound_value += fractional_value\n",
    "            return bound_value, fractional_value\n",
    "    \n",
    "    return bound_value, 0.0  # 所有物品都能完整装入\n",
    "\n",
    "def sort_items_by_density(items: List[Item]) -> List[Item]:\n",
    "    \"\"\"按价值密度降序排序物品（用于上界计算）\"\"\"\n",
    "    return sorted(items, key=lambda x: x.v / x.w, reverse=True)\n",
    "\n",
    "# ---------- 动态规划验证 ----------\n",
    "def dp_optimal(items: List[Item], C: int) -> Tuple[int, List[int]]:\n",
    "    \"\"\"容量维度 DP，O(nC)。规模过大时仅返回值，不回溯。\"\"\"\n",
    "    n = len(items)\n",
    "    dp = [0]*(C+1)\n",
    "    if (n+1)*(C+1) <= 2_000_000:\n",
    "        take = [[False]*(C+1) for _ in range(n)]\n",
    "        for i, it in enumerate(items):\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "                    take[i][c] = True\n",
    "        c = max(range(C+1), key=lambda x: dp[x])\n",
    "        chosen = []\n",
    "        for i in range(n-1, -1, -1):\n",
    "            if take[i][c]:\n",
    "                chosen.append(items[i].idx)\n",
    "                c -= items[i].w\n",
    "        return max(dp), chosen[::-1]\n",
    "    else:\n",
    "        for it in items:\n",
    "            w, v = it.w, it.v\n",
    "            for c in range(C, w-1, -1):\n",
    "                if dp[c-w] + v > dp[c]:\n",
    "                    dp[c] = dp[c-w] + v\n",
    "        return max(dp), []\n",
    "\n",
    "# ---------- 算法一：基础分支限界法 (bb_basic) ----------\n",
    "def branch_and_bound_basic(\n",
    "    items: List[Item],\n",
    "    capacity: int\n",
    ") -> Tuple[int, List[int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    基础分支限界法（深度优先搜索 + 分数背包上界剪枝）\n",
    "    返回: (最优价值, 最优解物品索引列表, 性能指标字典)\n",
    "    \n",
    "    算法特点:\n",
    "    1. 深度优先搜索（栈实现）\n",
    "    2. 物品按价值密度降序预处理，以获得更紧的上界\n",
    "    3. 使用分数背包上界进行剪枝\n",
    "    \n",
    "    性能指标:\n",
    "    - nodes_generated: 生成的节点总数\n",
    "    - nodes_explored: 实际探索的节点数\n",
    "    - first_opt_node: 首次找到最优解的节点编号\n",
    "    \"\"\"\n",
    "    # 按价值密度降序排序（提高上界质量）\n",
    "    sorted_items = sort_items_by_density(items)\n",
    "    n = len(sorted_items)\n",
    "    \n",
    "    # 性能指标记录\n",
    "    metrics = {\n",
    "        'nodes_generated': 1,  # 初始节点\n",
    "        'nodes_explored': 0,\n",
    "        'first_opt_node': 0\n",
    "    }\n",
    "    \n",
    "    # 全局最优解记录\n",
    "    best_value = 0\n",
    "    best_taken = []\n",
    "    \n",
    "    # 使用栈进行DFS（每个元素: (level, value, weight, taken)）\n",
    "    stack = [(0, 0, 0, [])]  # 从第0层开始\n",
    "    \n",
    "    while stack:\n",
    "        level, value, weight, taken = stack.pop()\n",
    "        metrics['nodes_explored'] += 1\n",
    "        \n",
    "        # 检查是否到达叶子节点\n",
    "        if level == n:\n",
    "            if value > best_value and weight <= capacity:\n",
    "                best_value = value\n",
    "                best_taken = taken\n",
    "                if metrics['first_opt_node'] == 0:\n",
    "                    metrics['first_opt_node'] = metrics['nodes_explored']\n",
    "            continue\n",
    "        \n",
    "        # 计算上界（包括当前节点的价值）\n",
    "        bound, _ = fractional_knapsack_bound(\n",
    "            sorted_items, capacity, level, value, weight\n",
    "        )\n",
    "        \n",
    "        # 剪枝：如果上界不大于当前最优解，放弃该分支\n",
    "        if bound <= best_value:\n",
    "            continue\n",
    "        \n",
    "        # 处理当前物品\n",
    "        current_item = sorted_items[level]\n",
    "        \n",
    "        # 分支1：不选当前物品（左子树）\n",
    "        stack.append((level + 1, value, weight, taken.copy()))\n",
    "        metrics['nodes_generated'] += 1\n",
    "        \n",
    "        # 分支2：选当前物品（右子树）- 需检查可行性\n",
    "        new_weight = weight + current_item.w\n",
    "        if new_weight <= capacity:\n",
    "            new_taken = taken + [current_item.idx]  # 记录原始索引\n",
    "            new_value = value + current_item.v\n",
    "            stack.append((level + 1, new_value, new_weight, new_taken))\n",
    "            metrics['nodes_generated'] += 1\n",
    "    \n",
    "    return best_value, best_taken, metrics\n",
    "\n",
    "# ---------- 算法二：增强分支限界法 (bb_enhanced) ----------\n",
    "def branch_and_bound_enhanced(\n",
    "    items: List[Item],\n",
    "    capacity: int\n",
    ") -> Tuple[int, List[int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    增强分支限界法（探测技术 + 最佳上界优先）\n",
    "    \n",
    "    返回: (最优价值, 最优解物品索引列表, 性能指标字典)\n",
    "    \n",
    "    核心优化：\n",
    "    1. 探测技术（Probing）：分支前评估子问题潜力，提前剪枝无望分支\n",
    "    2. 最佳界限节点选择（Best-Bound Node Selection）：总是扩展上界最高的节点\n",
    "    \n",
    "    性能指标:\n",
    "    - nodes_generated: 生成的节点总数\n",
    "    - nodes_explored: 实际探索的节点数\n",
    "    - first_opt_node: 首次找到最优解的节点编号\n",
    "    - nodes_pruned_by_probing: 被探测技术剪枝的节点数\n",
    "    \"\"\"\n",
    "    # 按价值密度降序排序\n",
    "    sorted_items = sort_items_by_density(items)\n",
    "    n = len(sorted_items)\n",
    "    \n",
    "    # 性能指标记录\n",
    "    metrics = {\n",
    "        'nodes_generated': 1,  # 初始节点\n",
    "        'nodes_explored': 0,\n",
    "        'first_opt_node': 0,\n",
    "        'nodes_pruned_by_probing': 0\n",
    "    }\n",
    "    \n",
    "    # 全局最优解记录\n",
    "    best_value = 0\n",
    "    best_taken = []\n",
    "    \n",
    "    # 活节点表（优先队列）- 实现最佳界限优先\n",
    "    priority_queue = []\n",
    "    \n",
    "    # 创建初始节点（包含上界信息）\n",
    "    initial_bound, _ = fractional_knapsack_bound(\n",
    "        sorted_items, capacity, 0, 0, 0\n",
    "    )\n",
    "    heapq.heappush(\n",
    "        priority_queue,\n",
    "        BBNode(\n",
    "            neg_bound=-initial_bound,  # 负值实现最大堆（优先取上界高的）\n",
    "            level=0,\n",
    "            value=0,\n",
    "            weight=0,\n",
    "            taken=[],\n",
    "            bound=initial_bound\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    while priority_queue:\n",
    "        # 1：最佳界限节点选择\n",
    "        current_node = heapq.heappop(priority_queue)\n",
    "        metrics['nodes_explored'] += 1\n",
    "        \n",
    "        level = current_node.level\n",
    "        value = current_node.value\n",
    "        weight = current_node.weight\n",
    "        taken = current_node.taken\n",
    "        current_bound = current_node.bound\n",
    "        \n",
    "        # 检查是否到达叶子节点\n",
    "        if level == n:\n",
    "            if value > best_value and weight <= capacity:\n",
    "                best_value = value\n",
    "                best_taken = taken\n",
    "                if metrics['first_opt_node'] == 0:\n",
    "                    metrics['first_opt_node'] = metrics['nodes_explored']\n",
    "            continue\n",
    "        \n",
    "        current_item = sorted_items[level]\n",
    "        \n",
    "        # 2：探测技术（Probing）\n",
    "        # 评估左分支（不选当前物品）\n",
    "        left_bound, _ = fractional_knapsack_bound(\n",
    "            sorted_items, capacity, level + 1, value, weight\n",
    "        )\n",
    "        \n",
    "        # 评估右分支（选当前物品）- 仅在可行时\n",
    "        new_weight = weight + current_item.w\n",
    "        right_bound = -float('inf')\n",
    "        right_feasible = new_weight <= capacity\n",
    "        \n",
    "        if right_feasible:\n",
    "            new_value = value + current_item.v\n",
    "            right_bound, _ = fractional_knapsack_bound(\n",
    "                sorted_items, capacity, level + 1, new_value, new_weight\n",
    "            )\n",
    "        \n",
    "        # 基于探测结果的剪枝决策\n",
    "        max_child_bound = max(left_bound, right_bound) if right_feasible else left_bound\n",
    "        \n",
    "        if max_child_bound <= best_value:\n",
    "            # 所有子分支都不可能产生更优解，完全剪枝该节点\n",
    "            metrics['nodes_pruned_by_probing'] += 1\n",
    "            continue\n",
    "        \n",
    "        # 创建子节点，按上界值加入优先队列\n",
    "        # 先创建右分支（选当前物品）\n",
    "        if right_feasible and right_bound > best_value:\n",
    "            new_taken = taken + [current_item.idx]\n",
    "            new_value = value + current_item.v\n",
    "            \n",
    "            right_node = BBNode(\n",
    "                neg_bound=-right_bound,\n",
    "                level=level + 1,\n",
    "                value=new_value,\n",
    "                weight=new_weight,\n",
    "                taken=new_taken,\n",
    "                bound=right_bound\n",
    "            )\n",
    "            heapq.heappush(priority_queue, right_node)\n",
    "            metrics['nodes_generated'] += 1\n",
    "        \n",
    "        # 创建左分支（不选当前物品）\n",
    "        if left_bound > best_value:\n",
    "            left_node = BBNode(\n",
    "                neg_bound=-left_bound,\n",
    "                level=level + 1,\n",
    "                value=value,\n",
    "                weight=weight,\n",
    "                taken=taken.copy(),\n",
    "                bound=left_bound\n",
    "            )\n",
    "            heapq.heappush(priority_queue, left_node)\n",
    "            metrics['nodes_generated'] += 1\n",
    "    \n",
    "    return best_value, best_taken, metrics\n",
    "\n",
    "# ---------- 算法字典 ----------\n",
    "ALGOS: Dict[str, Callable[[List[Item], int], Tuple[int, List[int], Dict]]] = {\n",
    "    \"bb_basic\": branch_and_bound_basic,\n",
    "    \"bb_enhanced\": branch_and_bound_enhanced,\n",
    "}\n",
    "\n",
    "# ---------- 实例生成器（6种类型） ----------\n",
    "C_BASE_DEFAULT = 1000\n",
    "\n",
    "def gen_uncorrelated(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"Uncorrelated: 重量价值独立均匀分布\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    for i in range(n):\n",
    "        w = rng.randint(1, 100)\n",
    "        v = rng.randint(1, 100)\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    return items, C\n",
    "\n",
    "def gen_weakly_correlated(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"WeaklyCorr: 弱相关 (v ≈ w + noise)\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    for i in range(n):\n",
    "        w = rng.randint(1, 100)\n",
    "        v = max(1, w + rng.randint(-10, 10))\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    return items, C\n",
    "\n",
    "def gen_strongly_correlated(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"StronglyCorr: 强相关 (v = w + Δ)\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    for i in range(n):\n",
    "        w = rng.randint(1, 100)\n",
    "        v = w + 10  # Δ = 10\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    return items, C\n",
    "\n",
    "def gen_greedy_trap(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"GreedyTrap: 贪心陷阱实例\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    # 第一个物品：重量大但密度极高\n",
    "    items.append(Item(w=100, v=1000, idx=0))\n",
    "    # 其他物品：重量小但密度略低\n",
    "    for i in range(1, n):\n",
    "        w = rng.randint(1, 10)\n",
    "        v = w * 8  # 密度8\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    return items, C\n",
    "\n",
    "def gen_subset_sum(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"SubsetSum: 子集和问题 (v = w)\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    for i in range(n):\n",
    "        w = rng.randint(1, 100)\n",
    "        items.append(Item(w=w, v=w, idx=i))\n",
    "    return items, C\n",
    "\n",
    "def gen_probing_benefit(n:int, seed:int, cap_ratio=0.5, C_base:int=C_BASE_DEFAULT):\n",
    "    \"\"\"ProbingBenefit: 专门突出探测优势的实例\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    C = int(C_base * cap_ratio)\n",
    "    items = []\n",
    "    # 前半部分：高密度小物品\n",
    "    for i in range(n // 2):\n",
    "        w = rng.randint(1, 5)\n",
    "        v = w * 20  # 高密度\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    # 后半部分：低密度大物品\n",
    "    for i in range(n // 2, n):\n",
    "        w = rng.randint(50, 100)\n",
    "        v = w // 2  # 低密度\n",
    "        items.append(Item(w=w, v=v, idx=i))\n",
    "    return items, C\n",
    "\n",
    "GENS = {\n",
    "    \"Uncorrelated\": gen_uncorrelated,\n",
    "    \"WeaklyCorr\": gen_weakly_correlated,\n",
    "    \"StronglyCorr\": gen_strongly_correlated,\n",
    "    \"GreedyTrap\": gen_greedy_trap,\n",
    "    \"SubsetSum\": gen_subset_sum,\n",
    "    \"ProbingBenefit\": gen_probing_benefit,\n",
    "}\n",
    "\n",
    "# ---------- 单实例评测 ----------\n",
    "def run_instance(items: List[Item], C: int, timeout: int = 300):\n",
    "    \"\"\"在单个实例上评测两种分支限界算法，返回记录\"\"\"\n",
    "    n = len(items)\n",
    "    total_w = sum(it.w for it in items)\n",
    "    \n",
    "    # 获取最优解（用于验证）\n",
    "    t0 = time.perf_counter()\n",
    "    if n <= 30 and C <= 10000:  # 小规模使用DP验证\n",
    "        opt_val, opt_taken = dp_optimal(items, C)\n",
    "        opt_known = True\n",
    "    else:\n",
    "        opt_val = 0\n",
    "        opt_taken = []\n",
    "        opt_known = False\n",
    "    opt_time = time.perf_counter() - t0\n",
    "    \n",
    "    rows = []\n",
    "    for algo_name, algo_func in ALGOS.items():\n",
    "        try:\n",
    "            # 设置超时机制\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # 开始内存追踪\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            # 运行算法\n",
    "            val, taken, metrics = algo_func(items, C)\n",
    "            exec_time = time.perf_counter() - start_time\n",
    "            \n",
    "            # 获取内存使用情况\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            # 检查超时\n",
    "            if timeout > 0 and exec_time > timeout:\n",
    "                raise TimeoutError(f\"Algorithm {algo_name} exceeded timeout\")\n",
    "            \n",
    "            # 计算剪枝比例\n",
    "            pruning_ratio = (1 - metrics['nodes_explored'] / max(metrics['nodes_generated'], 1)) * 100\n",
    "            \n",
    "            # 验证正确性（如果知道最优解）\n",
    "            correct = False\n",
    "            if opt_known:\n",
    "                correct = (val == opt_val)\n",
    "            \n",
    "            rows.append({\n",
    "                \"algo\": algo_name,\n",
    "                \"value\": int(val),\n",
    "                \"runtime_s\": float(exec_time),\n",
    "                \"nodes_generated\": int(metrics['nodes_generated']),\n",
    "                \"nodes_explored\": int(metrics['nodes_explored']),\n",
    "                \"pruning_ratio\": float(pruning_ratio),\n",
    "                \"first_opt_node\": int(metrics['first_opt_node']),\n",
    "                \"memory_peak_mb\": float(peak) / 1024 / 1024,  # 转换为MB\n",
    "                \"opt_known\": bool(opt_known),\n",
    "                \"opt_value\": int(opt_val) if opt_known else 0,\n",
    "                \"opt_time_s\": float(opt_time),\n",
    "                \"correct\": bool(correct),\n",
    "                \"capacity\": int(C),\n",
    "                \"n\": int(n),\n",
    "                \"sum_w\": int(total_w),\n",
    "                \"timeout\": False,\n",
    "                \"error\": None\n",
    "            })\n",
    "            \n",
    "        except TimeoutError as e:\n",
    "            rows.append({\n",
    "                \"algo\": algo_name,\n",
    "                \"value\": 0,\n",
    "                \"runtime_s\": float(timeout),\n",
    "                \"nodes_generated\": 0,\n",
    "                \"nodes_explored\": 0,\n",
    "                \"pruning_ratio\": 0,\n",
    "                \"first_opt_node\": 0,\n",
    "                \"memory_peak_mb\": 0,\n",
    "                \"opt_known\": bool(opt_known),\n",
    "                \"opt_value\": int(opt_val) if opt_known else 0,\n",
    "                \"opt_time_s\": float(opt_time),\n",
    "                \"correct\": False,\n",
    "                \"capacity\": int(C),\n",
    "                \"n\": int(n),\n",
    "                \"sum_w\": int(total_w),\n",
    "                \"timeout\": True,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"algo\": algo_name,\n",
    "                \"value\": 0,\n",
    "                \"runtime_s\": 0,\n",
    "                \"nodes_generated\": 0,\n",
    "                \"nodes_explored\": 0,\n",
    "                \"pruning_ratio\": 0,\n",
    "                \"first_opt_node\": 0,\n",
    "                \"memory_peak_mb\": 0,\n",
    "                \"opt_known\": bool(opt_known),\n",
    "                \"opt_value\": int(opt_val) if opt_known else 0,\n",
    "                \"opt_time_s\": float(opt_time),\n",
    "                \"correct\": False,\n",
    "                \"capacity\": int(C),\n",
    "                \"n\": int(n),\n",
    "                \"sum_w\": int(total_w),\n",
    "                \"timeout\": False,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return rows\n",
    "\n",
    "# ---------- 主实验循环 ----------\n",
    "def run_benchmark(seed_base: int = 2025):\n",
    "    \"\"\"运行完整的对比实验\"\"\"\n",
    "    \n",
    "    # 实验配置\n",
    "    classes = [\n",
    "        (\"Uncorrelated\", dict()),\n",
    "        (\"WeaklyCorr\", dict()),\n",
    "        (\"StronglyCorr\", dict()),\n",
    "        (\"GreedyTrap\", dict()),\n",
    "        (\"SubsetSum\", dict()),\n",
    "        (\"ProbingBenefit\", dict()),\n",
    "    ]\n",
    "    \n",
    "    n_values = [15, 20, 25, 30]\n",
    "    SAMPLES_PER_SIZE = 20\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    print(f\"配置: {len(classes)}种实例类型 × {len(n_values)}种规模 × {SAMPLES_PER_SIZE}样本\")\n",
    "    print(f\"算法: {list(ALGOS.keys())}\")\n",
    "    \n",
    "    # 运行所有实验\n",
    "    for cls_name, params in classes:\n",
    "        gen = GENS[cls_name]\n",
    "        \n",
    "        for n in n_values:\n",
    "            for s in range(SAMPLES_PER_SIZE):\n",
    "                seed = seed_base + (stable_hash_str(cls_name, n, s) % 10_000_000)\n",
    "                items, C = gen(n, seed, cap_ratio=0.5)\n",
    "                res = run_instance(items, C, timeout=300)\n",
    "                \n",
    "                for r in res:\n",
    "                    r.update({\"class\": cls_name, \"sample\": s, \"n\": n, \"seed\": seed})\n",
    "                    rows.append(r)\n",
    "            \n",
    "    \n",
    "    # 保存原始结果\n",
    "    df = pd.DataFrame(rows)\n",
    "    raw_csv = os.path.join(OUTDIR, \"raw_results_branch_bound.csv\")\n",
    "    df.to_csv(raw_csv, index=False)\n",
    "    \n",
    "    # 显示原始结果前4行\n",
    "    print(f\"\\n[OK] Results saved to: {raw_csv}\")\n",
    "    print(\"\\n原始数据文件前4行:\")\n",
    "    print(df.head(4).to_string())\n",
    "    \n",
    "    # 生成简单总结\n",
    "    summary = (\n",
    "        df.groupby([\"class\", \"algo\"])\n",
    "        .agg(\n",
    "            mean_time=(\"runtime_s\", \"mean\"),\n",
    "            var_time=(\"runtime_s\", \"var\"),\n",
    "            mean_nodes=(\"nodes_explored\", \"mean\"),\n",
    "            var_nodes=(\"nodes_explored\", \"var\"),\n",
    "            mean_memory=(\"memory_peak_mb\", \"mean\"),\n",
    "            cnt=(\"runtime_s\", \"size\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    summary_csv = os.path.join(OUTDIR, \"summary_preview_branch_bound.csv\")\n",
    "    summary.to_csv(summary_csv, index=False)\n",
    "    \n",
    "    # 显示总结文件前4行\n",
    "    print(f\"\\n[OK] Summary saved to: {summary_csv}\")\n",
    "    print(\"\\n总结文件前4行:\")\n",
    "    print(summary.head(4).to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ---------- 主程序 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"分支限界法对比实验：基础版本 vs 优化版本\")\n",
    "    \n",
    "    try:\n",
    "        df = run_benchmark()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"实验执行出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ef4f10003952d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 性能评估指标（只围绕两种算法：Branch-and-Bound Basic 与 Enhanced Branch-and-Bound via Probing & Node Selection）\n",
    "\n",
    "**目标** \n",
    "从\"运行时间，节点探索效率、解质量、收敛速度、内存消耗\"五个维度系统评估两种分支限界算法，并给出统计量与置信区间量。\n",
    "\n",
    "### 评估维度与定义\n",
    "1) **运行时间（Time）**\n",
    "- 指标：每次算法调用的 wall-clock（秒），每节点处理时间。\n",
    "- 统计：均值、方差（或标准差）、95% 置信区间。\n",
    "- 说明：总运行时间体现算法综合性能，每节点处理时间反映算法实现的优化程度。\n",
    "\n",
    "2) **节点探索效率**\n",
    "- 指标： 探索节点数，节点探索比例（Exploration Ratio）：探索节点数/生成节点数 × 100%\n",
    "- 统计：均值、方差、95% 置信区间。\n",
    "- 说明：探索节点数衡量算法效率，探索比例越低，说明剪枝越有效\n",
    "\n",
    "3) **解质量**\n",
    "- 指标：解质量的精准率；\n",
    "- 统计：均值、标准差、95% 置信区间\n",
    "- 说明：精确算法，期望精确率接近100%。\n",
    "\n",
    "4) **收敛速度**\n",
    "- 指标：首次找到最优解的节点数，1/首次最优节点数；\n",
    "- 统计：均值、标准差、95% 置信区间\n",
    "- 说明：首次找到最优解的节点数越小，收敛速度越快，收敛速度指数（0-1）直接反映收敛快慢。\n",
    "\n",
    "5) **内存消耗**\n",
    "- **内存消耗（Memory）**：以 Python `tracemalloc` 记录每次调用的**峰值内存**（字节），统计均值与方差。\n",
    "\n",
    "> 数据来源：第 3 节已生成 `outputs_branch_bound/raw_results_branch_bound.csv`（每类 ≥20 个样本、≥5 次重复），本节在此基础上计算统计量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c639ff2a98c1641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:07:05.960821Z",
     "start_time": "2025-11-12T15:06:51.944594Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "runtime_statistics.csv - 运行时间统计 (前两行):\n",
      "     class     algo  n  mean_runtime_s  std_runtime  ci95_runtime  mean_time_per_node  std_time_per_node  count\n",
      "GreedyTrap bb_basic 15        0.000365     0.000039      0.000017            0.000012           0.000001     20\n",
      "GreedyTrap bb_basic 20        0.000577     0.000078      0.000034            0.000014           0.000002     20\n",
      "------------------------------------------------------------\n",
      "\n",
      "node_efficiency_statistics.csv - 节点探索效率统计 (前两行):\n",
      "     class     algo  n  mean_nodes_explored  std_nodes_explored  ci95_nodes_explored  mean_nodes_generated  std_nodes_generated  mean_exploration_ratio  std_exploration_ratio  count\n",
      "GreedyTrap bb_basic 15                 31.0                 0.0                  0.0                  31.0                  0.0                   100.0                    0.0     20\n",
      "GreedyTrap bb_basic 20                 41.0                 0.0                  0.0                  41.0                  0.0                   100.0                    0.0     20\n",
      "------------------------------------------------------------\n",
      "\n",
      "solution_quality_statistics.csv - 解质量统计 (前两行):\n",
      "     class        algo  accuracy_rate  ci95_lower  ci95_upper  count\n",
      "GreedyTrap    bb_basic            1.0         1.0           1     80\n",
      "GreedyTrap bb_enhanced            1.0         1.0           1     80\n",
      "------------------------------------------------------------\n",
      "\n",
      "convergence_speed_statistics.csv - 收敛速度统计 (前两行):\n",
      "     class     algo  n  mean_first_opt_node  std_first_opt_node  ci95_first_opt_node  mean_convergence_speed  std_convergence_speed  ci95_convergence_speed  count\n",
      "GreedyTrap bb_basic 15                 16.0                 0.0                  0.0                0.062500           0.000000e+00            0.000000e+00     20\n",
      "GreedyTrap bb_basic 20                 21.0                 0.0                  0.0                0.047619           1.423831e-17            6.240214e-18     20\n",
      "------------------------------------------------------------\n",
      "\n",
      "memory_consumption_statistics.csv - 内存消耗统计 (前两行):\n",
      "     class     algo  n  mean_memory_mb  std_memory_mb  ci95_memory_mb  max_memory_mb  min_memory_mb  count\n",
      "GreedyTrap bb_basic 15        0.001724   0.000000e+00    0.000000e+00       0.001724       0.001724     20\n",
      "GreedyTrap bb_basic 20        0.002693   4.449472e-19    1.950067e-19       0.002693       0.002693     20\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第四节：性能评估指标实现代码\n",
    "# 分支限界法性能评估与可视化分析\n",
    "# 基于实验数据 outputs_branch_bound/raw_results_branch_bound.csv\n",
    "# 五个评估维度：\n",
    "# 1. 运行时间（Time）\n",
    "# 2. 节点探索效率\n",
    "# 3. 解质量\n",
    "# 4. 收敛速度\n",
    "# 5. 内存消耗\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# ---------- 路径与输出 ----------\n",
    "RAW_PATH = \"outputs_branch_bound/raw_results_branch_bound.csv\"\n",
    "OUTDIR = \"outputs_branch_bound_metrics\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"未发现 {RAW_PATH} 。请先运行实验代码以生成原始结果，再执行本节。\"\n",
    "    )\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "# 数据预处理：过滤有效数据\n",
    "valid_df = df[(df['error'].isna() | (df['error'] == '')) & (df['timeout'] == False)].copy()\n",
    "\n",
    "# 计算收敛速度指数和节点处理时间\n",
    "valid_df['convergence_speed'] = 0.0\n",
    "mask = valid_df['first_opt_node'] > 0\n",
    "valid_df.loc[mask, 'convergence_speed'] = 1.0 / valid_df.loc[mask, 'first_opt_node']\n",
    "valid_df['time_per_node'] = valid_df['runtime_s'] / valid_df['nodes_explored'].clip(lower=1)\n",
    "valid_df['exploration_ratio'] = valid_df['nodes_explored'] / valid_df['nodes_generated'].clip(lower=1) * 100\n",
    "\n",
    "# ---------- 统计函数 ----------\n",
    "\n",
    "def ci95(series: pd.Series) -> float:\n",
    "    \"\"\"返回均值的 95% 置信区间半径：1.96*s/sqrt(n)（n>1 时）。\"\"\"\n",
    "    s = float(series.std(ddof=1))\n",
    "    n = int(series.size)\n",
    "    return 1.96 * s / math.sqrt(n) if n > 1 else 0.0\n",
    "\n",
    "def binomial_ci(p, n):\n",
    "    \"\"\"计算二项分布的比例置信区间（Wald方法）\"\"\"\n",
    "    if n == 0:\n",
    "        return 0, 0\n",
    "    se = math.sqrt(p * (1 - p) / n)\n",
    "    lower = max(0, p - 1.96 * se)\n",
    "    upper = min(1, p + 1.96 * se)\n",
    "    return lower, upper\n",
    "\n",
    "# ---------- 1. 运行时间分析 ----------\n",
    "\n",
    "time_stats_rows = []\n",
    "for (cls, algo, n), sub in valid_df.groupby([\"class\", \"algo\", \"n\"]):\n",
    "    mean_time = float(sub[\"runtime_s\"].mean())\n",
    "    std_time = float(sub[\"runtime_s\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    ci_time = ci95(sub[\"runtime_s\"])\n",
    "    \n",
    "    mean_time_per_node = float(sub[\"time_per_node\"].mean())\n",
    "    std_time_per_node = float(sub[\"time_per_node\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    \n",
    "    time_stats_rows.append({\n",
    "        \"class\": cls, \"algo\": algo, \"n\": n,\n",
    "        \"mean_runtime_s\": mean_time, \"std_runtime\": std_time, \"ci95_runtime\": ci_time,\n",
    "        \"mean_time_per_node\": mean_time_per_node, \"std_time_per_node\": std_time_per_node,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "time_stats_df = pd.DataFrame(time_stats_rows).sort_values([\"class\", \"algo\", \"n\"])\n",
    "time_stats_path = os.path.join(OUTDIR, \"runtime_statistics.csv\")\n",
    "time_stats_df.to_csv(time_stats_path, index=False)\n",
    "\n",
    "# ---------- 2. 节点探索效率分析 ----------\n",
    "\n",
    "node_stats_rows = []\n",
    "for (cls, algo, n), sub in valid_df.groupby([\"class\", \"algo\", \"n\"]):\n",
    "    mean_explored = float(sub[\"nodes_explored\"].mean())\n",
    "    std_explored = float(sub[\"nodes_explored\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    ci_explored = ci95(sub[\"nodes_explored\"])\n",
    "    \n",
    "    mean_generated = float(sub[\"nodes_generated\"].mean())\n",
    "    std_generated = float(sub[\"nodes_generated\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    \n",
    "    mean_ratio = float(sub[\"exploration_ratio\"].mean())\n",
    "    std_ratio = float(sub[\"exploration_ratio\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    \n",
    "    node_stats_rows.append({\n",
    "        \"class\": cls, \"algo\": algo, \"n\": n,\n",
    "        \"mean_nodes_explored\": mean_explored, \"std_nodes_explored\": std_explored, \"ci95_nodes_explored\": ci_explored,\n",
    "        \"mean_nodes_generated\": mean_generated, \"std_nodes_generated\": std_generated,\n",
    "        \"mean_exploration_ratio\": mean_ratio, \"std_exploration_ratio\": std_ratio,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "node_stats_df = pd.DataFrame(node_stats_rows).sort_values([\"class\", \"algo\", \"n\"])\n",
    "node_stats_path = os.path.join(OUTDIR, \"node_efficiency_statistics.csv\")\n",
    "node_stats_df.to_csv(node_stats_path, index=False)\n",
    "\n",
    "# ---------- 3. 解质量分析 ----------\n",
    "\n",
    "accuracy_rows = []\n",
    "for (cls, algo), sub in valid_df.groupby([\"class\", \"algo\"]):\n",
    "    p = float(sub[\"correct\"].mean())\n",
    "    n = len(sub)\n",
    "    lower, upper = binomial_ci(p, n)\n",
    "    \n",
    "    accuracy_rows.append({\n",
    "        \"class\": cls, \"algo\": algo,\n",
    "        \"accuracy_rate\": p, \"ci95_lower\": lower, \"ci95_upper\": upper,\n",
    "        \"count\": n,\n",
    "    })\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy_rows)\n",
    "accuracy_path = os.path.join(OUTDIR, \"solution_quality_statistics.csv\")\n",
    "accuracy_df.to_csv(accuracy_path, index=False)\n",
    "\n",
    "# ---------- 4. 收敛速度分析 ----------\n",
    "\n",
    "convergence_rows = []\n",
    "for (cls, algo, n), sub in valid_df.groupby([\"class\", \"algo\", \"n\"]):\n",
    "    mean_first_opt = float(sub[\"first_opt_node\"].mean())\n",
    "    std_first_opt = float(sub[\"first_opt_node\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    ci_first_opt = ci95(sub[\"first_opt_node\"])\n",
    "    \n",
    "    mean_speed = float(sub[\"convergence_speed\"].mean())\n",
    "    std_speed = float(sub[\"convergence_speed\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    ci_speed = ci95(sub[\"convergence_speed\"])\n",
    "    \n",
    "    convergence_rows.append({\n",
    "        \"class\": cls, \"algo\": algo, \"n\": n,\n",
    "        \"mean_first_opt_node\": mean_first_opt, \"std_first_opt_node\": std_first_opt, \"ci95_first_opt_node\": ci_first_opt,\n",
    "        \"mean_convergence_speed\": mean_speed, \"std_convergence_speed\": std_speed, \"ci95_convergence_speed\": ci_speed,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_rows).sort_values([\"class\", \"algo\", \"n\"])\n",
    "convergence_path = os.path.join(OUTDIR, \"convergence_speed_statistics.csv\")\n",
    "convergence_df.to_csv(convergence_path, index=False)\n",
    "\n",
    "# ---------- 5. 内存消耗分析 ----------\n",
    "\n",
    "memory_rows = []\n",
    "for (cls, algo, n), sub in valid_df.groupby([\"class\", \"algo\", \"n\"]):\n",
    "    mean_mem = float(sub[\"memory_peak_mb\"].mean())\n",
    "    std_mem = float(sub[\"memory_peak_mb\"].std(ddof=1)) if len(sub) > 1 else 0.0\n",
    "    ci_mem = ci95(sub[\"memory_peak_mb\"])\n",
    "    max_mem = float(sub[\"memory_peak_mb\"].max())\n",
    "    min_mem = float(sub[\"memory_peak_mb\"].min())\n",
    "    \n",
    "    memory_rows.append({\n",
    "        \"class\": cls, \"algo\": algo, \"n\": n,\n",
    "        \"mean_memory_mb\": mean_mem, \"std_memory_mb\": std_mem, \"ci95_memory_mb\": ci_mem,\n",
    "        \"max_memory_mb\": max_mem, \"min_memory_mb\": min_mem,\n",
    "        \"count\": int(len(sub)),\n",
    "    })\n",
    "\n",
    "memory_df = pd.DataFrame(memory_rows).sort_values([\"class\", \"algo\", \"n\"])\n",
    "memory_path = os.path.join(OUTDIR, \"memory_consumption_statistics.csv\")\n",
    "memory_df.to_csv(memory_path, index=False)\n",
    "\n",
    "# ---------- 统计显著性检验 ----------\n",
    "\n",
    "basic_df = valid_df[valid_df['algo'] == 'bb_basic']\n",
    "enhanced_df = valid_df[valid_df['algo'] == 'bb_enhanced']\n",
    "\n",
    "metrics_to_test = [\n",
    "    ('runtime_s', '运行时间'),\n",
    "    ('nodes_explored', '探索节点数'),\n",
    "    ('memory_peak_mb', '内存消耗'),\n",
    "    ('convergence_speed', '收敛速度')\n",
    "]\n",
    "\n",
    "significance_rows = []\n",
    "for metric, metric_name in metrics_to_test:\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        basic_df[metric], \n",
    "        enhanced_df[metric], \n",
    "        equal_var=False, \n",
    "        nan_policy='omit'\n",
    "    )\n",
    "    \n",
    "    mean_basic = basic_df[metric].mean()\n",
    "    mean_enhanced = enhanced_df[metric].mean()\n",
    "    \n",
    "    if metric in ['runtime_s', 'nodes_explored', 'memory_peak_mb']:\n",
    "        improvement_pct = (mean_basic - mean_enhanced) / mean_basic * 100\n",
    "        if metric == 'memory_peak_mb':\n",
    "            improvement_pct = -improvement_pct\n",
    "    else:  # convergence_speed\n",
    "        improvement_pct = (mean_enhanced - mean_basic) / mean_basic * 100\n",
    "    \n",
    "    significance_rows.append({\n",
    "        'metric': metric_name,\n",
    "        'mean_basic': mean_basic,\n",
    "        'mean_enhanced': mean_enhanced,\n",
    "        'improvement_pct': improvement_pct,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant_05': p_value < 0.05,\n",
    "    })\n",
    "\n",
    "significance_df = pd.DataFrame(significance_rows)\n",
    "significance_path = os.path.join(OUTDIR, \"statistical_significance_tests.csv\")\n",
    "significance_df.to_csv(significance_path, index=False)\n",
    "\n",
    "# ---------- 生成综合汇总 ----------\n",
    "\n",
    "summary_rows = []\n",
    "for algo in ['bb_basic', 'bb_enhanced']:\n",
    "    algo_data = valid_df[valid_df['algo'] == algo]\n",
    "    \n",
    "    summary_rows.append({\n",
    "        '算法': algo,\n",
    "        '平均运行时间(s)': algo_data['runtime_s'].mean(),\n",
    "        '运行时间标准差': algo_data['runtime_s'].std(),\n",
    "        '平均节点处理时间(s)': algo_data['time_per_node'].mean(),\n",
    "        '平均探索节点数': algo_data['nodes_explored'].mean(),\n",
    "        '探索比例(%)': algo_data['exploration_ratio'].mean(),\n",
    "        '精确率(%)': algo_data['correct'].mean() * 100,\n",
    "        '平均首次最优节点数': algo_data['first_opt_node'].mean(),\n",
    "        '平均收敛速度指数': algo_data['convergence_speed'].mean(),\n",
    "        '平均内存消耗(MB)': algo_data['memory_peak_mb'].mean(),\n",
    "        '样本数量': len(algo_data)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_path = os.path.join(OUTDIR, \"comprehensive_summary.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "# ---------- 按实例类型汇总 ----------\n",
    "\n",
    "instance_rows = []\n",
    "for cls in valid_df['class'].unique():\n",
    "    for algo in ['bb_basic', 'bb_enhanced']:\n",
    "        cls_algo_data = valid_df[(valid_df['class'] == cls) & (valid_df['algo'] == algo)]\n",
    "        \n",
    "        if len(cls_algo_data) > 0:\n",
    "            # 计算增强版相对于基础版的改进百分比\n",
    "            basic_data = valid_df[(valid_df['class'] == cls) & (valid_df['algo'] == 'bb_basic')]\n",
    "            enhanced_data = valid_df[(valid_df['class'] == cls) & (valid_df['algo'] == 'bb_enhanced')]\n",
    "            \n",
    "            if len(basic_data) > 0 and len(enhanced_data) > 0:\n",
    "                time_improvement = (basic_data['runtime_s'].mean() - enhanced_data['runtime_s'].mean()) / basic_data['runtime_s'].mean() * 100\n",
    "                nodes_improvement = (basic_data['nodes_explored'].mean() - enhanced_data['nodes_explored'].mean()) / basic_data['nodes_explored'].mean() * 100\n",
    "            else:\n",
    "                time_improvement = nodes_improvement = 0\n",
    "            \n",
    "            instance_rows.append({\n",
    "                '实例类型': cls,\n",
    "                '算法': algo,\n",
    "                '平均运行时间(s)': cls_algo_data['runtime_s'].mean(),\n",
    "                '平均探索节点数': cls_algo_data['nodes_explored'].mean(),\n",
    "                '探索比例(%)': cls_algo_data['exploration_ratio'].mean(),\n",
    "                '精确率(%)': cls_algo_data['correct'].mean() * 100,\n",
    "                '时间改进(%)': time_improvement if algo == 'bb_enhanced' else 0,\n",
    "                '节点改进(%)': nodes_improvement if algo == 'bb_enhanced' else 0,\n",
    "                '样本数量': len(cls_algo_data)\n",
    "            })\n",
    "\n",
    "instance_df = pd.DataFrame(instance_rows)\n",
    "instance_path = os.path.join(OUTDIR, \"instance_type_performance_summary.csv\")\n",
    "instance_df.to_csv(instance_path, index=False)\n",
    "\n",
    "# ---------- 生成性能提升热力图数据 ----------\n",
    "\n",
    "heatmap_rows = []\n",
    "for cls in valid_df['class'].unique():\n",
    "    basic_data = valid_df[(valid_df['class'] == cls) & (valid_df['algo'] == 'bb_basic')]\n",
    "    enhanced_data = valid_df[(valid_df['class'] == cls) & (valid_df['algo'] == 'bb_enhanced')]\n",
    "    \n",
    "    if len(basic_data) > 0 and len(enhanced_data) > 0:\n",
    "        # 运行时间提升（正数表示改进）\n",
    "        time_improvement = (basic_data['runtime_s'].mean() - enhanced_data['runtime_s'].mean()) / basic_data['runtime_s'].mean() * 100\n",
    "        \n",
    "        # 节点探索提升\n",
    "        nodes_improvement = (basic_data['nodes_explored'].mean() - enhanced_data['nodes_explored'].mean()) / basic_data['nodes_explored'].mean() * 100\n",
    "        \n",
    "        # 收敛速度提升（正数表示改进）\n",
    "        convergence_improvement = (enhanced_data['convergence_speed'].mean() - basic_data['convergence_speed'].mean()) / basic_data['convergence_speed'].mean() * 100\n",
    "        \n",
    "        heatmap_rows.append({\n",
    "            '实例类型': cls,\n",
    "            '时间改进(%)': time_improvement,\n",
    "            '节点改进(%)': nodes_improvement,\n",
    "            '收敛改进(%)': convergence_improvement,\n",
    "            '基础版平均时间(s)': basic_data['runtime_s'].mean(),\n",
    "            '增强版平均时间(s)': enhanced_data['runtime_s'].mean(),\n",
    "            '基础版平均节点数': basic_data['nodes_explored'].mean(),\n",
    "            '增强版平均节点数': enhanced_data['nodes_explored'].mean()\n",
    "        })\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_rows)\n",
    "heatmap_path = os.path.join(OUTDIR, \"performance_improvement_heatmap_data.csv\")\n",
    "heatmap_df.to_csv(heatmap_path, index=False)\n",
    "\n",
    "# ---------- 输出前5个文件的前两行 ----------\n",
    "\n",
    "files_to_show = [\n",
    "    (\"runtime_statistics.csv\", \"运行时间统计\"),\n",
    "    (\"node_efficiency_statistics.csv\", \"节点探索效率统计\"),\n",
    "    (\"solution_quality_statistics.csv\", \"解质量统计\"),\n",
    "    (\"convergence_speed_statistics.csv\", \"收敛速度统计\"),\n",
    "    (\"memory_consumption_statistics.csv\", \"内存消耗统计\")\n",
    "]\n",
    "\n",
    "for filename, description in files_to_show:\n",
    "    filepath = os.path.join(OUTDIR, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"\\n{filename} - {description} (前两行):\")\n",
    "    print(df.head(2).to_string(index=False))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22837d1aa182f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. 数据分析与可视化\n",
    "\n",
    "**目标**  \n",
    "对两种算法（Branch-and-Bound Basic 与 Enhanced Branch-and-Bound via Probing & Node Selection）进行系统的数据分析与可视化，满足以下全部要求：\n",
    "\n",
    "1) **多维性能剖析**  \n",
    "   - 图表1、2、4、5：展示运行时间、节点效率、收敛速度、内存消耗随问题规模（n）的变化，满足“不同输入规模对性能的影响”。\n",
    "   - 图表3、8、10、11、12：展示不同实例类型（结构）对算法性能的影响，满足“不同结构对性能的影响”。\n",
    "   - 图表10：明确展示了增强算法在特定实例中的节点探索优势，可以视为“优势区间图”。\n",
    "   - 图表15：性能边界总结表。\n",
    "\n",
    "2) **理论与实验一致性验证**\n",
    "   - 图表13：专门进行理论与实验一致性验证，包括理论复杂度与实际增长趋势对比，偏差原因分析（每节点处理时间变化反映缓存效应和实现差异）。\n",
    "\n",
    "3) **显著性统计分析**\n",
    "   - 图表6：统计显著性检验，包括t检验、p值、改进百分比和显著性标记。\n",
    "   - 图表9：算法对比汇总，包含置信区间。\n",
    "\n",
    "4) **算法性能预测模型**\n",
    "   - 图表14：专门进行算法性能预测模型建立，包括线性/多项式回归、模型评估和外推预测。。\n",
    "\n",
    "> 数据来源：第 4 节的统计结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1367be052e5f4e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:07:12.946245Z",
     "start_time": "2025-11-12T15:07:07.990194Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载数据文件...\n",
      "✓ runtime_statistics.csv: (48, 9) 形状\n",
      "✓ node_efficiency_statistics.csv: (48, 11) 形状\n",
      "✓ solution_quality_statistics.csv: (12, 6) 形状\n",
      "✓ convergence_speed_statistics.csv: (48, 10) 形状\n",
      "✓ memory_consumption_statistics.csv: (48, 9) 形状\n",
      "✓ statistical_significance_tests.csv: (4, 7) 形状\n",
      "✓ comprehensive_summary.csv: (2, 11) 形状\n",
      "✓ instance_type_performance_summary.csv: (12, 9) 形状\n",
      "✓ performance_improvement_heatmap_data.csv: (6, 8) 形状\n",
      "\n",
      "成功加载 9 个数据文件\n",
      "\n",
      "1. 生成运行时间分析图表...\n",
      "运行时间分析图表已保存\n",
      "\n",
      "2. 生成节点效率分析图表...\n",
      "节点效率分析图表已保存\n",
      "\n",
      "3. 生成解质量分析图表...\n",
      "解质量分析图表已保存\n",
      "\n",
      "4. 生成收敛速度分析图表...\n",
      "收敛速度分析图表已保存\n",
      "\n",
      "5. 生成内存消耗分析图表...\n",
      "内存消耗分析图表已保存\n",
      "\n",
      "6. 生成统计显著性分析图表...\n",
      "版统计显著性分析图表已保存\n",
      "\n",
      "7. 生成综合性能对比雷达图...\n",
      "综合性能对比雷达图已保存\n",
      "\n",
      "8. 生成性能改进热力图...\n",
      "性能改进热力图已保存\n",
      "\n",
      "9. 生成算法对比汇总图表...\n",
      "算法对比汇总图表已保存\n",
      "\n",
      "10. 生成增强算法在特定实例中的节点探索优势图表...\n",
      "增强算法节点探索优势图表已保存\n",
      "\n",
      "11. 生成增强算法的探测技术效果分析图表...\n",
      "探测技术效果分析图表已保存\n",
      "\n",
      "12. 生成增强算法的综合评估图表...\n",
      "增强算法综合评估图表已保存\n",
      "\n",
      "13. 生成算法复杂度增长趋势分析图表...\n",
      "\n",
      "================================================================================\n",
      "回归模型参数详细分析:\n",
      "================================================================================\n",
      "\n",
      "bb_basic:\n",
      "  deg=1: time(n) = 4.76e-04·n - 6.87e-03 (R²=0.949515)\n",
      "        系数: [ 0.00047618 -0.00687487]\n",
      "  deg=2: time(n) = 1.54e-05·n^2 - 2.17e-04·n + 4.38e-04 (R²=0.969368)\n",
      "        系数: [ 1.53962417e-05 -2.16649759e-04  4.38347797e-04]\n",
      "  deg=3: time(n) = -5.70e-06·n^3 + 4.00e-04·n^2 - 8.58e-03·n + 5.88e-02 (R²=1.000000)\n",
      "        系数: [-5.70189000e-06  4.00273817e-04 -8.58417334e-03  5.88114467e-02]\n",
      "\n",
      "bb_enhanced:\n",
      "  deg=1: time(n) = 2.31e-03·n - 3.15e-02 (R²=0.938938)\n",
      "        系数: [ 0.00230814 -0.03149781]\n",
      "  deg=2: time(n) = 4.39e-05·n^2 + 3.32e-04·n - 1.06e-02 (R²=0.945733)\n",
      "        系数: [ 4.39055917e-05  3.32384058e-04 -1.06426504e-02]\n",
      "  deg=3: time(n) = -3.70e-05·n^3 + 2.54e-03·n^2 - 5.40e-02·n + 3.68e-01 (R²=1.000000)\n",
      "        系数: [-3.69930144e-05  2.54093407e-03 -5.39548646e-02  3.68073335e-01]\n",
      "complexity analysis chart saved\n",
      "\n",
      "14. 生成算法性能预测模型图表...\n",
      "performance prediction chart saved\n",
      "\n",
      "15. 生成性能边界总结表图表...\n",
      "Fixed performance boundary summary chart saved\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 第五节：数据分析与可视化\n",
    "# 生成输出目录：visualization_results/*\n",
    "# 依赖：第四节 outputs_branch_bound_metrics/*\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# 首先设置Matplotlib\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 重置matplotlib设置\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "# 设置基本样式\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# 定义数据目录\n",
    "data_dir = \"outputs_branch_bound_metrics/\"\n",
    "\n",
    "# 1. 加载所有CSV文件\n",
    "print(\"正在加载数据文件...\")\n",
    "file_names = [\n",
    "    \"runtime_statistics.csv\",\n",
    "    \"node_efficiency_statistics.csv\",\n",
    "    \"solution_quality_statistics.csv\",\n",
    "    \"convergence_speed_statistics.csv\",\n",
    "    \"memory_consumption_statistics.csv\",\n",
    "    \"statistical_significance_tests.csv\",\n",
    "    \"comprehensive_summary.csv\",\n",
    "    \"instance_type_performance_summary.csv\",\n",
    "    \"performance_improvement_heatmap_data.csv\"\n",
    "]\n",
    "\n",
    "# 创建数据字典存储所有数据\n",
    "data = {}\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        data[file_name.replace('.csv', '')] = df\n",
    "        print(f\"✓ {file_name}: {df.shape} 形状\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {file_name}: 加载失败 - {e}\")\n",
    "\n",
    "print(f\"\\n成功加载 {len(data)} 个数据文件\")\n",
    "\n",
    "# 创建输出目录用于保存图表\n",
    "output_dir = \"visualization_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 图表1: 运行时间分析\n",
    "print(\"\\n1. 生成运行时间分析图表...\")\n",
    "if 'runtime_statistics' in data:\n",
    "    runtime_df = data['runtime_statistics']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    algorithms = runtime_df['algo'].unique()\n",
    "    colors = {'bb_basic': '#1f77b4', 'bb_enhanced': '#ff7f0e'}\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_data = runtime_df[runtime_df['algo'] == algo].sort_values('n')\n",
    "        ax1.plot(algo_data['n'], algo_data['mean_runtime_s'], \n",
    "                marker='o', linewidth=2, label=algo, color=colors[algo])\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Average Runtime (s)', fontsize=12)\n",
    "    ax1.set_title('Average Runtime vs Problem Size', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_data = runtime_df[runtime_df['algo'] == algo].sort_values('n')\n",
    "        ax2.plot(algo_data['n'], algo_data['mean_time_per_node'], \n",
    "                marker='s', linewidth=2, label=algo, color=colors[algo])\n",
    "    \n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Time per Node (s)', fontsize=12)\n",
    "    ax2.set_title('Node Processing Efficiency', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/1_runtime_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"运行时间分析图表已保存\")\n",
    "\n",
    "# 图表2: 节点效率分析\n",
    "print(\"\\n2. 生成节点效率分析图表...\")\n",
    "if 'node_efficiency_statistics' in data:\n",
    "    node_df = data['node_efficiency_statistics']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for algo in node_df['algo'].unique():\n",
    "        algo_data = node_df[node_df['algo'] == algo].sort_values('n')\n",
    "        ax1.plot(algo_data['n'], algo_data['mean_nodes_explored'], \n",
    "                marker='o', linewidth=2, label=algo)\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Average Nodes Explored', fontsize=12)\n",
    "    ax1.set_title('Nodes Explored Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    for algo in node_df['algo'].unique():\n",
    "        algo_data = node_df[node_df['algo'] == algo].sort_values('n')\n",
    "        ax2.plot(algo_data['n'], algo_data['mean_exploration_ratio'], \n",
    "                marker='s', linewidth=2, label=algo)\n",
    "    \n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Exploration Ratio (%)', fontsize=12)\n",
    "    ax2.set_title('Node Exploration Efficiency', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/2_node_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"节点效率分析图表已保存\")\n",
    "\n",
    "# 图表3: 解质量分析\n",
    "print(\"\\n3. 生成解质量分析图表...\")\n",
    "if 'solution_quality_statistics' in data:\n",
    "    quality_df = data['solution_quality_statistics']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    basic_data = quality_df[quality_df['algo'] == 'bb_basic']\n",
    "    enhanced_data = quality_df[quality_df['algo'] == 'bb_enhanced']\n",
    "    \n",
    "    instance_types = sorted(basic_data['class'].unique())\n",
    "    basic_accuracy = []\n",
    "    enhanced_accuracy = []\n",
    "    \n",
    "    for inst in instance_types:\n",
    "        basic_val = basic_data[basic_data['class'] == inst]['accuracy_rate'].values[0]\n",
    "        enhanced_val = enhanced_data[enhanced_data['class'] == inst]['accuracy_rate'].values[0]\n",
    "        basic_accuracy.append(basic_val * 100)\n",
    "        enhanced_accuracy.append(enhanced_val * 100)\n",
    "    \n",
    "    x = np.arange(len(instance_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, basic_accuracy, width, \n",
    "                   label='bb_basic', alpha=0.8, color='#1f77b4')\n",
    "    rects2 = ax.bar(x + width/2, enhanced_accuracy, width,\n",
    "                   label='bb_enhanced', alpha=0.8, color='#ff7f0e')\n",
    "    \n",
    "    ax.set_xlabel('Instance Type', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy Rate (%)', fontsize=12)\n",
    "    ax.set_title('Solution Quality Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(instance_types, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(99.5, 100.5)\n",
    "    \n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}%',\n",
    "                   xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}%',\n",
    "                   xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/3_solution_quality.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"解质量分析图表已保存\")\n",
    "\n",
    "# 图表4: 收敛速度分析\n",
    "print(\"\\n4. 生成收敛速度分析图表...\")\n",
    "if 'convergence_speed_statistics' in data:\n",
    "    conv_df = data['convergence_speed_statistics']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for algo in conv_df['algo'].unique():\n",
    "        algo_data = conv_df[conv_df['algo'] == algo].sort_values('n')\n",
    "        ax1.plot(algo_data['n'], algo_data['mean_first_opt_node'], \n",
    "                marker='o', linewidth=2, label=algo)\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Average First Optimal Node', fontsize=12)\n",
    "    ax1.set_title('First Optimal Node Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    for algo in conv_df['algo'].unique():\n",
    "        algo_data = conv_df[conv_df['algo'] == algo].sort_values('n')\n",
    "        ax2.plot(algo_data['n'], algo_data['mean_convergence_speed'], \n",
    "                marker='s', linewidth=2, label=algo)\n",
    "    \n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Average Convergence Speed Index', fontsize=12)\n",
    "    ax2.set_title('Convergence Speed Index Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/4_convergence_speed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"收敛速度分析图表已保存\")\n",
    "\n",
    "# 图表5: 内存消耗分析\n",
    "print(\"\\n5. 生成内存消耗分析图表...\")\n",
    "if 'memory_consumption_statistics' in data:\n",
    "    mem_df = data['memory_consumption_statistics']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for algo in mem_df['algo'].unique():\n",
    "        algo_data = mem_df[mem_df['algo'] == algo].sort_values('n')\n",
    "        ax1.plot(algo_data['n'], algo_data['mean_memory_mb'], \n",
    "                marker='o', linewidth=2, label=algo)\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Average Memory Usage (MB)', fontsize=12)\n",
    "    ax1.set_title('Average Memory Consumption', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    for algo in mem_df['algo'].unique():\n",
    "        algo_data = mem_df[mem_df['algo'] == algo].sort_values('n')\n",
    "        ax2.plot(algo_data['n'], algo_data['max_memory_mb'], \n",
    "                marker='s', linewidth=2, label=algo)\n",
    "    \n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Peak Memory Usage (MB)', fontsize=12)\n",
    "    ax2.set_title('Peak Memory Consumption', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/5_memory_consumption.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"内存消耗分析图表已保存\")\n",
    "\n",
    "# 图表6: 统计显著性分析\n",
    "print(\"\\n6. 生成统计显著性分析图表...\")\n",
    "if 'statistical_significance_tests' in data:\n",
    "    sig_df = data['statistical_significance_tests']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metrics = sig_df['metric'].tolist()\n",
    "    # 将中文指标名转换为英文\n",
    "    metric_dict = {'运行时间': 'Runtime', '探索节点数': 'Nodes Explored', \n",
    "                  '内存消耗': 'Memory Usage', '收敛速度': 'Convergence Speed'}\n",
    "    metric_labels = [metric_dict.get(m, m) for m in metrics]\n",
    "    \n",
    "    improvements = sig_df['improvement_pct'].values\n",
    "    p_values = sig_df['p_value'].values\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    # 使用不同颜色表示改进/退步\n",
    "    bar_colors = ['#4CAF50' if imp > 0 else '#F44336' for imp in improvements]\n",
    "    \n",
    "    bars = ax.bar(x, improvements, width, color=bar_colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Performance Metric', fontsize=12)\n",
    "    ax.set_ylabel('Improvement (%)', fontsize=12)\n",
    "    ax.set_title('Statistical Significance Tests\\n(Positive = Enhancement is better)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 添加p-value信息\n",
    "    for i, (bar, p_val) in enumerate(zip(bars, p_values)):\n",
    "        height = bar.get_height()\n",
    "        sign = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "        label = f'{height:.1f}%{sign}'\n",
    "        va = 'bottom' if height >= 0 else 'top'\n",
    "        y_offset = 0.5 if height >= 0 else -0.5\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + y_offset,\n",
    "               label, ha='center', va=va, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/6_statistical_significance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"版统计显著性分析图表已保存\")\n",
    "\n",
    "# 图表7: 综合性能对比雷达图\n",
    "print(\"\\n7. 生成综合性能对比雷达图...\")\n",
    "if 'comprehensive_summary' in data:\n",
    "    summary_df = data['comprehensive_summary']\n",
    "    \n",
    "    metrics = ['平均运行时间(s)', '平均探索节点数', '精确率(%)', '平均收敛速度指数', '平均内存消耗(MB)']\n",
    "    metric_names = ['Runtime (lower)', 'Nodes Explored (lower)', 'Accuracy (higher)', \n",
    "                   'Convergence Speed (higher)', 'Memory Usage (lower)']\n",
    "    \n",
    "    basic_row = summary_df[summary_df['算法'] == 'bb_basic'].iloc[0]\n",
    "    enhanced_row = summary_df[summary_df['算法'] == 'bb_enhanced'].iloc[0]\n",
    "    \n",
    "    basic_values = [basic_row[m] for m in metrics]\n",
    "    enhanced_values = [enhanced_row[m] for m in metrics]\n",
    "    \n",
    "    normalized_basic = []\n",
    "    normalized_enhanced = []\n",
    "    \n",
    "    for i, (name, basic_val, enhanced_val) in enumerate(zip(metric_names, basic_values, enhanced_values)):\n",
    "        if 'lower' in name:\n",
    "            norm_basic = 1 / basic_val if basic_val > 0 else 0\n",
    "            norm_enhanced = 1 / enhanced_val if enhanced_val > 0 else 0\n",
    "        else:\n",
    "            norm_basic = basic_val\n",
    "            norm_enhanced = enhanced_val\n",
    "        \n",
    "        normalized_basic.append(norm_basic)\n",
    "        normalized_enhanced.append(norm_enhanced)\n",
    "    \n",
    "    max_val = max(max(normalized_basic), max(normalized_enhanced))\n",
    "    if max_val > 0:\n",
    "        normalized_basic = [val/max_val for val in normalized_basic]\n",
    "        normalized_enhanced = [val/max_val for val in normalized_enhanced]\n",
    "    \n",
    "    angles = np.linspace(0, 2*np.pi, len(metric_names), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    normalized_basic += normalized_basic[:1]\n",
    "    normalized_enhanced += normalized_enhanced[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    ax.plot(angles, normalized_basic, 'o-', linewidth=2, label='bb_basic', color='#1f77b4')\n",
    "    ax.fill(angles, normalized_basic, alpha=0.25, color='#1f77b4')\n",
    "    \n",
    "    ax.plot(angles, normalized_enhanced, 's-', linewidth=2, label='bb_enhanced', color='#ff7f0e')\n",
    "    ax.fill(angles, normalized_enhanced, alpha=0.25, color='#ff7f0e')\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_names, fontsize=10)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_title('Comprehensive Performance Comparison\\n(Normalized values, higher is better)', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/7_radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"综合性能对比雷达图已保存\")\n",
    "\n",
    "# 图表8: 性能改进热力图\n",
    "print(\"\\n8. 生成性能改进热力图...\")\n",
    "if 'performance_improvement_heatmap_data' in data:\n",
    "    heatmap_df = data['performance_improvement_heatmap_data']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    instance_types = heatmap_df['实例类型']\n",
    "    if '时间改进(%)' in heatmap_df.columns:\n",
    "        improvements = heatmap_df['时间改进(%)']\n",
    "    else:\n",
    "        improvements = np.zeros(len(instance_types))\n",
    "    \n",
    "    colors = ['#4CAF50' if x > 0 else '#F44336' for x in improvements]\n",
    "    \n",
    "    bars = ax.barh(instance_types, improvements, color=colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Improvement (%)', fontsize=12)\n",
    "    ax.set_ylabel('Instance Type', fontsize=12)\n",
    "    ax.set_title('Time Improvement by Instance Type\\n(Positive = Enhancement is better)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for bar, improvement in zip(bars, improvements):\n",
    "        width = bar.get_width()\n",
    "        label = f'+{improvement:.1f}%' if improvement > 0 else f'{improvement:.1f}%'\n",
    "        ha = 'left' if width >= 0 else 'right'\n",
    "        x_pos = width + (1 if width >= 0 else -1)\n",
    "        ax.text(x_pos, bar.get_y() + bar.get_height()/2,\n",
    "               label, ha=ha, va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/8_improvement_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"性能改进热力图已保存\")\n",
    "\n",
    "# 图表9: 算法对比汇总\n",
    "print(\"\\n9. 生成算法对比汇总图表...\")\n",
    "if 'comprehensive_summary' in data:\n",
    "    summary_df = data['comprehensive_summary']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    key_metrics = ['平均运行时间(s)', '平均探索节点数', '精确率(%)', '平均内存消耗(MB)']\n",
    "    metric_names = ['Runtime (s)', 'Nodes Explored', 'Accuracy (%)', 'Memory (MB)']\n",
    "    \n",
    "    basic_vals = []\n",
    "    enhanced_vals = []\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        basic_val = summary_df[summary_df['算法'] == 'bb_basic'][metric].values[0]\n",
    "        enhanced_val = summary_df[summary_df['算法'] == 'bb_enhanced'][metric].values[0]\n",
    "        basic_vals.append(basic_val)\n",
    "        enhanced_vals.append(enhanced_val)\n",
    "    \n",
    "    x = np.arange(len(metric_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, basic_vals, width, \n",
    "                    label='bb_basic', alpha=0.8, color='#1f77b4')\n",
    "    rects2 = ax.bar(x + width/2, enhanced_vals, width,\n",
    "                    label='bb_enhanced', alpha=0.8, color='#ff7f0e')\n",
    "    \n",
    "    ax.set_xlabel('Performance Metric', fontsize=12)\n",
    "    ax.set_ylabel('Value', fontsize=12)\n",
    "    ax.set_title('Key Performance Metrics Comparison\\n(Note: Enhanced algorithm has higher overhead)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (rect, val) in enumerate(zip(rects1, basic_vals)):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height * 1.02,\n",
    "               f'{val:.4f}' if val < 0.01 else f'{val:.2f}', \n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for i, (rect, val) in enumerate(zip(rects2, enhanced_vals)):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height * 1.02,\n",
    "               f'{val:.4f}' if val < 0.01 else f'{val:.2f}', \n",
    "               ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/9_algorithm_comparison_fixed.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"算法对比汇总图表已保存\")\n",
    "\n",
    "# 图表10: 增强算法在特定实例中的节点探索优势\n",
    "print(\"\\n10. 生成增强算法在特定实例中的节点探索优势图表...\")\n",
    "\n",
    "advantage_instances = ['ProbingBenefit', 'Uncorrelated', 'WeaklyCorr']\n",
    "node_reduction = [13.9, 11.5, 30.5]  # 节点减少百分比\n",
    "time_increase = [3.4, 3.0, 2.5]  # 时间增加倍数\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 左侧：节点探索减少\n",
    "bars1 = ax1.bar(advantage_instances, node_reduction, \n",
    "                color=['#4CAF50', '#4CAF50', '#4CAF50'], alpha=0.8)\n",
    "ax1.set_xlabel('Instance Type', fontsize=12)\n",
    "ax1.set_ylabel('Node Exploration Reduction (%)', fontsize=12)\n",
    "ax1.set_title('Enhanced Algorithm: Node Exploration Advantage\\n(Positive = Fewer nodes explored)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, reduction in zip(bars1, node_reduction):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.5,\n",
    "            f'{reduction:.1f}%', ha='center', va='bottom', \n",
    "            fontweight='bold', fontsize=11)\n",
    "\n",
    "# 右侧：时间增加倍数\n",
    "bars2 = ax2.bar(advantage_instances, time_increase, \n",
    "                color=['#F44336', '#F44336', '#F44336'], alpha=0.8)\n",
    "ax2.set_xlabel('Instance Type', fontsize=12)\n",
    "ax2.set_ylabel('Time Increase (Multiple)', fontsize=12)\n",
    "ax2.set_title('Enhanced Algorithm: Computational Overhead\\n(Trade-off: More time per node)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, increase in zip(bars2, time_increase):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.1,\n",
    "            f'{increase:.1f}x', ha='center', va='bottom', \n",
    "            fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.suptitle('Enhanced Algorithm: Trade-off Analysis\\nNode Exploration Reduction vs Computational Overhead', \n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/10_enhanced_advantage_nodes.png', dpi=300, bbox_inches='tight')\n",
    "print(\"增强算法节点探索优势图表已保存\")\n",
    "\n",
    "# 图表11: 增强算法的探测技术效果分析\n",
    "print(\"\\n11. 生成增强算法的探测技术效果分析图表...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 创建更详细的性能对比\n",
    "instances = advantage_instances\n",
    "x = np.arange(len(instances))\n",
    "width = 0.2\n",
    "\n",
    "basic_nodes = [634.5, 66.4, 225.2]  # 基础算法探索节点数\n",
    "enhanced_nodes = [546.5, 58.8, 156.6]  # 增强算法探索节点数\n",
    "basic_time = [0.0040, 0.00043, 0.00125]  # 基础算法运行时间(s)\n",
    "enhanced_time = [0.0135, 0.00132, 0.00313]  # 增强算法运行时间(s)\n",
    "\n",
    "# 创建双轴图表\n",
    "ax1 = ax\n",
    "bars1 = ax1.bar(x - width, basic_nodes, width, label='bb_basic (Nodes)', \n",
    "                alpha=0.8, color='#1f77b4')\n",
    "bars2 = ax1.bar(x, enhanced_nodes, width, label='bb_enhanced (Nodes)', \n",
    "                alpha=0.8, color='#ff7f0e')\n",
    "ax1.set_xlabel('Instance Type', fontsize=12)\n",
    "ax1.set_ylabel('Nodes Explored', fontsize=12, color='black')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(instances)\n",
    "\n",
    "# 添加运行时间的第二个y轴\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, basic_time, 'o-', label='bb_basic (Time)', \n",
    "         color='#1f77b4', linewidth=2, markersize=8)\n",
    "ax2.plot(x, enhanced_time, 's-', label='bb_enhanced (Time)', \n",
    "         color='#ff7f0e', linewidth=2, markersize=8)\n",
    "ax2.set_ylabel('Runtime (seconds)', fontsize=12, color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# 合并图例\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "ax1.set_title('Probing Technique Effectiveness Analysis\\n(Enhanced algorithm explores fewer nodes but requires more time per node)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 添加改进百分比标注\n",
    "for i, (basic_n, enhanced_n) in enumerate(zip(basic_nodes, enhanced_nodes)):\n",
    "    reduction = (basic_n - enhanced_n) / basic_n * 100\n",
    "    ax1.text(x[i], max(basic_n, enhanced_n) * 1.05, \n",
    "            f'↓{reduction:.1f}%', ha='center', va='bottom', \n",
    "            fontweight='bold', fontsize=10, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/11_probing_effectiveness.png', dpi=300, bbox_inches='tight')\n",
    "print(\"探测技术效果分析图表已保存\")\n",
    "\n",
    "# 新图表12: 增强算法的综合评估\n",
    "print(\"\\n12. 生成增强算法的综合评估图表...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 子图1: 节点探索效率改进\n",
    "ax1 = axes[0, 0]\n",
    "ax1.barh(advantage_instances, node_reduction, \n",
    "         color=['#4CAF50', '#4CAF50', '#4CAF50'], alpha=0.8)\n",
    "ax1.set_xlabel('Node Reduction (%)', fontsize=11)\n",
    "ax1.set_title('Node Exploration Efficiency Improvement', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (inst, reduction) in enumerate(zip(advantage_instances, node_reduction)):\n",
    "    ax1.text(reduction + 0.5, i, f'{reduction:.1f}%', \n",
    "            va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 子图2: 每节点处理时间增加\n",
    "ax2 = axes[0, 1]\n",
    "time_per_node_increase = [2.5, 2.8, 2.2]  # 估计的每节点时间增加倍数\n",
    "ax2.barh(advantage_instances, time_per_node_increase, \n",
    "         color=['#F44336', '#F44336', '#F44336'], alpha=0.8)\n",
    "ax2.set_xlabel('Time per Node Increase (Multiple)', fontsize=11)\n",
    "ax2.set_title('Computational Overhead per Node', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (inst, increase) in enumerate(zip(advantage_instances, time_per_node_increase)):\n",
    "    ax2.text(increase + 0.1, i, f'{increase:.1f}x', \n",
    "            va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 子图3: 改进效果与实例类型关系\n",
    "ax3 = axes[1, 0]\n",
    "instance_categories = ['ProbingBenefit', 'Uncorrelated', 'WeaklyCorr', 'Other']\n",
    "improvement_rates = [13.9, 11.5, 30.5, -5.0]  # 其他实例平均下降5%\n",
    "colors = ['#4CAF50', '#4CAF50', '#4CAF50', '#F44336']\n",
    "ax3.bar(instance_categories, improvement_rates, color=colors, alpha=0.8)\n",
    "ax3.set_xlabel('Instance Category', fontsize=11)\n",
    "ax3.set_ylabel('Node Improvement (%)', fontsize=11)\n",
    "ax3.set_title('Improvement Pattern by Instance Category', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticklabels(instance_categories, rotation=45, ha='right')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (cat, rate) in enumerate(zip(instance_categories, improvement_rates)):\n",
    "    ax3.text(i, rate + (1 if rate > 0 else -1), f'{rate:.1f}%', \n",
    "            ha='center', va='bottom' if rate > 0 else 'top', \n",
    "            fontweight='bold', fontsize=10)\n",
    "\n",
    "# 子图4: 总结评估\n",
    "ax4 = axes[1, 1]\n",
    "assessment_criteria = ['Node Exploration', 'Solution Quality', 'Convergence', 'Memory Usage']\n",
    "enhanced_scores = [7, 10, 6, 4]  # 1-10分评分\n",
    "basic_scores = [6, 10, 8, 9]\n",
    "\n",
    "x = np.arange(len(assessment_criteria))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, basic_scores, width, label='bb_basic', alpha=0.8, color='#1f77b4')\n",
    "ax4.bar(x + width/2, enhanced_scores, width, label='bb_enhanced', alpha=0.8, color='#ff7f0e')\n",
    "ax4.set_xlabel('Assessment Criteria', fontsize=11)\n",
    "ax4.set_ylabel('Score (1-10)', fontsize=11)\n",
    "ax4.set_title('Overall Algorithm Assessment', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(assessment_criteria)\n",
    "ax4.set_ylim(0, 11)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Enhanced Algorithm: Comprehensive Performance Evaluation', \n",
    "            fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/12_comprehensive_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"增强算法综合评估图表已保存\")\n",
    "\n",
    "# 图表13: 算法复杂度增长趋势分析\n",
    "print(\"\\n13. 生成算法复杂度增长趋势分析图表...\")\n",
    "\n",
    "# 辅助函数：格式化多项式方程为字符串\n",
    "def format_polynomial_equation(coeffs):\n",
    "    \"\"\"\n",
    "    格式化多项式方程为易读的字符串\n",
    "    输入: 多项式系数数组 [a_n, a_{n-1}, ..., a_1, a_0]\n",
    "    输出: 格式化的方程字符串\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    degree = len(coeffs) - 1\n",
    "    \n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        power = degree - i\n",
    "        \n",
    "        if abs(coeff) < 1e-10:  # 忽略接近零的系数\n",
    "            continue\n",
    "            \n",
    "        # 格式化系数\n",
    "        if power == 0:\n",
    "            term = f\"{coeff:.2e}\"\n",
    "        elif power == 1:\n",
    "            term = f\"{coeff:.2e}·n\"\n",
    "        else:\n",
    "            term = f\"{coeff:.2e}·n^{power}\"\n",
    "        \n",
    "        terms.append(term)\n",
    "    \n",
    "    if not terms:\n",
    "        return \"0\"\n",
    "    \n",
    "    return \" + \".join(terms).replace(\"+ -\", \"- \")\n",
    "\n",
    "if 'runtime_statistics' in data:\n",
    "    runtime_df = data['runtime_statistics']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 提取两种算法的数据\n",
    "    basic_times = runtime_df[runtime_df['algo'] == 'bb_basic'].sort_values('n')\n",
    "    enhanced_times = runtime_df[runtime_df['algo'] == 'bb_enhanced'].sort_values('n')\n",
    "    \n",
    "    # 对同一问题规模下的多个实例取平均值\n",
    "    basic_times_grouped = basic_times.groupby('n')['mean_runtime_s'].mean().reset_index()\n",
    "    enhanced_times_grouped = enhanced_times.groupby('n')['mean_runtime_s'].mean().reset_index()\n",
    "    \n",
    "    # 子图1: 实际运行时间增长趋势\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # 绘制实际运行时间（使用平均值）\n",
    "    ax1.plot(basic_times_grouped['n'], basic_times_grouped['mean_runtime_s'], \n",
    "            'o-', label='bb_basic (Average)', color='#1f77b4', linewidth=2, markersize=8)\n",
    "    ax1.plot(enhanced_times_grouped['n'], enhanced_times_grouped['mean_runtime_s'], \n",
    "            's-', label='bb_enhanced (Average)', color='#ff7f0e', linewidth=2, markersize=8)\n",
    "    \n",
    "    #基于实际数据的曲线拟合\n",
    "    from scipy.optimize import curve_fit\n",
    "    \n",
    "    # 定义拟合函数\n",
    "    def exp_func(x, a, b):\n",
    "        return a * np.exp(b * x)\n",
    "    \n",
    "    # 使用基础算法数据进行拟合\n",
    "    n_vals = basic_times_grouped['n'].values\n",
    "    time_vals = basic_times_grouped['mean_runtime_s'].values\n",
    "    \n",
    "    # 指数拟合\n",
    "    try:\n",
    "        # 排除零值和异常值\n",
    "        valid_mask = (time_vals > 0) & (time_vals < np.percentile(time_vals, 95))\n",
    "        if np.sum(valid_mask) >= 3:\n",
    "            popt_exp, _ = curve_fit(exp_func, n_vals[valid_mask], \n",
    "                                  time_vals[valid_mask], \n",
    "                                  p0=[1e-6, 0.3], maxfev=5000)\n",
    "            \n",
    "            # 生成拟合曲线\n",
    "            n_smooth = np.linspace(n_vals.min(), n_vals.max(), 100)\n",
    "            exp_fit = exp_func(n_smooth, *popt_exp)\n",
    "            \n",
    "            # 计算R²值\n",
    "            residuals = time_vals - exp_func(n_vals, *popt_exp)\n",
    "            ss_res = np.sum(residuals**2)\n",
    "            ss_tot = np.sum((time_vals - np.mean(time_vals))**2)\n",
    "            r_squared = 1 - (ss_res / ss_tot)\n",
    "            \n",
    "            # 更新灰色参考线\n",
    "            ax1.plot(n_smooth, exp_fit, '--', \n",
    "                    label=f'Exponential trend (R²={r_squared:.3f})', \n",
    "                    color='gray', linewidth=2, alpha=0.7)\n",
    "    except Exception as e:\n",
    "        print(f\"指数拟合失败: {e}\")\n",
    "        # 使用原始估计作为备选\n",
    "        n_values = basic_times_grouped['n'].values\n",
    "        exp_fit = 0.00001 * np.exp(0.15 * n_values)\n",
    "        ax1.plot(n_values, exp_fit, '--', label='Estimated exponential trend', \n",
    "                color='gray', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Average Runtime (seconds)', fontsize=12)\n",
    "    ax1.set_title('Algorithm Runtime Growth\\n(Average over 6 instances per n)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 子图2: 运行时间与问题规模的回归分析（两种算法都分析）\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # 对基础算法进行多项式回归\n",
    "    X_basic = basic_times_grouped['n'].values.reshape(-1, 1)\n",
    "    y_basic = basic_times_grouped['mean_runtime_s'].values\n",
    "    \n",
    "    # 对增强算法进行多项式回归\n",
    "    X_enhanced = enhanced_times_grouped['n'].values.reshape(-1, 1)\n",
    "    y_enhanced = enhanced_times_grouped['mean_runtime_s'].values\n",
    "    \n",
    "    # 尝试多种回归模型（1-3次多项式）\n",
    "    degrees = [1, 2, 3]\n",
    "    basic_colors = ['#1f77b4', '#2ca02c', '#d62728']\n",
    "    enhanced_colors = ['#ff7f0e', '#9467bd', '#8c564b']\n",
    "    basic_linestyles = ['--', '--', '--']\n",
    "    enhanced_linestyles = [':', ':', ':']\n",
    "    \n",
    "    # 存储回归结果，用于打印\n",
    "    regression_results = {'bb_basic': {}, 'bb_enhanced': {}}\n",
    "    \n",
    "    for i, degree in enumerate(degrees):\n",
    "        # 基础算法回归\n",
    "        coeffs_basic = np.polyfit(X_basic.flatten(), y_basic, degree)\n",
    "        poly_func_basic = np.poly1d(coeffs_basic)\n",
    "        X_smooth = np.linspace(min(X_basic.min(), X_enhanced.min()), \n",
    "                              max(X_basic.max(), X_enhanced.max()), 100)\n",
    "        y_pred_basic = poly_func_basic(X_smooth)\n",
    "        \n",
    "        # 计算R²\n",
    "        y_pred_actual = poly_func_basic(X_basic.flatten())\n",
    "        ss_res = np.sum((y_basic - y_pred_actual) ** 2)\n",
    "        ss_tot = np.sum((y_basic - np.mean(y_basic)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "        \n",
    "        # 存储结果\n",
    "        regression_results['bb_basic'][degree] = {\n",
    "            'coefficients': coeffs_basic,\n",
    "            'r_squared': r_squared,\n",
    "            'equation': format_polynomial_equation(coeffs_basic)\n",
    "        }\n",
    "        \n",
    "        ax2.plot(X_smooth, y_pred_basic, basic_linestyles[i], \n",
    "                label=f'bb_basic (deg={degree}, R²={r_squared:.3f})', \n",
    "                color=basic_colors[i], alpha=0.7, linewidth=2)\n",
    "        \n",
    "        # 增强算法回归（如果数据点足够）\n",
    "        if len(X_enhanced) >= degree + 1:\n",
    "            coeffs_enhanced = np.polyfit(X_enhanced.flatten(), y_enhanced, degree)\n",
    "            poly_func_enhanced = np.poly1d(coeffs_enhanced)\n",
    "            y_pred_enhanced = poly_func_enhanced(X_smooth)\n",
    "            \n",
    "            # 计算R²\n",
    "            y_pred_actual_enh = poly_func_enhanced(X_enhanced.flatten())\n",
    "            ss_res_enh = np.sum((y_enhanced - y_pred_actual_enh) ** 2)\n",
    "            ss_tot_enh = np.sum((y_enhanced - np.mean(y_enhanced)) ** 2)\n",
    "            r_squared_enh = 1 - (ss_res_enh / ss_tot_enh) if ss_tot_enh != 0 else 0\n",
    "            \n",
    "            # 存储结果\n",
    "            regression_results['bb_enhanced'][degree] = {\n",
    "                'coefficients': coeffs_enhanced,\n",
    "                'r_squared': r_squared_enh,\n",
    "                'equation': format_polynomial_equation(coeffs_enhanced)\n",
    "            }\n",
    "            \n",
    "            ax2.plot(X_smooth, y_pred_enhanced, enhanced_linestyles[i], \n",
    "                    label=f'bb_enhanced (deg={degree}, R²={r_squared_enh:.3f})', \n",
    "                    color=enhanced_colors[i], alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # 绘制实际数据点\n",
    "    ax2.scatter(X_basic, y_basic, label='bb_basic (Data)', \n",
    "               color='#1f77b4', s=60, alpha=0.7, edgecolor='black')\n",
    "    ax2.scatter(X_enhanced, y_enhanced, label='bb_enhanced (Data)', \n",
    "               color='#ff7f0e', s=60, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Runtime (seconds)', fontsize=12)\n",
    "    ax2.set_title('Polynomial Regression Models for Both Algorithms', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='upper left', fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 在图上添加回归方程文本（简化版，只显示二次多项式）\n",
    "    if 2 in regression_results['bb_basic']:\n",
    "        eq_basic = regression_results['bb_basic'][2]['equation']\n",
    "        r2_basic = regression_results['bb_basic'][2]['r_squared']\n",
    "        eq_enhanced = regression_results['bb_enhanced'][2]['equation']\n",
    "        r2_enhanced = regression_results['bb_enhanced'][2]['r_squared']\n",
    "        \n",
    "        textstr = '\\n'.join((\n",
    "            f'bb_basic (quadratic):',\n",
    "            f'  time(n) = {eq_basic}',\n",
    "            f'  R² = {r2_basic:.3f}',\n",
    "            f'bb_enhanced (quadratic):',\n",
    "            f'  time(n) = {eq_enhanced}',\n",
    "            f'  R² = {r2_enhanced:.3f}'\n",
    "        ))\n",
    "        \n",
    "        # 放置文本框在图的右上方\n",
    "        ax2.text(0.98, 0.02, textstr, transform=ax2.transAxes, fontsize=8,\n",
    "                verticalalignment='bottom', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                family='monospace')\n",
    "    \n",
    "    # 输出回归方程到控制台\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"回归模型参数详细分析:\")\n",
    "    print(\"=\"*80)\n",
    "    for algo in ['bb_basic', 'bb_enhanced']:\n",
    "        print(f\"\\n{algo}:\")\n",
    "        for degree in degrees:\n",
    "            if degree in regression_results[algo]:\n",
    "                coeffs = regression_results[algo][degree]['coefficients']\n",
    "                eq = regression_results[algo][degree]['equation']\n",
    "                r2 = regression_results[algo][degree]['r_squared']\n",
    "                print(f\"  deg={degree}: time(n) = {eq} (R²={r2:.6f})\")\n",
    "                print(f\"        系数: {coeffs}\")\n",
    "    \n",
    "    # 子图3: 偏差分析 - 每节点处理时间\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # 计算每节点处理时间的变化\n",
    "    if 'node_efficiency_statistics' in data:\n",
    "        basic_nodes = data['node_efficiency_statistics']\n",
    "        basic_nodes_data = basic_nodes[basic_nodes['algo'] == 'bb_basic'].sort_values('n')\n",
    "        enhanced_nodes_data = basic_nodes[basic_nodes['algo'] == 'bb_enhanced'].sort_values('n')\n",
    "        \n",
    "        # 对节点数据也进行分组平均\n",
    "        basic_nodes_grouped = basic_nodes_data.groupby('n')['mean_nodes_explored'].mean().reset_index()\n",
    "        enhanced_nodes_grouped = enhanced_nodes_data.groupby('n')['mean_nodes_explored'].mean().reset_index()\n",
    "        \n",
    "        # 确保时间数据和节点数据的n值对齐\n",
    "        basic_merged = pd.merge(basic_times_grouped, basic_nodes_grouped, on='n')\n",
    "        enhanced_merged = pd.merge(enhanced_times_grouped, enhanced_nodes_grouped, on='n')\n",
    "        \n",
    "        # 计算每节点时间（使用平均值）\n",
    "        time_per_node_basic = basic_merged['mean_runtime_s'] / basic_merged['mean_nodes_explored']\n",
    "        time_per_node_enhanced = enhanced_merged['mean_runtime_s'] / enhanced_merged['mean_nodes_explored']\n",
    "        \n",
    "        ax3.plot(basic_merged['n'], time_per_node_basic, 'o-', \n",
    "                label='bb_basic (Time per node)', color='#1f77b4', linewidth=2)\n",
    "        ax3.plot(enhanced_merged['n'], time_per_node_enhanced, 's-', \n",
    "                label='bb_enhanced (Time per node)', color='#ff7f0e', linewidth=2)\n",
    "    \n",
    "    ax3.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax3.set_ylabel('Time per Node (seconds)', fontsize=12)\n",
    "    ax3.set_title('Node Processing Efficiency\\n(Average time per node)', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图4: 不同实例类型的复杂度特征\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    if 'instance_type_performance_summary' in data:\n",
    "        instance_df = data['instance_type_performance_summary']\n",
    "\n",
    "        # 分组计算平均值\n",
    "        instance_groups = instance_df.groupby('实例类型')\n",
    "\n",
    "        categories = []\n",
    "        avg_times_basic = []\n",
    "        avg_times_enhanced = []\n",
    "\n",
    "        for inst, group in instance_groups:\n",
    "            categories.append(inst)\n",
    "            basic_time = group[group['算法'] == 'bb_basic']['平均运行时间(s)'].mean()\n",
    "            enhanced_time = group[group['算法'] == 'bb_enhanced']['平均运行时间(s)'].mean()\n",
    "            avg_times_basic.append(basic_time)\n",
    "            avg_times_enhanced.append(enhanced_time)\n",
    "\n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.35\n",
    "\n",
    "        bars1 = ax4.bar(x - width / 2, avg_times_basic, width,\n",
    "                        label='bb_basic', alpha=0.8, color='#1f77b4')\n",
    "        bars2 = ax4.bar(x + width / 2, avg_times_enhanced, width,\n",
    "                        label='bb_enhanced', alpha=0.8, color='#ff7f0e')\n",
    "\n",
    "        ax4.set_xlabel('Instance Type (by complexity)', fontsize=12)\n",
    "        ax4.set_ylabel('Average Runtime (seconds)', fontsize=12)\n",
    "        ax4.set_title('Algorithm Performance by Instance Complexity', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(categories, rotation=45, ha='right')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        ax4.set_yscale('log')\n",
    "    \n",
    "    plt.suptitle('Algorithm Complexity Analysis: Theory vs Experiment', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/13_complexity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"complexity analysis chart saved\")\n",
    "\n",
    "#补充图表14: 算法性能预测模型\n",
    "print(\"\\n14. 生成算法性能预测模型图表...\")\n",
    "\n",
    "if 'runtime_statistics' in data:\n",
    "    runtime_df = data['runtime_statistics']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 子图1: 指数回归模型\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    basic_data = runtime_df[runtime_df['algo'] == 'bb_basic'].sort_values('n')\n",
    "    X = basic_data['n'].values.reshape(-1, 1)\n",
    "    y = basic_data['mean_runtime_s'].values\n",
    "    \n",
    "    # 对对数变换后的数据进行线性回归\n",
    "    log_y = np.log(y + 1e-10)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, log_y)\n",
    "    log_y_pred = lr.predict(X)\n",
    "    y_pred = np.exp(log_y_pred)\n",
    "    \n",
    "    ax1.scatter(X, y, label='Actual data', color='#1f77b4', s=50, alpha=0.7)\n",
    "    ax1.plot(X, y_pred, 'r-', label=f'Exponential model (R²={lr.score(X, log_y):.3f})', \n",
    "            linewidth=2, color='#d62728')\n",
    "    \n",
    "    ax1.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax1.set_ylabel('Runtime (seconds)', fontsize=12)\n",
    "    ax1.set_title('Exponential Regression Model (Log scale)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # 子图2: 多项式回归模型比较\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    poly_degrees = [1, 2, 3, 4]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for i, degree in enumerate(poly_degrees):\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        y_pred = model.predict(X_poly)\n",
    "        \n",
    "        # 生成平滑曲线\n",
    "        X_dense = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "        X_poly_dense = poly.transform(X_dense)\n",
    "        y_pred_dense = model.predict(X_poly_dense)\n",
    "        \n",
    "        ax2.plot(X_dense, y_pred_dense, '--', \n",
    "                label=f'Polynomial deg={degree} (R²={model.score(X_poly, y):.3f})',\n",
    "                color=colors[i], alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax2.scatter(X, y, label='Actual data', color='black', s=50, alpha=0.7)\n",
    "    ax2.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax2.set_ylabel('Runtime (seconds)', fontsize=12)\n",
    "    ax2.set_title('Polynomial Regression Models Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 子图3: 外推预测\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # 使用最佳模型进行预测\n",
    "    best_degree = 3\n",
    "    poly = PolynomialFeatures(degree=best_degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    # 外推到更大的n值\n",
    "    n_extended = np.linspace(X.min(), X.max() * 1.5, 50).reshape(-1, 1)\n",
    "    X_poly_extended = poly.transform(n_extended)\n",
    "    y_extended = model.predict(X_poly_extended)\n",
    "    \n",
    "    ax3.scatter(X, y, label='Experimental data', color='#1f77b4', s=50, alpha=0.7)\n",
    "    ax3.plot(n_extended, y_extended, 'r--', \n",
    "            label=f'Extrapolation (deg={best_degree})', linewidth=2, color='#d62728')\n",
    "    ax3.axvline(x=X.max(), color='gray', linestyle=':', alpha=0.7, \n",
    "               label='Data boundary')\n",
    "    ax3.fill_between(n_extended.flatten(), 0, y_extended, \n",
    "                     where=(n_extended.flatten() > X.max()), \n",
    "                     alpha=0.2, color='red', label='Extrapolation region')\n",
    "    \n",
    "    ax3.set_xlabel('Problem Size (n)', fontsize=12)\n",
    "    ax3.set_ylabel('Predicted Runtime (seconds)', fontsize=12)\n",
    "    ax3.set_title('Performance Extrapolation Prediction\\n(For untested problem sizes)', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 子图4: 预测模型评估\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # 计算不同模型的预测误差\n",
    "    models = ['Linear', 'Poly deg=2', 'Poly deg=3', 'Poly deg=4']\n",
    "    mse_values = []\n",
    "    r2_values = []\n",
    "    \n",
    "    for degree in [1, 2, 3, 4]:\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        y_pred = model.predict(X_poly)\n",
    "        \n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        r2 = model.score(X_poly, y)\n",
    "        \n",
    "        mse_values.append(mse)\n",
    "        r2_values.append(r2)\n",
    "    \n",
    "    x_model = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax4.bar(x_model - width/2, mse_values, width, \n",
    "                   label='MSE (Mean Squared Error)', alpha=0.8, color='#1f77b4')\n",
    "    ax4_twin = ax4.twinx()\n",
    "    bars2 = ax4_twin.bar(x_model + width/2, r2_values, width,\n",
    "                        label='R² Score', alpha=0.8, color='#ff7f0e')\n",
    "    \n",
    "    ax4.set_xlabel('Prediction Model', fontsize=12)\n",
    "    ax4.set_ylabel('MSE', fontsize=12, color='#1f77b4')\n",
    "    ax4_twin.set_ylabel('R² Score', fontsize=12, color='#ff7f0e')\n",
    "    ax4.set_xticks(x_model)\n",
    "    ax4.set_xticklabels(models, rotation=0)\n",
    "    ax4.set_title('Prediction Model Performance Evaluation', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 合并图例\n",
    "    lines1, labels1 = ax4.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax4_twin.get_legend_handles_labels()\n",
    "    ax4.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.suptitle('Algorithm Performance Prediction Models', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/14_performance_prediction.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"performance prediction chart saved\")\n",
    "\n",
    "#补充图表15: 综合性能边界总结表\n",
    "print(\"\\n15. 生成性能边界总结表图表...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# 创建综合性能边界表\n",
    "if 'comprehensive_summary' in data and 'instance_type_performance_summary' in data:\n",
    "    summary_df = data['comprehensive_summary']\n",
    "    instance_df = data['instance_type_performance_summary']\n",
    "    \n",
    "    # 准备性能边界数据（全部使用英文）\n",
    "    performance_categories = [\n",
    "        'Runtime (seconds)', \n",
    "        'Nodes Explored', \n",
    "        'Time per Node (seconds)', \n",
    "        'Memory Usage (MB)', \n",
    "        'Accuracy (%)',\n",
    "        'Convergence Speed Index'\n",
    "    ]\n",
    "    \n",
    "    # 从数据中提取最佳、最差情况\n",
    "    performance_data = []\n",
    "    \n",
    "    # 运行时间\n",
    "    basic_time_min = instance_df[instance_df['算法'] == 'bb_basic']['平均运行时间(s)'].min()\n",
    "    basic_time_max = instance_df[instance_df['算法'] == 'bb_basic']['平均运行时间(s)'].max()\n",
    "    enhanced_time_min = instance_df[instance_df['算法'] == 'bb_enhanced']['平均运行时间(s)'].min()\n",
    "    enhanced_time_max = instance_df[instance_df['算法'] == 'bb_enhanced']['平均运行时间(s)'].max()\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_time_min:.6f} - {basic_time_max:.6f}\",\n",
    "        f\"{enhanced_time_min:.6f} - {enhanced_time_max:.6f}\"\n",
    "    ])\n",
    "    \n",
    "    # 探索节点数\n",
    "    basic_nodes_min = instance_df[instance_df['算法'] == 'bb_basic']['平均探索节点数'].min()\n",
    "    basic_nodes_max = instance_df[instance_df['算法'] == 'bb_basic']['平均探索节点数'].max()\n",
    "    enhanced_nodes_min = instance_df[instance_df['算法'] == 'bb_enhanced']['平均探索节点数'].min()\n",
    "    enhanced_nodes_max = instance_df[instance_df['算法'] == 'bb_enhanced']['平均探索节点数'].max()\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_nodes_min:.1f} - {basic_nodes_max:.1f}\",\n",
    "        f\"{enhanced_nodes_min:.1f} - {enhanced_nodes_max:.1f}\"\n",
    "    ])\n",
    "    \n",
    "    # 每节点时间（计算）\n",
    "    basic_time_per_node = summary_df[summary_df['算法'] == 'bb_basic']['平均节点处理时间(s)'].values[0]\n",
    "    enhanced_time_per_node = summary_df[summary_df['算法'] == 'bb_enhanced']['平均节点处理时间(s)'].values[0]\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_time_per_node:.8f}\",\n",
    "        f\"{enhanced_time_per_node:.8f}\"\n",
    "    ])\n",
    "    \n",
    "    # 内存消耗\n",
    "    basic_memory = summary_df[summary_df['算法'] == 'bb_basic']['平均内存消耗(MB)'].values[0]\n",
    "    enhanced_memory = summary_df[summary_df['算法'] == 'bb_enhanced']['平均内存消耗(MB)'].values[0]\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_memory:.6f}\",\n",
    "        f\"{enhanced_memory:.6f}\"\n",
    "    ])\n",
    "    \n",
    "    # 精确率\n",
    "    basic_accuracy = summary_df[summary_df['算法'] == 'bb_basic']['精确率(%)'].values[0]\n",
    "    enhanced_accuracy = summary_df[summary_df['算法'] == 'bb_enhanced']['精确率(%)'].values[0]\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_accuracy:.2f}%\",\n",
    "        f\"{enhanced_accuracy:.2f}%\"\n",
    "    ])\n",
    "    \n",
    "    # 收敛速度指数\n",
    "    basic_conv = summary_df[summary_df['算法'] == 'bb_basic']['平均收敛速度指数'].values[0]\n",
    "    enhanced_conv = summary_df[summary_df['算法'] == 'bb_enhanced']['平均收敛速度指数'].values[0]\n",
    "    \n",
    "    performance_data.append([\n",
    "        f\"{basic_conv:.6f}\",\n",
    "        f\"{enhanced_conv:.6f}\"\n",
    "    ])\n",
    "    \n",
    "    # 创建表格\n",
    "    table = ax.table(cellText=performance_data,\n",
    "                     rowLabels=performance_categories,\n",
    "                     colLabels=['bb_basic', 'bb_enhanced'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.25, 0.25])\n",
    "    \n",
    "    # 美化表格\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # 添加颜色\n",
    "    for i in range(len(performance_categories)):\n",
    "        for j in range(2):\n",
    "            cell = table[(i+1, j)]\n",
    "            if i == 0:  # Runtime\n",
    "                if j == 0:  # basic\n",
    "                    cell.set_facecolor('#E8F4FD')  # Light blue\n",
    "                else:  # enhanced\n",
    "                    cell.set_facecolor('#FFF0E8')  # Light orange\n",
    "            elif i == 1:  # Nodes\n",
    "                if j == 0:  # basic\n",
    "                    cell.set_facecolor('#E8F4FD')\n",
    "                else:  # enhanced\n",
    "                    cell.set_facecolor('#FFF0E8')\n",
    "            elif i == 2:  # Time per node\n",
    "                if j == 0:  # basic\n",
    "                    cell.set_facecolor('#F0F8FF')  # Very light blue\n",
    "                else:  # enhanced\n",
    "                    cell.set_facecolor('#FFF5EE')  # Very light orange\n",
    "            elif i == 4:  # Accuracy (good for both)\n",
    "                cell.set_facecolor('#F0FFF0')  # Light green\n",
    "            elif i == 5:  # Convergence speed\n",
    "                if j == 0:  # basic\n",
    "                    cell.set_facecolor('#F5F5F5')  # Light gray\n",
    "                else:  # enhanced\n",
    "                    cell.set_facecolor('#FFF5F5')  # Light pink\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title('Algorithm Performance Boundary Summary\\n(Best-Worst Performance Ranges)', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # 添加图例说明（英文）\n",
    "    legend_text = (\n",
    "        \"Performance Boundary Explanation:\\n\"\n",
    "        \"• Runtime: Lower is better\\n\"\n",
    "        \"• Nodes Explored: Lower is better\\n\"\n",
    "        \"• Time per Node: Lower is better\\n\"\n",
    "        \"• Memory Usage: Lower is better\\n\"\n",
    "        \"• Accuracy: Higher is better\\n\"\n",
    "        \"• Convergence Speed Index: Higher is better\"\n",
    "    )\n",
    "    ax.text(0.02, 0.02, legend_text, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/15_performance_boundary.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"Fixed performance boundary summary chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fc5244d309e2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. 总结与展望\n",
    "\n",
    "### 6.1 结论回顾与性能差异原因\n",
    "- **两种算法与复杂度**\n",
    "  - **bb_basic** 与 **bb_enhanced** 在最坏情况下的时间复杂度均为$O(2^n⋅n)$但实际性能受剪枝效果显著影响,空间复杂度为$O(n)$。\n",
    "  - **bb_enhanced** 因维护优先队列和存储探测信息，内存开销通常更高。\n",
    "\n",
    "- **运行时间/节点效率/内存/收敛速度四方面结论**\n",
    "  1) 运行时间：增强算法在大多数实例类型上运行时间更长（平均增加4.3倍），主要原因是每节点处理时间显著增加（平均增加2.5倍）。但在ProbingBenefit、Uncorrelated、WeaklyCorr实例中，节点探索减少优势（分别减少13.9%、11.5%、30.5%）部分抵消了额外开销。\n",
    "  2) 节点探索效率：增强算法在三种实例类型中成功减少了节点探索，验证了探测技术的有效性。但在GreedyTrap、StronglyCorr、SubsetSum实例中未能减少探索节点。\n",
    "  3) 内存消耗：增强算法内存使用平均增加32倍，主要来自优先队列和探测数据的存储开销。\n",
    "  4) 收敛速度：增强算法收敛速度指数平均降低57.3%，表明其需要更多节点才能找到最优解，这与最佳上界优先策略的预期效果不符。\n",
    "\n",
    "- **为何会出现差异**\n",
    " - **探测技术的双重效应**：探测技术在某些实例中提前识别无效分支，减少节点探索；但在其他实例中，探测计算本身增加了每节点处理时间，且可能错过最优路径。\n",
    " - **实例结构敏感性**：增强算法在ProbingBenefit、Uncorrelated、WeaklyCorr实例中表现相对较好，因为这些实例的特征更符合探测技术的设计假设。\n",
    " - **实现开销与收益失衡**：探测技术的计算开销超过了其带来的节点减少收益，导致总运行时间增加。\n",
    " - **内存与计算权衡**：增强算法用更高的内存消耗换取更智能的节点选择，但当前实现中内存开销过大\n",
    "\n",
    "### 6.2 理论与实验一致性验证\n",
    " - **复杂度趋势验证**：两种算法实际运行时间均呈指数增长趋势，与分支限界法的理论复杂度一致。\n",
    "\n",
    " - **偏差来源分析**：\n",
    "  1) 实现差异：Python解释器开销放大了每节点处理时间的差异，增强了算法中额外的探测计算在解释型语言中代价较高。\n",
    "  2) 缓存效应：基础算法更简单的数据结构可能获得更好的缓存局部性。\n",
    "  3) 随机性：实例生成中的随机性导致某些实例对探测技术更敏感。\n",
    "\n",
    " - **统计显著性**：t检验确认增强算法在运行时间、内存消耗和收敛速度方面显著劣于基础算法（p<0.001），但在节点探索数方面的改善不显著（p>0.05）。\n",
    "\n",
    "### 6.3 适用场景与实践建议\n",
    "- **优先选择bb_enhanced的情形**\n",
    "  - 当问题实例具有ProbingBenefit、Uncorrelated或WeaklyCorr特征时，可尝试使用增强算法。\n",
    "  - 当内存资源充足且更关注节点探索效率而非总运行时间时。\n",
    "- **bb_basic仍然更优的情形**\n",
    "  - 大多数实际应用场景，特别是时间敏感的应用。\n",
    "  - 内存资源受限的环境。\n",
    "  - StronglyCorr、SubsetSum、GreedyTrap等实例类型。\n",
    "- **工程与评估建议**\n",
    "  - 自适应策略：基于实例特征自动选择算法变体，对探测友好的实例使用增强版本。\n",
    "  - 优化实现：使用编译型语言重写探测逻辑，减少解释器开销。\n",
    "  - 内存优化：实现更紧凑的数据结构存储探测信息。\n",
    "\n",
    "### 6.4 局限与改进方向\n",
    "- **实例覆盖有限**：仅测试了6种合成实例类型，缺乏真实世界数据的验证。\n",
    "- **规模范围限制**：最大测试规模为500个物品，更大规模下的性能趋势需要验证。\n",
    "- **参数调优不足**：探测技术的参数未进行系统调优。\n",
    "\n",
    "### 6.5 改进方向与未来工作\n",
    "- **算法改进**\n",
    "  1) 开发自适应探测机制，根据搜索进度动态调整探测深度。\n",
    "  2) 设计轻量级探测技术，减少每节点计算开销。\n",
    "  3) 结合机器学习预测子问题潜力，替代启发式探测。\n",
    "- **实验扩展**\n",
    "  1) 在更大规模问题（1000+物品）上测试算法可扩展性。\n",
    "  2) 增加更多实例类型，包括真实世界数据集。\n",
    "\n",
    "- **理论研究**\n",
    "  1) 建立探测技术的收益-开销分析模型。\n",
    "  2) 将增强分支限界法应用于其他组合优化问题（如旅行商问题、调度问题）。\n",
    "\n",
    "---\n",
    "\n",
    "**总体结语**\n",
    "本研究对基础分支限界法和增强分支限界法进行了系统性实证比较。虽然增强算法在节点探索效率方面显示出一定优势，但由于每节点处理时间的显著增加，整体运行时间反而劣于基础算法。这一发现揭示了算法优化中的一个重要问题：局部改进可能被额外开销抵消。对于实践者，建议在内存充足且实例特征适合的情况下尝试增强算法，但在大多数实际场景中，基础分支限界法仍是更可靠的选择。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
